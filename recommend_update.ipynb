{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b21d73fc-27c4-4c6b-8fa1-b310ed63a926",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\23109\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\23109\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\23109\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing corpus analysis from corpus_analysis.pkl...\n",
      "Corpus loaded successfully!\n",
      "\n",
      "=== Testing Recommender ===\n",
      "\n",
      "\n",
      "Testing query 1/3: I am a JHU graduate student in data science, looking for internships in machine learning\n",
      "Query tokens: ['jhu', 'graduate', 'student', 'data', 'science', 'internship', 'machine', 'learning']\n",
      "Expanding query with related terms...\n",
      "Vectorizing query...\n",
      "Calculating similarities to corpus documents...\n",
      "Finding top 5 most relevant posts...\n",
      "\n",
      "Top 5 Recommendations for query: I am a JHU graduate student in data science, looking for internships in machine learning\n",
      "================================================================================\n",
      "3596. [0.779] How to start Data Science and Machine Learning Career?\n",
      "   So, what‚Äôs the buzz all about with Data Science and Machine Learning. Seems like every other developer is fascinated by these two terms. If you ask an...\n",
      "   subreddit: r/ReviewNPrep, score: 4\n",
      "   URL: https://www.reddit.com/r/ReviewNPrep/comments/r1lqul/how_to_start_data_science_and_machine_learning/\n",
      "--------------------------------------------------------------------------------\n",
      "7759. [0.749] Various Aspects of Data Science including Machine Learning\n",
      "   # Data science programs are already paving the road for the future, which is already here. In light of this, it is not surprising that data science is...\n",
      "   subreddit: r/aboutupdates, score: 1\n",
      "   URL: https://www.reddit.com/r/aboutupdates/comments/11jpq0a/various_aspects_of_data_science_including_machine/\n",
      "--------------------------------------------------------------------------------\n",
      "7665. [0.739] Data Science Course: Your Pathway to a High-Demand Career\n",
      "   In today‚Äôs technology-driven world, data has become the new currency. From small startups to large multinational corporations, businesses rely on data...\n",
      "   subreddit: r/DataScientist, score: 2\n",
      "   URL: https://www.reddit.com/r/DataScientist/comments/1ftfvhv/data_science_course_your_pathway_to_a_highdemand/\n",
      "--------------------------------------------------------------------------------\n",
      "7666. [0.739] Data Science Course: Your Pathway to a High-Demand Career\n",
      "   In today‚Äôs technology-driven world, data has become the new currency. From small startups to large multinational corporations, businesses rely on data...\n",
      "   subreddit: r/DataScientist, score: 2\n",
      "   URL: https://www.reddit.com/r/DataScientist/comments/1ftfv7v/data_science_course_your_pathway_to_a_highdemand/\n",
      "--------------------------------------------------------------------------------\n",
      "12800. [0.700] 15 New Data Science, Data Engineering and Machine Learning jobs\n",
      "   |Job Title|Company|Location|Country|Skills|\n",
      "|:-|:-|:-|:-|:-|\n",
      "|[Data Scientist.](https://datayoshi.com/offer/198223/data-scientist)|[NextLink Group](ht...\n",
      "   subreddit: r/jobbit, score: 1\n",
      "   URL: https://www.reddit.com/r/jobbit/comments/1goxfbd/15_new_data_science_data_engineering_and_machine/\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Testing query 2/3: Senior software engineer with React and Node.js experience\n",
      "Query tokens: ['senior', 'engineer', 'react', 'node']\n",
      "Expanding query with related terms...\n",
      "Vectorizing query...\n",
      "Calculating similarities to corpus documents...\n",
      "Finding top 5 most relevant posts...\n",
      "\n",
      "Top 5 Recommendations for query: Senior software engineer with React and Node.js experience\n",
      "================================================================================\n",
      "31783. [0.845] ‚úã May 14 - 48 new Senior Software Engineer Jobs\n",
      "   | Job Position | Salary | Locations |\n",
      "|------|------|------|\n",
      " |[Senior Software Engineer](https://echojobs.io/job/general-dynamics-mission-systems-inc...\n",
      "   subreddit: r/CodingJobs, score: 1\n",
      "   URL: https://www.reddit.com/r/CodingJobs/comments/1crk4pr/may_14_48_new_senior_software_engineer_jobs/\n",
      "--------------------------------------------------------------------------------\n",
      "22260. [0.844] ü¶ä May 18 - 98 new Senior Software Engineer Jobs\n",
      "   | Job Position | Salary | Locations |\n",
      "|------|------|------|\n",
      " |[Senior Engineering Manager](https://echojobs.io/job/thumbtack-senior-engineering-manag...\n",
      "   subreddit: r/CodingJobs, score: 1\n",
      "   URL: https://www.reddit.com/r/CodingJobs/comments/1cupmnx/may_18_98_new_senior_software_engineer_jobs/\n",
      "--------------------------------------------------------------------------------\n",
      "39695. [0.842] üê¨ May 13 - 99 new Senior Software Engineer Jobs\n",
      "   | Job Position | Salary | Locations |\n",
      "|------|------|------|\n",
      " |[Senior Software Development Engineer](https://echojobs.io/job/expedia-senior-software-...\n",
      "   subreddit: r/CodingJobs, score: 1\n",
      "   URL: https://www.reddit.com/r/CodingJobs/comments/1cqr9cr/may_13_99_new_senior_software_engineer_jobs/\n",
      "--------------------------------------------------------------------------------\n",
      "22255. [0.839] ‚ú® May 30 - 98 new Senior Software Engineer Jobs\n",
      "   | Job Position | Salary | Locations |\n",
      "|------|------|------|\n",
      " |[Senior Frontend Engineer](https://echojobs.io/job/duckduckgo-senior-frontend-engineer-...\n",
      "   subreddit: r/CodingJobs, score: 1\n",
      "   URL: https://www.reddit.com/r/CodingJobs/comments/1d3w6d1/may_30_98_new_senior_software_engineer_jobs/\n",
      "--------------------------------------------------------------------------------\n",
      "39691. [0.837] üöÄ May 16 - 100 new Senior Software Engineer Jobs\n",
      "   | Job Position | Salary | Locations |\n",
      "|------|------|------|\n",
      " |[Sr. Software Engineer](https://echojobs.io/job/lattice-sr-software-engineer-ici3q?utm_...\n",
      "   subreddit: r/CodingJobs, score: 1\n",
      "   URL: https://www.reddit.com/r/CodingJobs/comments/1ct57jk/may_16_100_new_senior_software_engineer_jobs/\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Testing query 3/3: Entry level Python developer positions in Baltimore\n",
      "Query tokens: ['entry', 'level', 'python', 'baltimore']\n",
      "Expanding query with related terms...\n",
      "Vectorizing query...\n",
      "Calculating similarities to corpus documents...\n",
      "Finding top 5 most relevant posts...\n",
      "\n",
      "Top 5 Recommendations for query: Entry level Python developer positions in Baltimore\n",
      "================================================================================\n",
      "56111. [0.558] Compiling a list of \"entry-level\"/\"data-entry\" job title phrases. What else should be included here?\n",
      "   Ultimately trying to make sure I'm searching for all the right things when looking for an \"entry-level\" job that could be adjacent to \"data entry\" (wh...\n",
      "   subreddit: r/careeradvice, score: 1\n",
      "   URL: https://www.reddit.com/r/careeradvice/comments/1k5cfk2/compiling_a_list_of_entryleveldataentry_job_title/\n",
      "--------------------------------------------------------------------------------\n",
      "53734. [0.513] 19 New Data Science, Data Engineering and Machine Learning jobs\n",
      "   |Job Title|Company|Location|Country|Skills|\n",
      "|:-|:-|:-|:-|:-|\n",
      "|[Data Scientist Artificial Intelligence](https://datayoshi.com/offer/589500/data-scienti...\n",
      "   subreddit: r/BigDataJobs, score: 1\n",
      "   URL: https://www.reddit.com/r/BigDataJobs/comments/18drl5m/19_new_data_science_data_engineering_and_machine/\n",
      "--------------------------------------------------------------------------------\n",
      "15237. [0.508] 20 New Data Science, Data Engineering and Machine Learning jobs\n",
      "   |Job Title|Company|Location|Country|Skills|\n",
      "|:-|:-|:-|:-|:-|\n",
      "|[CIEL/SEL/23614: Data Scientist](https://datayoshi.com/offer/787395/ciel-sel-23614-data-...\n",
      "   subreddit: r/techjobs, score: 2\n",
      "   URL: https://www.reddit.com/r/techjobs/comments/12u9h2z/20_new_data_science_data_engineering_and_machine/\n",
      "--------------------------------------------------------------------------------\n",
      "7936. [0.505] 18 New Data Science, Data Engineering and Machine Learning jobs\n",
      "   |Job Title|Company|Location|Country|Skills|\n",
      "|:-|:-|:-|:-|:-|\n",
      "|[Lead Data Engineer ‚Äì Data Platform (Bangkok-Based,...](https://datayoshi.com/offer/6250...\n",
      "   subreddit: r/BigDataJobs, score: 1\n",
      "   URL: https://www.reddit.com/r/BigDataJobs/comments/1erc0il/18_new_data_science_data_engineering_and_machine/\n",
      "--------------------------------------------------------------------------------\n",
      "16902. [0.504] New PCED-30-01 Dumps for PCED ‚Äì Certified Entry-Level Data Analyst with Python Exam Preparation\n",
      "   To achieve the PCED ‚Äì Certified Entry-Level Data Analyst with Python certification successfully, you need to choose the new PCED-30-01 dumps from Dump...\n",
      "   subreddit: r/u_dumpsbase, score: 2\n",
      "   URL: https://www.reddit.com/r/u_dumpsbase/comments/1k7dtpl/new_pced3001_dumps_for_pced_certified_entrylevel/\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import Counter\n",
    "import warnings\n",
    "import pickle\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "warnings.filterwarnings('ignore')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Text preprocessing\n",
    "def preprocess_text(text):\n",
    "\n",
    "    if not isinstance(text, str) or not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    \n",
    "    # Remove special characters and replace with space\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Basic stopwords\n",
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "CS_BASIC_STOPWORDS = {\n",
    "    'job', 'jobs', 'looking', 'work', 'position', 'positions', 'company', 'companies',\n",
    "    'experience', 'programming', 'code', 'coding', 'developer', 'development', 'software',\n",
    "    'career', 'careers', 'role', 'roles', 'opportunity', 'opportunities', 'hiring',\n",
    "    'apply', 'application', 'interview', 'requirement', 'requirements', 'skill', 'skills',\n",
    "    'year', 'years', 'month', 'months', 'week', 'weeks', 'day', 'days',\n",
    "    'like', 'want', 'need', 'would', 'could', 'should', 'may', 'might', 'must',\n",
    "    'know', 'get', 'got', 'go', 'going', 'think', 'thought', 'see', 'look', 'help'\n",
    "}\n",
    "STOP_WORDS.update(CS_BASIC_STOPWORDS)\n",
    "\n",
    "# Tokenize and lemmatize\n",
    "def tokenize_and_lemmatize(text):\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Lemmatize and remove stopwords\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens \n",
    "              if token not in STOP_WORDS and len(token) > 2]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# This class holds preprocessed data and analysis results, separated from the recommender logic\n",
    "class RedditJobCorpusAnalysis:\n",
    "    def __init__(self, df):\n",
    "\n",
    "        self.df = df.copy()\n",
    "        self.vectorizer = None\n",
    "        self.tfidf_matrix = None\n",
    "        self.term_importance = {}\n",
    "        self.common_bigrams = {}\n",
    "        self.topics = {}\n",
    "        self.additional_stopwords = set()\n",
    "        self._preprocess_data()\n",
    "        self._analyze_corpus()\n",
    "        self._create_tfidf_matrix()\n",
    "    def _preprocess_data(self):\n",
    "        \n",
    "        print(\"Preprocessing corpus text...\")\n",
    "        # Create combined text field (title + selftext)\n",
    "        self.df['combined_text'] = self.df['title'].fillna('') + ' ' + self.df['selftext'].fillna('')\n",
    "        # Preprocess text with progress bar\n",
    "        print(\"Processing text fields...\")\n",
    "        self.df['processed_text'] = [preprocess_text(text) for text in tqdm(self.df['combined_text'], desc=\"Preprocessing texts\")]\n",
    "    \n",
    "    def _analyze_corpus(self):\n",
    "        \n",
    "        print(\"Analyzing corpus to identify important terms...\")\n",
    "        \n",
    "        # Extract the processed text\n",
    "        corpus = self.df['processed_text'].tolist()\n",
    "        \n",
    "        # Find additional stopwords based on high document frequency\n",
    "        print(\"Vectorizing corpus for stopword analysis...\")\n",
    "        count_vec = CountVectorizer(min_df=5)\n",
    "        count_matrix = count_vec.fit_transform(corpus)\n",
    "        count_features = count_vec.get_feature_names_out()\n",
    "        \n",
    "        # Calculate document frequency for each term\n",
    "        print(\"Calculating document frequencies...\")\n",
    "        doc_freq = np.array((count_matrix > 0).sum(axis=0)).flatten()\n",
    "        doc_freq_percent = doc_freq / len(corpus)\n",
    "        \n",
    "        # Terms that appear in more than 70% of documents might be domain-specific stopwords\n",
    "        potential_stopwords = {count_features[i] for i in range(len(count_features)) \n",
    "                               if doc_freq_percent[i] > 0.7}\n",
    "        \n",
    "        # TF-IDF scores\n",
    "        print(\"Creating TF-IDF representation for stopword filtering...\")\n",
    "        tfidf_vec = TfidfVectorizer(min_df=5, max_df=0.95)\n",
    "        tfidf_matrix = tfidf_vec.fit_transform(corpus)\n",
    "        tfidf_features = tfidf_vec.get_feature_names_out()\n",
    "        \n",
    "        # Calculate average TF-IDF for each term\n",
    "        avg_tfidf = np.array(tfidf_matrix.mean(axis=0)).flatten()\n",
    "        print(\"Filtering potential stopwords...\")\n",
    "        for term in tqdm(potential_stopwords, desc=\"Analyzing stopwords\"):\n",
    "            if term in tfidf_features:\n",
    "                idx = list(tfidf_features).index(term)\n",
    "                if avg_tfidf[idx] < 0.01:  # Low information content\n",
    "                    self.additional_stopwords.add(term)\n",
    "        \n",
    "        print(f\"Identified {len(self.additional_stopwords)} additional domain-specific stopwords\")\n",
    "        \n",
    "        # Update stopwords\n",
    "        global STOP_WORDS\n",
    "        STOP_WORDS.update(self.additional_stopwords)\n",
    "        \n",
    "        # Extract bigrams (phrases)\n",
    "        print(\"Extracting common bigrams...\")\n",
    "        bigram_vectorizer = CountVectorizer(ngram_range=(2, 2), min_df=5)\n",
    "        bigram_matrix = bigram_vectorizer.fit_transform(corpus)\n",
    "        bigram_features = bigram_vectorizer.get_feature_names_out()\n",
    "        \n",
    "        # Calculate bigram frequency\n",
    "        bigram_freq = np.array(bigram_matrix.sum(axis=0)).flatten()\n",
    "        \n",
    "        # Get top bigrams\n",
    "        top_bigram_indices = bigram_freq.argsort()[-300:][::-1]  # Top 300 bigrams\n",
    "        self.common_bigrams = {}\n",
    "        print(\"Organizing top bigrams...\")\n",
    "        for i in tqdm(top_bigram_indices, desc=\"Processing bigrams\"):\n",
    "            self.common_bigrams[bigram_features[i]] = bigram_freq[i]\n",
    "        \n",
    "        print(f\"Extracted {len(self.common_bigrams)} significant bigrams\")\n",
    "        \n",
    "        # Topic modeling using NMF\n",
    "        print(\"Performing topic modeling...\")\n",
    "        nmf_model = NMF(n_components=10, random_state=42)\n",
    "        nmf_topics = nmf_model.fit_transform(tfidf_matrix)\n",
    "        \n",
    "        # Get top terms for each topic\n",
    "        print(\"Extracting topics...\")\n",
    "        for topic_idx in tqdm(range(len(nmf_model.components_)), desc=\"Processing topics\"):\n",
    "            topic = nmf_model.components_[topic_idx]\n",
    "            top_terms_idx = topic.argsort()[-20:][::-1]  # Top 20 terms\n",
    "            top_terms = [tfidf_features[i] for i in top_terms_idx]\n",
    "            self.topics[f\"Topic {topic_idx+1}\"] = top_terms\n",
    "        \n",
    "        # Calculate TF-IDF importance for each term\n",
    "        print(\"Calculating term importance...\")\n",
    "        self.term_importance = {}\n",
    "        for i, term in enumerate(tqdm(tfidf_features, desc=\"Processing terms\")):\n",
    "            self.term_importance[term] = avg_tfidf[i]\n",
    "        \n",
    "        # Sort by importance\n",
    "        self.term_importance = dict(sorted(self.term_importance.items(), \n",
    "                                           key=lambda x: x[1], \n",
    "                                           reverse=True))\n",
    "        \n",
    "        print(\"Corpus analysis complete!\")\n",
    "    \n",
    "    def _create_tfidf_matrix(self):\n",
    "\n",
    "        # Create a corpus of processed documents\n",
    "        corpus = self.df['processed_text'].tolist()\n",
    "        \n",
    "        # Initialize and fit TF-IDF vectorizer\n",
    "        print(\"Creating TF-IDF vectors...\")\n",
    "        self.vectorizer = TfidfVectorizer(\n",
    "            tokenizer=tokenize_and_lemmatize,\n",
    "            min_df=2,\n",
    "            max_df=0.90,\n",
    "            max_features=5000,\n",
    "            ngram_range=(1, 2)\n",
    "        )\n",
    "        print(\"Fitting TF-IDF vectorizer (this may take a while)...\")\n",
    "        self.tfidf_matrix = self.vectorizer.fit_transform(tqdm(corpus, desc=\"Vectorizing documents\"))\n",
    "        feature_names = self.vectorizer.get_feature_names_out()\n",
    "        \n",
    "        print(f\"Vocabulary size: {len(feature_names)}\")\n",
    "        print(f\"TF-IDF matrix shape: {self.tfidf_matrix.shape}\")\n",
    "    \n",
    "    def get_top_terms(self, n=50):\n",
    "\n",
    "        return list(self.term_importance.items())[:n]\n",
    "    \n",
    "    def get_topics(self):\n",
    "\n",
    "        return self.topics\n",
    "\n",
    "\n",
    "# Base recommender class that uses an existing corpus analysis\n",
    "class DataDrivenRedditJobRecommender:\n",
    "    def __init__(self, corpus_analysis):\n",
    "\n",
    "        self.df = corpus_analysis.df\n",
    "        self.vectorizer = corpus_analysis.vectorizer\n",
    "        self.tfidf_matrix = corpus_analysis.tfidf_matrix\n",
    "        self.term_importance = corpus_analysis.term_importance\n",
    "        self.common_bigrams = corpus_analysis.common_bigrams\n",
    "        self.topics = corpus_analysis.topics\n",
    "    \n",
    "    def expand_query(self, query_tokens):\n",
    "\n",
    "        expanded_tokens = query_tokens.copy()\n",
    "        for token in query_tokens:\n",
    "            # Check if token appears in any topic\n",
    "            for topic, terms in self.topics.items():\n",
    "                if token in terms:\n",
    "                    # Add some related terms from the same topic\n",
    "                    related_terms = [t for t in terms[:10] if t != token]\n",
    "                    expanded_tokens.extend(related_terms[:3])  # Add up to 3 related terms\n",
    "        \n",
    "        # Weight tokens by importance\n",
    "        weighted_tokens = []\n",
    "        for token in expanded_tokens:\n",
    "            # If token is in term_importance, weight it accordingly\n",
    "            if token in self.term_importance:\n",
    "                weight = min(5, int(self.term_importance[token] * 100) + 1)\n",
    "                weighted_tokens.extend([token] * weight)\n",
    "            else:\n",
    "                weighted_tokens.append(token)\n",
    "        \n",
    "        # Add common bigrams that contain query tokens\n",
    "        for bigram, freq in self.common_bigrams.items():\n",
    "            token1, token2 = bigram.split()\n",
    "            if token1 in query_tokens or token2 in query_tokens:\n",
    "                # Add the bigram with a weight based on frequency\n",
    "                weight = min(3, int(np.log10(freq + 1)) + 1)\n",
    "                weighted_tokens.extend([bigram] * weight)\n",
    "        \n",
    "        return weighted_tokens\n",
    "    \n",
    "    def recommend(self, query, top_n=10):\n",
    "\n",
    "        # Preprocess the query\n",
    "        processed_query = preprocess_text(query)\n",
    "        \n",
    "        # Tokenize and lemmatize\n",
    "        query_tokens = tokenize_and_lemmatize(processed_query)\n",
    "        \n",
    "        if not query_tokens:\n",
    "            print(\"Query is too short or only contains stopwords.\")\n",
    "            return pd.DataFrame(columns=['id', 'title', 'selftext', 'subreddit', 'score', 'url', 'relevance_score'])\n",
    "        \n",
    "        # Print tokens for debugging\n",
    "        print(f\"Query tokens: {query_tokens}\")\n",
    "        \n",
    "        # Expand the query with related terms\n",
    "        print(\"Expanding query with related terms...\")\n",
    "        expanded_query_tokens = self.expand_query(query_tokens)\n",
    "        \n",
    "        # Convert the expanded query tokens back to a string\n",
    "        expanded_query = ' '.join(expanded_query_tokens)\n",
    "        \n",
    "        # Vectorize the query\n",
    "        print(\"Vectorizing query...\")\n",
    "        query_vector = self.vectorizer.transform([expanded_query])\n",
    "        \n",
    "        # Calculate cosine similarity between query and all posts\n",
    "        print(\"Calculating similarities to corpus documents...\")\n",
    "        cosine_similarities = cosine_similarity(query_vector, self.tfidf_matrix).flatten()\n",
    "        \n",
    "        # Add similarity scores\n",
    "        temp_df = self.df.copy()\n",
    "        temp_df['relevance_score'] = cosine_similarities\n",
    "        \n",
    "        # Get top N recommendations\n",
    "        print(f\"Finding top {top_n} most relevant posts...\")\n",
    "        recommendations = temp_df.sort_values('relevance_score', ascending=False).head(top_n)\n",
    "        \n",
    "        # Select columns to return\n",
    "        result = recommendations[['id', 'title', 'selftext', 'subreddit', 'score', 'url', 'relevance_score']]\n",
    "        \n",
    "        # Round relevance score for readability\n",
    "        result['relevance_score'] = result['relevance_score'].round(3)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def search(self, query, top_n=10):\n",
    "\n",
    "        recommendations = self.recommend(query, top_n)\n",
    "        \n",
    "        print(f\"\\nTop {top_n} Recommendations for query: {query}\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        if recommendations.empty:\n",
    "            print(\"No relevant results found.\")\n",
    "            return recommendations\n",
    "        \n",
    "        for i, row in recommendations.iterrows():\n",
    "            # Format the selftext to show a preview\n",
    "            selftext = row['selftext'] if pd.notna(row['selftext']) else \"\"\n",
    "            selftext_preview = selftext[:150] + \"...\" if len(str(selftext)) > 150 else selftext\n",
    "            \n",
    "            print(f\"{i+1}. [{row['relevance_score']:.3f}] {row['title']}\")\n",
    "            print(f\"   {selftext_preview}\")\n",
    "            print(f\"   subreddit: r/{row['subreddit']}, score: {row['score']}\")\n",
    "            print(f\"   URL: {row['url']}\")\n",
    "            print(\"-\" * 80)\n",
    "        \n",
    "        return recommendations\n",
    "\n",
    "\n",
    "def analyze_and_save_corpus(csv_file='merged_reddit_only.csv', output_file='corpus_analysis.pkl'):\n",
    "    \n",
    "    print(f\"Loading data from {csv_file}...\")\n",
    "    df = pd.read_csv(csv_file)\n",
    "    print(\"Initializing corpus analysis...\")\n",
    "    corpus_analysis = RedditJobCorpusAnalysis(df)\n",
    "    \n",
    "    print(f\"Saving corpus analysis to {output_file}...\")\n",
    "    with open(output_file, 'wb') as f:\n",
    "        pickle.dump(corpus_analysis, f)\n",
    "    \n",
    "    print(\"Analysis complete and saved!\")\n",
    "    return corpus_analysis\n",
    "\n",
    "def load_corpus(corpus_file='corpus_analysis.pkl'):\n",
    "\n",
    "    print(f\"Loading corpus analysis from {corpus_file}...\")\n",
    "    with open(corpus_file, 'rb') as f:\n",
    "        corpus_analysis = pickle.load(f)\n",
    "    return corpus_analysis\n",
    "\n",
    "\n",
    "# Example usage with corpus persistence\n",
    "if __name__ == \"__main__\":\n",
    "    corpus_file = 'corpus_analysis.pkl'\n",
    "    \n",
    "    # Check if saved analysis exists\n",
    "    if os.path.exists(corpus_file):\n",
    "        print(f\"Loading existing corpus analysis from {corpus_file}...\")\n",
    "        with open(corpus_file, 'rb') as f:\n",
    "            corpus_analysis = pickle.load(f)\n",
    "        print(\"Corpus loaded successfully!\")\n",
    "    else:\n",
    "        print(\"No saved corpus analysis found. Creating new one...\")\n",
    "        # Load data from CSV\n",
    "        df = pd.read_csv('merged_reddit_only.csv')\n",
    "        print(f\"Loaded {len(df)} posts from CSV.\")\n",
    "        print(\"\\nInitializing corpus analysis...\")\n",
    "        corpus_analysis = RedditJobCorpusAnalysis(df)\n",
    "        \n",
    "        # Save for future use\n",
    "        print(f\"Saving corpus analysis to {corpus_file}...\")\n",
    "        with open(corpus_file, 'wb') as f:\n",
    "            pickle.dump(corpus_analysis, f)\n",
    "        print(\"Corpus analysis saved successfully!\")\n",
    "    \n",
    "    # Create recommender\n",
    "    basic_recommender = DataDrivenRedditJobRecommender(corpus_analysis)\n",
    "    \n",
    "    # Example queries\n",
    "    print(\"\\n=== Testing Recommender ===\")\n",
    "    queries = [\n",
    "        \"I am a JHU graduate student in data science, looking for internships in machine learning\",\n",
    "        \"Senior software engineer with React and Node.js experience\",\n",
    "        \"Entry level Python developer positions in Baltimore\"\n",
    "    ]\n",
    "    \n",
    "    for i, query in enumerate(queries):\n",
    "        print(f\"\\n\\nTesting query {i+1}/{len(queries)}: {query}\")\n",
    "        recommendations = basic_recommender.search(query, top_n=5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
