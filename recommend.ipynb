{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6facc2b5-1c15-4978-a1e9-3adb10ffdade",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\23109\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\23109\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\23109\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing corpus analysis (this only happens once)...\n",
      "Preprocessing corpus text...\n",
      "Analyzing corpus to identify important terms...\n",
      "Identified 0 additional domain-specific stopwords\n",
      "Extracting common bigrams...\n",
      "Extracted 300 significant bigrams\n",
      "Performing topic modeling...\n",
      "Corpus analysis complete!\n",
      "Creating TF-IDF vectors...\n",
      "Vocabulary size: 5000\n",
      "TF-IDF matrix shape: (113411, 5000)\n",
      "Recommendation system ready!\n",
      "Top 30 terms by importance:\n",
      "  and: 0.0523\n",
      "  to: 0.0501\n",
      "  the: 0.0442\n",
      "  in: 0.0407\n",
      "  for: 0.0371\n",
      "  gesucht: 0.0338\n",
      "  hiring: 0.0315\n",
      "  engineer: 0.0281\n",
      "  of: 0.0270\n",
      "  job: 0.0264\n",
      "  is: 0.0253\n",
      "  you: 0.0219\n",
      "  remote: 0.0216\n",
      "  my: 0.0209\n",
      "  usd: 0.0204\n",
      "  it: 0.0195\n",
      "  with: 0.0194\n",
      "  at: 0.0178\n",
      "  senior: 0.0171\n",
      "  software: 0.0169\n",
      "  or: 0.0166\n",
      "  on: 0.0166\n",
      "  jobs: 0.0166\n",
      "  apply: 0.0164\n",
      "  have: 0.0163\n",
      "  more: 0.0155\n",
      "  that: 0.0153\n",
      "  me: 0.0139\n",
      "  this: 0.0137\n",
      "  are: 0.0136\n",
      "\n",
      "Example topics discovered:\n",
      "  Topic 1: to, my, the, it, and, in, but, have, that, of\n",
      "  Topic 2: usd, engineer, ca, us, software, san, senior, remote, francisco, staff\n",
      "  Topic 3: amazon, id, job, jobs, learn, see, please, engineer, apply, more\n",
      "\n",
      "=== Testing Basic Recommender ===\n",
      "\n",
      "\n",
      "Testing query: I am a JHU graduate student in data science, looking for internships in machine learning\n",
      "Query tokens: ['jhu', 'graduate', 'student', 'data', 'science', 'internship', 'machine', 'learning']\n",
      "\n",
      "Top 5 Recommendations for query: I am a JHU graduate student in data science, looking for internships in machine learning\n",
      "================================================================================\n",
      "32577. [0.836] 20 New Data Science, Data Engineering and Machine Learning jobs\n",
      "   |Job Title|Company|Location|Country|Skills|\n",
      "|:-|:-|:-|:-|:-|\n",
      "|[Data Engineer -   ETL , SQL , Python , pyspark , G...](https://datayoshi.com/offer/1677...\n",
      "   subreddit: r/BigDataJobs, score: 1\n",
      "   URL: https://www.reddit.com/r/BigDataJobs/comments/1glvlk6/20_new_data_science_data_engineering_and_machine/\n",
      "--------------------------------------------------------------------------------\n",
      "32587. [0.825] 24 New Data Science, Data Engineering and Machine Learning jobs\n",
      "   |Job Title|Company|Location|Country|Skills|\n",
      "|:-|:-|:-|:-|:-|\n",
      "|[Data Analyst / Power BI Consultant (w/m/d)](https://datayoshi.com/offer/466684/data-ana...\n",
      "   subreddit: r/BigDataJobs, score: 2\n",
      "   URL: https://www.reddit.com/r/BigDataJobs/comments/1gl3lds/24_new_data_science_data_engineering_and_machine/\n",
      "--------------------------------------------------------------------------------\n",
      "32598. [0.815] 22 New Data Science, Data Engineering and Machine Learning jobs\n",
      "   |Job Title|Company|Location|Country|Skills|\n",
      "|:-|:-|:-|:-|:-|\n",
      "|[Data Engineer IoT (m/w/d) 95% remote ID01255 (free...](https://datayoshi.com/offer/2539...\n",
      "   subreddit: r/BigDataJobs, score: 1\n",
      "   URL: https://www.reddit.com/r/BigDataJobs/comments/1gkbtir/22_new_data_science_data_engineering_and_machine/\n",
      "--------------------------------------------------------------------------------\n",
      "32629. [0.811] 22 New Data Science, Data Engineering and Machine Learning jobs\n",
      "   |Job Title|Company|Location|Country|Skills|\n",
      "|:-|:-|:-|:-|:-|\n",
      "|[Data Engineer](https://datayoshi.com/offer/692053/data-engineer)|[PayPal](https://www.d...\n",
      "   subreddit: r/BigDataJobs, score: 1\n",
      "   URL: https://www.reddit.com/r/BigDataJobs/comments/1ghalqs/22_new_data_science_data_engineering_and_machine/\n",
      "--------------------------------------------------------------------------------\n",
      "32446. [0.802] 22 New Data Science, Data Engineering and Machine Learning jobs\n",
      "   |Job Title|Company|Location|Country|Skills|\n",
      "|:-|:-|:-|:-|:-|\n",
      "|[Data Engineer](https://datayoshi.com/offer/475167/data-engineer)|[Anthony Nolan](https:...\n",
      "   subreddit: r/BigDataJobs, score: 1\n",
      "   URL: https://www.reddit.com/r/BigDataJobs/comments/1gwkilv/22_new_data_science_data_engineering_and_machine/\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Testing query: Senior software engineer with React and Node.js experience\n",
      "Query tokens: ['senior', 'engineer', 'react', 'node']\n",
      "\n",
      "Top 5 Recommendations for query: Senior software engineer with React and Node.js experience\n",
      "================================================================================\n",
      "89277. [0.760] [HIRING] Argent is hiring Senior Solidity Engineer\n",
      "   \n",
      "   subreddit: r/blockchain_jobs, score: 1\n",
      "   URL: https://cjl.ist/2WWR01W?ref=blockchain_jobs\n",
      "--------------------------------------------------------------------------------\n",
      "81737. [0.760] [HIRING] Argent is hiring Senior Solidity Engineer\n",
      "   \n",
      "   subreddit: r/blockchainJobs, score: 1\n",
      "   URL: https://cjl.ist/2WWR01W?ref=blockchainJobs\n",
      "--------------------------------------------------------------------------------\n",
      "89572. [0.727] [HIRING] QUANTA is hiring Software Engineer\n",
      "   \n",
      "   subreddit: r/blockchain_jobs, score: 1\n",
      "   URL: https://cjl.ist/2O5dg7P?ref=blockchain_jobs\n",
      "--------------------------------------------------------------------------------\n",
      "89209. [0.727] [HIRING] Omise is hiring Software Engineer\n",
      "   \n",
      "   subreddit: r/blockchain_jobs, score: 1\n",
      "   URL: https://cjl.ist/2TwxPfk?ref=blockchain_jobs\n",
      "--------------------------------------------------------------------------------\n",
      "68588. [0.727] Need job as a Telecom or IT Engineer\n",
      "   \n",
      "   subreddit: r/JobsUAE, score: 2\n",
      "   URL: https://www.reddit.com/r/JobsUAE/comments/18puyv9/need_job_as_a_telecom_or_it_engineer/\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Testing query: Entry level Python developer positions in Baltimore\n",
      "Query tokens: ['entry', 'level', 'python', 'baltimore']\n",
      "\n",
      "Top 5 Recommendations for query: Entry level Python developer positions in Baltimore\n",
      "================================================================================\n",
      "26705. [0.967] Entry level Abcellera Job\n",
      "   [https://abcellera.com/careers-openings/?gh\\_jid=6236994003](https://abcellera.com/careers-openings/?gh_jid=6236994003)\n",
      "   subreddit: r/VancouverJobs, score: 13\n",
      "   URL: https://www.reddit.com/r/VancouverJobs/comments/1g4g9oe/entry_level_abcellera_job/\n",
      "--------------------------------------------------------------------------------\n",
      "37652. [0.967] Hiring U.S Steel Entry Level\n",
      "   https://sjobs.brassring.com/TGnewUI/Search/home/HomeWithPreLoad?PageType=JobDetails&partnerid=25307&siteid=5238&Areq=12945BR#jobDetails=3292006_5238\n",
      "   subreddit: r/houstonjobs, score: 2\n",
      "   URL: https://www.reddit.com/r/houstonjobs/comments/1i0q9hn/hiring_us_steel_entry_level/\n",
      "--------------------------------------------------------------------------------\n",
      "60549. [0.967] Entry-Level Software Developer\n",
      "   \n",
      "   subreddit: r/HawaiiJobs, score: 5\n",
      "   URL: https://jobs.hawaiitech.com/job/pacxa-honolulu-3-entry-level-software-developer/\n",
      "--------------------------------------------------------------------------------\n",
      "54110. [0.967] Entry Level\n",
      "   \n",
      "   subreddit: r/HuntsvilleAlabamaJobs, score: 1\n",
      "   URL: https://akamerica.wd5.myworkdayjobs.com/aka/job/Athens-AL/Quality-Engineer_R318-2?source=indeed_sponsored_custom\n",
      "--------------------------------------------------------------------------------\n",
      "70856. [0.858] Any entry-level opportunities available? :)\n",
      "   \n",
      "   subreddit: r/linux_jobs, score: 3\n",
      "   URL: https://www.reddit.com/r/linux_jobs/comments/10tfaug/any_entrylevel_opportunities_available/\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "=== Testing Enhanced Recommender ===\n",
      "Query tokens: ['jhu', 'graduate', 'student', 'data', 'science', 'internship', 'machine', 'learning']\n",
      "\n",
      "Top 5 Recommendations for query: I am a JHU graduate student in data science, looking for internships in machine learning\n",
      "================================================================================\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported format string passed to Series.__format__",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 532\u001b[0m\n\u001b[0;32m    530\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== Testing Enhanced Recommender ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    531\u001b[0m enhanced_recommender \u001b[38;5;241m=\u001b[39m recommenders[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menhanced\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m--> 532\u001b[0m recommendations \u001b[38;5;241m=\u001b[39m enhanced_recommender\u001b[38;5;241m.\u001b[39msearch(queries[\u001b[38;5;241m0\u001b[39m], top_n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m    534\u001b[0m \u001b[38;5;66;03m# Show key insights from the corpus\u001b[39;00m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== Top Terms in the Corpus ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 375\u001b[0m, in \u001b[0;36mDataDrivenRedditJobRecommender.search\u001b[1;34m(self, query, top_n)\u001b[0m\n\u001b[0;32m    372\u001b[0m selftext \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mselftext\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mnotna(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mselftext\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    373\u001b[0m selftext_preview \u001b[38;5;241m=\u001b[39m selftext[:\u001b[38;5;241m150\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mstr\u001b[39m(selftext)) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m150\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m selftext\n\u001b[1;32m--> 375\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelevance_score\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    376\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mselftext_preview\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   subreddit: r/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubreddit\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported format string passed to Series.__format__"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Download NLTK resources (uncomment these lines first time you run the script)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Text preprocessing\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Clean and preprocess text data for NLP tasks.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    \n",
    "    # Remove special characters and replace with space\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Basic stopwords - we'll enhance these based on corpus analysis\n",
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "CS_BASIC_STOPWORDS = {\n",
    "    'job', 'jobs', 'looking', 'work', 'position', 'positions', 'company', 'companies',\n",
    "    'experience', 'programming', 'code', 'coding', 'developer', 'development', 'software',\n",
    "    'career', 'careers', 'role', 'roles', 'opportunity', 'opportunities', 'hiring',\n",
    "    'apply', 'application', 'interview', 'requirement', 'requirements', 'skill', 'skills',\n",
    "    'year', 'years', 'month', 'months', 'week', 'weeks', 'day', 'days',\n",
    "    'like', 'want', 'need', 'would', 'could', 'should', 'may', 'might', 'must',\n",
    "    'know', 'get', 'got', 'go', 'going', 'think', 'thought', 'see', 'look', 'help'\n",
    "}\n",
    "STOP_WORDS.update(CS_BASIC_STOPWORDS)\n",
    "\n",
    "# Tokenize and lemmatize\n",
    "def tokenize_and_lemmatize(text):\n",
    "    \"\"\"\n",
    "    Tokenize and lemmatize text, removing stopwords.\n",
    "    \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Lemmatize and remove stopwords\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens \n",
    "              if token not in STOP_WORDS and len(token) > 2]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# This class holds preprocessed data and analysis results, separated from the recommender logic\n",
    "class RedditJobCorpusAnalysis:\n",
    "    def __init__(self, df):\n",
    "        \"\"\"\n",
    "        Analyze a Reddit job post corpus to extract key insights.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        df : pandas.DataFrame\n",
    "            DataFrame containing Reddit posts with columns like 'id', 'title', 'selftext', etc.\n",
    "        \"\"\"\n",
    "        self.df = df.copy()\n",
    "        self.vectorizer = None\n",
    "        self.tfidf_matrix = None\n",
    "        self.term_importance = {}\n",
    "        self.common_bigrams = {}\n",
    "        self.topics = {}\n",
    "        self.additional_stopwords = set()\n",
    "        \n",
    "        # Run preprocessing and analysis\n",
    "        self._preprocess_data()\n",
    "        self._analyze_corpus()\n",
    "        self._create_tfidf_matrix()\n",
    "        \n",
    "        # Print insights\n",
    "        self._print_insights()\n",
    "    \n",
    "    def _preprocess_data(self):\n",
    "        \"\"\"\n",
    "        Preprocess the data for analysis.\n",
    "        \"\"\"\n",
    "        print(\"Preprocessing corpus text...\")\n",
    "        \n",
    "        # Create combined text field (title + selftext)\n",
    "        self.df['combined_text'] = self.df['title'].fillna('') + ' ' + self.df['selftext'].fillna('')\n",
    "        \n",
    "        # Preprocess text\n",
    "        self.df['processed_text'] = self.df['combined_text'].apply(preprocess_text)\n",
    "    \n",
    "    def _analyze_corpus(self):\n",
    "        \"\"\"\n",
    "        Analyze the corpus to identify important terms, common bigrams,\n",
    "        and additional stopwords specific to this dataset.\n",
    "        \"\"\"\n",
    "        print(\"Analyzing corpus to identify important terms...\")\n",
    "        \n",
    "        # Extract the processed text\n",
    "        corpus = self.df['processed_text'].tolist()\n",
    "        \n",
    "        # Find additional stopwords based on high document frequency\n",
    "        count_vec = CountVectorizer(min_df=5)\n",
    "        count_matrix = count_vec.fit_transform(corpus)\n",
    "        count_features = count_vec.get_feature_names_out()\n",
    "        \n",
    "        # Calculate document frequency for each term\n",
    "        doc_freq = np.array((count_matrix > 0).sum(axis=0)).flatten()\n",
    "        doc_freq_percent = doc_freq / len(corpus)\n",
    "        \n",
    "        # Terms that appear in more than 70% of documents might be domain-specific stopwords\n",
    "        potential_stopwords = {count_features[i] for i in range(len(count_features)) \n",
    "                               if doc_freq_percent[i] > 0.7}\n",
    "        \n",
    "        # But we should be careful not to remove important CS terms\n",
    "        # So let's check their TF-IDF scores\n",
    "        tfidf_vec = TfidfVectorizer(min_df=5, max_df=0.95)\n",
    "        tfidf_matrix = tfidf_vec.fit_transform(corpus)\n",
    "        tfidf_features = tfidf_vec.get_feature_names_out()\n",
    "        \n",
    "        # Calculate average TF-IDF for each term\n",
    "        avg_tfidf = np.array(tfidf_matrix.mean(axis=0)).flatten()\n",
    "        \n",
    "        # Only add to stopwords if the average TF-IDF is low (not very informative)\n",
    "        for term in potential_stopwords:\n",
    "            if term in tfidf_features:\n",
    "                idx = list(tfidf_features).index(term)\n",
    "                if avg_tfidf[idx] < 0.01:  # Low information content\n",
    "                    self.additional_stopwords.add(term)\n",
    "        \n",
    "        print(f\"Identified {len(self.additional_stopwords)} additional domain-specific stopwords\")\n",
    "        \n",
    "        # Update stopwords\n",
    "        global STOP_WORDS\n",
    "        STOP_WORDS.update(self.additional_stopwords)\n",
    "        \n",
    "        # Extract bigrams (phrases)\n",
    "        print(\"Extracting common bigrams...\")\n",
    "        bigram_vectorizer = CountVectorizer(ngram_range=(2, 2), min_df=5)\n",
    "        bigram_matrix = bigram_vectorizer.fit_transform(corpus)\n",
    "        bigram_features = bigram_vectorizer.get_feature_names_out()\n",
    "        \n",
    "        # Calculate bigram frequency\n",
    "        bigram_freq = np.array(bigram_matrix.sum(axis=0)).flatten()\n",
    "        \n",
    "        # Get top bigrams\n",
    "        top_bigram_indices = bigram_freq.argsort()[-300:][::-1]  # Top 300 bigrams\n",
    "        self.common_bigrams = {bigram_features[i]: bigram_freq[i] for i in top_bigram_indices}\n",
    "        \n",
    "        print(f\"Extracted {len(self.common_bigrams)} significant bigrams\")\n",
    "        \n",
    "        # Topic modeling using NMF\n",
    "        print(\"Performing topic modeling...\")\n",
    "        nmf_model = NMF(n_components=10, random_state=42)\n",
    "        nmf_topics = nmf_model.fit_transform(tfidf_matrix)\n",
    "        \n",
    "        # Get top terms for each topic\n",
    "        for topic_idx, topic in enumerate(nmf_model.components_):\n",
    "            top_terms_idx = topic.argsort()[-20:][::-1]  # Top 20 terms\n",
    "            top_terms = [tfidf_features[i] for i in top_terms_idx]\n",
    "            self.topics[f\"Topic {topic_idx+1}\"] = top_terms\n",
    "        \n",
    "        # Calculate TF-IDF importance for each term\n",
    "        self.term_importance = {}\n",
    "        for i, term in enumerate(tfidf_features):\n",
    "            self.term_importance[term] = avg_tfidf[i]\n",
    "        \n",
    "        # Sort by importance\n",
    "        self.term_importance = dict(sorted(self.term_importance.items(), \n",
    "                                           key=lambda x: x[1], \n",
    "                                           reverse=True))\n",
    "        \n",
    "        print(\"Corpus analysis complete!\")\n",
    "    \n",
    "    def _create_tfidf_matrix(self):\n",
    "        \"\"\"\n",
    "        Create TF-IDF matrix for the entire corpus.\n",
    "        \"\"\"\n",
    "        # Create a corpus of processed documents\n",
    "        corpus = self.df['processed_text'].tolist()\n",
    "        \n",
    "        # Initialize and fit TF-IDF vectorizer\n",
    "        print(\"Creating TF-IDF vectors...\")\n",
    "        self.vectorizer = TfidfVectorizer(\n",
    "            tokenizer=tokenize_and_lemmatize,\n",
    "            min_df=2,             # Ignore terms that appear in less than 2 documents\n",
    "            max_df=0.90,          # Ignore terms that appear in more than 90% of documents\n",
    "            max_features=5000,    # Limit vocabulary size\n",
    "            ngram_range=(1, 2)    # Use both unigrams and bigrams\n",
    "        )\n",
    "        \n",
    "        # Create TF-IDF matrix\n",
    "        self.tfidf_matrix = self.vectorizer.fit_transform(corpus)\n",
    "        \n",
    "        # Get feature names\n",
    "        feature_names = self.vectorizer.get_feature_names_out()\n",
    "        \n",
    "        print(f\"Vocabulary size: {len(feature_names)}\")\n",
    "        print(f\"TF-IDF matrix shape: {self.tfidf_matrix.shape}\")\n",
    "    \n",
    "    def _print_insights(self):\n",
    "        \"\"\"\n",
    "        Print key insights from the corpus analysis.\n",
    "        \"\"\"\n",
    "        print(\"Recommendation system ready!\")\n",
    "        \n",
    "        # Print top terms for debugging\n",
    "        top_terms = list(self.term_importance.items())[:30]\n",
    "        print(\"Top 30 terms by importance:\")\n",
    "        for term, score in top_terms:\n",
    "            print(f\"  {term}: {score:.4f}\")\n",
    "        \n",
    "        # Print example topics\n",
    "        print(\"\\nExample topics discovered:\")\n",
    "        for topic_name, terms in list(self.topics.items())[:3]:\n",
    "            print(f\"  {topic_name}: {', '.join(terms[:10])}\")\n",
    "    \n",
    "    def get_top_terms(self, n=50):\n",
    "        \"\"\"\n",
    "        Return the top n most important terms from the corpus.\n",
    "        \"\"\"\n",
    "        return list(self.term_importance.items())[:n]\n",
    "    \n",
    "    def get_topics(self):\n",
    "        \"\"\"\n",
    "        Return the topics discovered in the corpus.\n",
    "        \"\"\"\n",
    "        return self.topics\n",
    "\n",
    "\n",
    "# Base recommender class that uses an existing corpus analysis\n",
    "class DataDrivenRedditJobRecommender:\n",
    "    def __init__(self, corpus_analysis):\n",
    "        \"\"\"\n",
    "        Initialize the recommender with a corpus analysis object.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        corpus_analysis : RedditJobCorpusAnalysis\n",
    "            A complete corpus analysis with preprocessed data and TF-IDF matrix\n",
    "        \"\"\"\n",
    "        self.df = corpus_analysis.df\n",
    "        self.vectorizer = corpus_analysis.vectorizer\n",
    "        self.tfidf_matrix = corpus_analysis.tfidf_matrix\n",
    "        self.term_importance = corpus_analysis.term_importance\n",
    "        self.common_bigrams = corpus_analysis.common_bigrams\n",
    "        self.topics = corpus_analysis.topics\n",
    "    \n",
    "    def expand_query(self, query_tokens):\n",
    "        \"\"\"\n",
    "        Expand the query with related terms based on corpus analysis.\n",
    "        \"\"\"\n",
    "        expanded_tokens = query_tokens.copy()\n",
    "        \n",
    "        # Expand with topic-related terms\n",
    "        for token in query_tokens:\n",
    "            # Check if token appears in any topic\n",
    "            for topic, terms in self.topics.items():\n",
    "                if token in terms:\n",
    "                    # Add some related terms from the same topic\n",
    "                    related_terms = [t for t in terms[:10] if t != token]\n",
    "                    expanded_tokens.extend(related_terms[:3])  # Add up to 3 related terms\n",
    "        \n",
    "        # Weight tokens by importance\n",
    "        weighted_tokens = []\n",
    "        for token in expanded_tokens:\n",
    "            # If token is in term_importance, weight it accordingly\n",
    "            if token in self.term_importance:\n",
    "                weight = min(5, int(self.term_importance[token] * 100) + 1)\n",
    "                weighted_tokens.extend([token] * weight)\n",
    "            else:\n",
    "                weighted_tokens.append(token)\n",
    "        \n",
    "        # Add common bigrams that contain query tokens\n",
    "        for bigram, freq in self.common_bigrams.items():\n",
    "            token1, token2 = bigram.split()\n",
    "            if token1 in query_tokens or token2 in query_tokens:\n",
    "                # Add the bigram with a weight based on frequency\n",
    "                weight = min(3, int(np.log10(freq + 1)) + 1)\n",
    "                weighted_tokens.extend([bigram] * weight)\n",
    "        \n",
    "        return weighted_tokens\n",
    "    \n",
    "    def recommend(self, query, top_n=10):\n",
    "        \"\"\"\n",
    "        Recommend posts based on a query string.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        query : str\n",
    "            The query string (can be a long sentence)\n",
    "        top_n : int\n",
    "            Number of recommendations to return\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pandas.DataFrame\n",
    "            DataFrame with top_n recommendations and relevance scores\n",
    "        \"\"\"\n",
    "        # Preprocess the query\n",
    "        processed_query = preprocess_text(query)\n",
    "        \n",
    "        # Tokenize and lemmatize\n",
    "        query_tokens = tokenize_and_lemmatize(processed_query)\n",
    "        \n",
    "        if not query_tokens:\n",
    "            print(\"Warning: Query is too short or only contains stopwords.\")\n",
    "            return pd.DataFrame(columns=['id', 'title', 'selftext', 'subreddit', 'score', 'url', 'relevance_score'])\n",
    "        \n",
    "        # Print tokens for debugging\n",
    "        print(f\"Query tokens: {query_tokens}\")\n",
    "        \n",
    "        # Expand the query with related terms\n",
    "        expanded_query_tokens = self.expand_query(query_tokens)\n",
    "        \n",
    "        # Convert the expanded query tokens back to a string\n",
    "        expanded_query = ' '.join(expanded_query_tokens)\n",
    "        \n",
    "        # Vectorize the query\n",
    "        query_vector = self.vectorizer.transform([expanded_query])\n",
    "        \n",
    "        # Calculate cosine similarity between query and all posts\n",
    "        cosine_similarities = cosine_similarity(query_vector, self.tfidf_matrix).flatten()\n",
    "        \n",
    "        # Add similarity scores to a temporary dataframe (to avoid modifying the original)\n",
    "        temp_df = self.df.copy()\n",
    "        temp_df['relevance_score'] = cosine_similarities\n",
    "        \n",
    "        # Get top N recommendations\n",
    "        recommendations = temp_df.sort_values('relevance_score', ascending=False).head(top_n)\n",
    "        \n",
    "        # Select columns to return\n",
    "        result = recommendations[['id', 'title', 'selftext', 'subreddit', 'score', 'url', 'relevance_score']]\n",
    "        \n",
    "        # Round relevance score for readability\n",
    "        result['relevance_score'] = result['relevance_score'].round(3)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def search(self, query, top_n=10):\n",
    "        \"\"\"\n",
    "        Alias for recommend method with formatted output for display.\n",
    "        \"\"\"\n",
    "        recommendations = self.recommend(query, top_n)\n",
    "        \n",
    "        print(f\"\\nTop {top_n} Recommendations for query: {query}\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        if recommendations.empty:\n",
    "            print(\"No relevant results found.\")\n",
    "            return recommendations\n",
    "        \n",
    "        for i, row in recommendations.iterrows():\n",
    "            # Format the selftext to show a preview\n",
    "            selftext = row['selftext'] if pd.notna(row['selftext']) else \"\"\n",
    "            selftext_preview = selftext[:150] + \"...\" if len(str(selftext)) > 150 else selftext\n",
    "            \n",
    "            print(f\"{i+1}. [{row['relevance_score']:.3f}] {row['title']}\")\n",
    "            print(f\"   {selftext_preview}\")\n",
    "            print(f\"   subreddit: r/{row['subreddit']}, score: {row['score']}\")\n",
    "            print(f\"   URL: {row['url']}\")\n",
    "            print(\"-\" * 80)\n",
    "        \n",
    "        return recommendations\n",
    "\n",
    "\n",
    "# Enhanced recommender that considers score and recency\n",
    "class EnhancedDataDrivenRecommender(DataDrivenRedditJobRecommender):\n",
    "    def recommend(self, query, top_n=10, relevance_weight=0.7, score_weight=0.15, recency_weight=0.15):\n",
    "        \"\"\"\n",
    "        Enhanced recommendation that considers relevance, post score, and recency.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        query : str\n",
    "            The query string\n",
    "        top_n : int\n",
    "            Number of recommendations to return\n",
    "        relevance_weight : float\n",
    "            Weight given to text relevance (0-1)\n",
    "        score_weight : float\n",
    "            Weight given to post score (0-1)\n",
    "        recency_weight : float\n",
    "            Weight given to post recency (0-1)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pandas.DataFrame\n",
    "            DataFrame with top_n recommendations and scores\n",
    "        \"\"\"\n",
    "        # Get base recommendations with higher initial pool\n",
    "        initial_pool_size = min(top_n * 3, len(self.df))\n",
    "        base_recommendations = super().recommend(query, initial_pool_size)\n",
    "        \n",
    "        if base_recommendations.empty:\n",
    "            return base_recommendations\n",
    "        \n",
    "        # Normalize post score\n",
    "        max_score = base_recommendations['score'].max()\n",
    "        if max_score > 0:\n",
    "            base_recommendations['score_normalized'] = base_recommendations['score'] / max_score\n",
    "        else:\n",
    "            base_recommendations['score_normalized'] = 0\n",
    "        \n",
    "        # Check if created_utc exists before trying to use it\n",
    "        has_created_utc = 'created_utc' in base_recommendations.columns\n",
    "        \n",
    "        if has_created_utc:\n",
    "            try:\n",
    "                # Convert to datetime\n",
    "                base_recommendations['created_dt'] = pd.to_datetime(base_recommendations['created_utc'], unit='s')\n",
    "                \n",
    "                max_date = base_recommendations['created_dt'].max()\n",
    "                min_date = base_recommendations['created_dt'].min()\n",
    "                date_range = (max_date - min_date).total_seconds()\n",
    "                \n",
    "                if date_range > 0:\n",
    "                    base_recommendations['recency_score'] = base_recommendations.apply(\n",
    "                        lambda x: (x['created_dt'] - min_date).total_seconds() / date_range, axis=1\n",
    "                    )\n",
    "                else:\n",
    "                    base_recommendations['recency_score'] = 1.0\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not process recency from created_utc: {e}\")\n",
    "                has_created_utc = False\n",
    "        \n",
    "        if not has_created_utc:\n",
    "            # If no created_utc field or error processing it, don't use recency\n",
    "            base_recommendations['recency_score'] = 0\n",
    "            \n",
    "            # Redistribute weights\n",
    "            total = relevance_weight + score_weight\n",
    "            relevance_weight = relevance_weight / total\n",
    "            score_weight = score_weight / total\n",
    "            recency_weight = 0\n",
    "        \n",
    "        # Calculate combined score\n",
    "        base_recommendations['combined_score'] = (\n",
    "            (relevance_weight * base_recommendations['relevance_score']) +\n",
    "            (score_weight * base_recommendations['score_normalized']) +\n",
    "            (recency_weight * base_recommendations['recency_score'])\n",
    "        )\n",
    "        \n",
    "        # Get top N by combined score\n",
    "        final_recommendations = base_recommendations.sort_values('combined_score', ascending=False).head(top_n)\n",
    "        \n",
    "        # Rename and round for output\n",
    "        final_recommendations = final_recommendations.rename(columns={'combined_score': 'relevance_score'})\n",
    "        final_recommendations['relevance_score'] = final_recommendations['relevance_score'].round(3)\n",
    "        \n",
    "        # Select columns to return\n",
    "        cols = ['id', 'title', 'selftext', 'subreddit', 'score', 'url', 'relevance_score']\n",
    "        result = final_recommendations[cols]\n",
    "        \n",
    "        return result\n",
    "\n",
    "\n",
    "# Factory function to create both recommender types with shared analysis\n",
    "def create_recommenders(df):\n",
    "    \"\"\"\n",
    "    Factory function that creates both basic and enhanced recommenders,\n",
    "    sharing the preprocessing and analysis work.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame containing Reddit posts\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary containing both recommender instances\n",
    "    \"\"\"\n",
    "    print(\"\\nInitializing corpus analysis (this only happens once)...\")\n",
    "    \n",
    "    # Perform corpus analysis once\n",
    "    corpus_analysis = RedditJobCorpusAnalysis(df)\n",
    "    \n",
    "    # Create both recommender types using the same analysis\n",
    "    basic = DataDrivenRedditJobRecommender(corpus_analysis)\n",
    "    enhanced = EnhancedDataDrivenRecommender(corpus_analysis)\n",
    "    \n",
    "    return {\n",
    "        'basic': basic,\n",
    "        'enhanced': enhanced\n",
    "    }\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Load data from CSV\n",
    "    df = pd.read_csv('posts_job.csv')\n",
    "    \n",
    "    # Create both recommender types with shared analysis\n",
    "    recommenders = create_recommenders(df)\n",
    "    \n",
    "    # Get the basic recommender\n",
    "    basic_recommender = recommenders['basic']\n",
    "    \n",
    "    # Example queries\n",
    "    print(\"\\n=== Testing Basic Recommender ===\")\n",
    "    queries = [\n",
    "        \"I am a JHU graduate student in data science, looking for internships in machine learning\",\n",
    "        \"Senior software engineer with React and Node.js experience\",\n",
    "        \"Entry level Python developer positions in Baltimore\"\n",
    "    ]\n",
    "    \n",
    "    for query in queries:\n",
    "        print(f\"\\n\\nTesting query: {query}\")\n",
    "        recommendations = basic_recommender.search(query, top_n=5)\n",
    "    \n",
    "    # Use the enhanced recommender with the same corpus analysis\n",
    "    print(\"\\n\\n=== Testing Enhanced Recommender ===\")\n",
    "    enhanced_recommender = recommenders['enhanced']\n",
    "    recommendations = enhanced_recommender.search(queries[0], top_n=5)\n",
    "    \n",
    "    # Show key insights from the corpus\n",
    "    print(\"\\n\\n=== Top Terms in the Corpus ===\")\n",
    "    corpus_analysis = recommenders['basic'].term_importance\n",
    "    top_terms = list(corpus_analysis.items())[:20]\n",
    "    for term, score in top_terms:\n",
    "        print(f\"{term}: {score:.4f}\")\n",
    "    \n",
    "    print(\"\\n=== Key Topics Discovered ===\")\n",
    "    topics = recommenders['basic'].topics\n",
    "    for topic_name, terms in list(topics.items())[:5]:\n",
    "        print(f\"{topic_name}: {', '.join(terms[:10])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808bdf6a-5895-43ae-b066-747362e09a73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5368a4c-2ff5-42c2-819a-1263db61cf28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
