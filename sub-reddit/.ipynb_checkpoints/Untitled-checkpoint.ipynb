{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a120b91-d9b9-4472-9c3f-f25a7a623ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-28 15:17:29,663 - INFO - Accessing full communities search for keyword: jobs\n",
      "2025-04-28 15:17:29,663 - INFO - Search URL: https://www.reddit.com/search/?q=jobs&type=communities\n",
      "2025-04-28 15:17:34,619 - INFO - Initially found 10 communities\n",
      "2025-04-28 15:17:38,566 - INFO - Scroll 1: Found 9 new communities (total: 19)\n",
      "2025-04-28 15:17:43,063 - INFO - Scroll 2: Found 10 new communities (total: 29)\n",
      "2025-04-28 15:17:48,242 - INFO - Scroll 3: Found 10 new communities (total: 39)\n",
      "2025-04-28 15:17:53,913 - INFO - Scroll 4: Found 10 new communities (total: 49)\n",
      "2025-04-28 15:18:00,020 - INFO - Scroll 5: Found 10 new communities (total: 59)\n",
      "2025-04-28 15:18:06,678 - INFO - Scroll 6: Found 10 new communities (total: 69)\n",
      "2025-04-28 15:18:13,628 - INFO - Scroll 7: Found 10 new communities (total: 79)\n",
      "2025-04-28 15:18:21,260 - INFO - Scroll 8: Found 10 new communities (total: 89)\n",
      "2025-04-28 15:18:29,303 - INFO - Scroll 9: Found 10 new communities (total: 99)\n",
      "2025-04-28 15:18:38,030 - INFO - Scroll 10: Found 10 new communities (total: 109)\n",
      "2025-04-28 15:18:46,940 - INFO - Scroll 11: Found 9 new communities (total: 118)\n",
      "2025-04-28 15:18:56,619 - INFO - Scroll 12: Found 10 new communities (total: 128)\n",
      "2025-04-28 15:19:06,689 - INFO - Scroll 13: Found 10 new communities (total: 138)\n",
      "2025-04-28 15:19:17,674 - INFO - Scroll 14: Found 9 new communities (total: 147)\n",
      "2025-04-28 15:19:28,975 - INFO - Scroll 15: Found 10 new communities (total: 157)\n",
      "2025-04-28 15:19:40,612 - INFO - Scroll 16: Found 10 new communities (total: 167)\n",
      "2025-04-28 15:19:52,895 - INFO - Scroll 17: Found 10 new communities (total: 177)\n",
      "2025-04-28 15:20:05,647 - INFO - Scroll 18: Found 9 new communities (total: 186)\n",
      "2025-04-28 15:20:21,794 - INFO - Scroll 19: Found 10 new communities (total: 196)\n",
      "2025-04-28 15:20:46,172 - INFO - Scroll 20: Found 10 new communities (total: 206)\n",
      "2025-04-28 15:21:00,543 - INFO - Scroll 21: Found 10 new communities (total: 216)\n",
      "2025-04-28 15:21:15,830 - INFO - Scroll 22: Found 7 new communities (total: 223)\n",
      "2025-04-28 15:21:31,044 - INFO - Scroll 23: Found 10 new communities (total: 233)\n",
      "2025-04-28 15:21:46,649 - INFO - Scroll 24: Found 10 new communities (total: 243)\n",
      "2025-04-28 15:21:49,657 - INFO - No height change after scroll 25 - attempt 1/3\n",
      "2025-04-28 15:21:52,669 - INFO - No height change after scroll 26 - attempt 2/3\n",
      "2025-04-28 15:21:55,678 - INFO - No height change after scroll 27 - attempt 3/3\n",
      "2025-04-28 15:21:55,679 - INFO - No new communities found for 3 consecutive scrolls - stopping\n",
      "2025-04-28 15:22:14,218 - INFO - Total communities found after 27 scrolls: 243\n",
      "2025-04-28 15:22:14,224 - INFO - Saved 243 communities to jobs_communities.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 243 communities related to the search:\n",
      "------------------------------------------------------------\n",
      "1. r/jobs\n",
      "   Description: /r/jobs is the number one community for advice relating to your career. Head to our discord for live...\n",
      "   Members: 2M, Online: 535\n",
      "   URL: https://www.reddit.com/r/jobs/\n",
      "------------------------------------------------------------\n",
      "2. r/RemoteJobs\n",
      "   Description: Remote jobs: the future of work! READ RULES BEFORE POSTING! :) This is a place to discuss remote wor...\n",
      "   Members: 273K, Online: 46\n",
      "   URL: https://www.reddit.com/r/RemoteJobs/\n",
      "------------------------------------------------------------\n",
      "3. r/DesignJobs\n",
      "   Description: Some redditors are skilled professionals, some redditors need skilled professionals. Scroll down for...\n",
      "   Members: 163K, Online: 26\n",
      "   URL: https://www.reddit.com/r/DesignJobs/\n",
      "------------------------------------------------------------\n",
      "4. r/AskReddit\n",
      "   Description: r/AskReddit is the place to ask and answer thought-provoking questions.\n",
      "   Members: 55M, Online: 11K\n",
      "   URL: https://www.reddit.com/r/AskReddit/\n",
      "------------------------------------------------------------\n",
      "5. r/antiwork\n",
      "   Description: A subreddit for those who want to end work, are curious about ending work, want to get the most out ...\n",
      "   Members: 2.9M, Online: 1.1K\n",
      "   URL: https://www.reddit.com/r/antiwork/\n",
      "------------------------------------------------------------\n",
      "6. r/torontoJobs\n",
      "   Description: Redditor approved jobs in the GTHA.\n",
      "   Members: 61K, Online: 20\n",
      "   URL: https://www.reddit.com/r/torontoJobs/\n",
      "------------------------------------------------------------\n",
      "7. r/CanadaJobs\n",
      "   Description: This is a place to support job seekers and hiring managers. This community represents inclusivity, k...\n",
      "   Members: 23K, Online: 2\n",
      "   URL: https://www.reddit.com/r/CanadaJobs/\n",
      "------------------------------------------------------------\n",
      "8. r/recruitinghell\n",
      "   Description: Did a recruiter make you send them a resume and still fill out all the same information on their web...\n",
      "   Members: 844K, Online: 640\n",
      "   URL: https://www.reddit.com/r/recruitinghell/\n",
      "------------------------------------------------------------\n",
      "9. r/careerguidance\n",
      "   Description: A place to discuss career options, to ask questions and give advice!\n",
      "   Members: 4.7M, Online: 344\n",
      "   URL: https://www.reddit.com/r/careerguidance/\n",
      "------------------------------------------------------------\n",
      "10. r/DubaiJobs\n",
      "   Description: We all know how daunting the job hunt can be in Dubai at times. Post up any vacancies you know of ov...\n",
      "   Members: 15K, Online: 9\n",
      "   URL: https://www.reddit.com/r/DubaiJobs/\n",
      "------------------------------------------------------------\n",
      "11. r/Germany_Jobs\n",
      "   Description: Do you want to work in Germany? We want to build a community for finding an english or german speaki...\n",
      "   Members: 16K, Online: 8\n",
      "   URL: https://www.reddit.com/r/Germany_Jobs/\n",
      "------------------------------------------------------------\n",
      "12. r/cscareerquestions\n",
      "   Description: CSCareerQuestions is a community for those who are in the process of entering or are already part of...\n",
      "   Members: 2.2M, Online: 570\n",
      "   URL: https://www.reddit.com/r/cscareerquestions/\n",
      "------------------------------------------------------------\n",
      "13. r/UKJobs\n",
      "   Description: A community intended to provide a place for users wanting to ask questions or create discussions, al...\n",
      "   Members: 292K, Online: 245\n",
      "   URL: https://www.reddit.com/r/UKJobs/\n",
      "------------------------------------------------------------\n",
      "14. r/SideJobs\n",
      "   Description: Jobs you can do outside of your normal job for extra income.\n",
      "   Members: 3.5K, Online: 6\n",
      "   URL: https://www.reddit.com/r/SideJobs/\n",
      "------------------------------------------------------------\n",
      "15. r/IndiaJobsOpenings\n",
      "   Description: Your go-to subreddit for finding and sharing job opportunities across India. Here, members can post ...\n",
      "   Members: 7K, Online: 2\n",
      "   URL: https://www.reddit.com/r/IndiaJobsOpenings/\n",
      "------------------------------------------------------------\n",
      "16. r/politics\n",
      "   Description: /r/Politics is for news and discussion about U.S. politics.\n",
      "   Members: 8.8M, Online: 12K\n",
      "   URL: https://www.reddit.com/r/politics/\n",
      "------------------------------------------------------------\n",
      "17. r/WorkOnlineJobs\n",
      "   Description: Online Jobs - Work at Home\n",
      "   Members: 5K, Online: 1\n",
      "   URL: https://www.reddit.com/r/WorkOnlineJobs/\n",
      "------------------------------------------------------------\n",
      "18. r/NetworkingJobs\n",
      "   Description: \n",
      "   Members: 15K, Online: 5\n",
      "   URL: https://www.reddit.com/r/NetworkingJobs/\n",
      "------------------------------------------------------------\n",
      "19. r/memes\n",
      "   Description: Memes! A way of describing cultural information being shared. An element of a culture or system of b...\n",
      "   Members: 35M, Online: 1.6K\n",
      "   URL: https://www.reddit.com/r/memes/\n",
      "------------------------------------------------------------\n",
      "20. r/funny\n",
      "   Description: Reddit's largest humor depository\n",
      "   Members: 67M, Online: 2.3K\n",
      "   URL: https://www.reddit.com/r/funny/\n",
      "------------------------------------------------------------\n",
      "21. r/RemoteJobsSearch\n",
      "   Description: Hi everyone! This is a group where we can post Remote opportunities and we can share opinions and di...\n",
      "   Members: 1.7K, Online: 4\n",
      "   URL: https://www.reddit.com/r/RemoteJobsSearch/\n",
      "------------------------------------------------------------\n",
      "22. r/PythonJobs\n",
      "   Description: Python jobs are posted here\n",
      "   Members: 28K, Online: 25\n",
      "   URL: https://www.reddit.com/r/PythonJobs/\n",
      "------------------------------------------------------------\n",
      "23. r/personalfinance\n",
      "   Description: Learn about budgeting, saving, getting out of debt, credit, investing, and retirement planning. Join...\n",
      "   Members: 21M, Online: 1.5K\n",
      "   URL: https://www.reddit.com/r/personalfinance/\n",
      "------------------------------------------------------------\n",
      "24. r/ITCareerQuestions\n",
      "   Description: This subreddit is designed to help anyone in or interested in the IT field to ask career-related que...\n",
      "   Members: 492K, Online: 174\n",
      "   URL: https://www.reddit.com/r/ITCareerQuestions/\n",
      "------------------------------------------------------------\n",
      "25. r/OnlineJobsPH\n",
      "   Description: 💪 For Filipino Remote Workers to improve and learn new skills. ⭐ We are here to share and help Freel...\n",
      "   Members: 4.9K, Online: 5\n",
      "   URL: https://www.reddit.com/r/OnlineJobsPH/\n",
      "------------------------------------------------------------\n",
      "26. r/CyberSecurityJobs\n",
      "   Description: A forum for discussing cybersecurity career information.\n",
      "   Members: 41K, Online: 9\n",
      "   URL: https://www.reddit.com/r/CyberSecurityJobs/\n",
      "------------------------------------------------------------\n",
      "27. r/Showerthoughts\n",
      "   Description: A subreddit for sharing those miniature epiphanies that make the mundane more amazing.\n",
      "   Members: 34M, Online: 531\n",
      "   URL: https://www.reddit.com/r/Showerthoughts/\n",
      "------------------------------------------------------------\n",
      "28. r/Accounting\n",
      "   Description: Primarily for accountants and aspiring accountants to learn about and discuss their career choice. A...\n",
      "   Members: 1.1M, Online: 264\n",
      "   URL: https://www.reddit.com/r/Accounting/\n",
      "------------------------------------------------------------\n",
      "29. r/JobsPhilippines\n",
      "   Description: \n",
      "   Members: 73K, Online: 9\n",
      "   URL: https://www.reddit.com/r/JobsPhilippines/\n",
      "------------------------------------------------------------\n",
      "30. r/DataScienceJobs\n",
      "   Description: A place for people to post data science/machine learning jobs as well as those searching for jobs to...\n",
      "   Members: 32K, Online: 10\n",
      "   URL: https://www.reddit.com/r/DataScienceJobs/\n",
      "------------------------------------------------------------\n",
      "31. r/WritingPrompts\n",
      "   Description: Writing Prompts. You're a writer and you just want to flex those muscles? You've come to the right p...\n",
      "   Members: 19M, Online: 419\n",
      "   URL: https://www.reddit.com/r/WritingPrompts/\n",
      "------------------------------------------------------------\n",
      "32. r/AmItheAsshole\n",
      "   Description: A catharsis for the frustrated moral philosopher in all of us, and a place to finally find out if yo...\n",
      "   Members: 24M, Online: 9.5K\n",
      "   URL: https://www.reddit.com/r/AmItheAsshole/\n",
      "------------------------------------------------------------\n",
      "33. r/IWantOutJobs\n",
      "   Description: The purpose of this subreddit is to post employment opportunities for remote work or companies that ...\n",
      "   Members: 16K, Online: 5\n",
      "   URL: https://www.reddit.com/r/IWantOutJobs/\n",
      "------------------------------------------------------------\n",
      "34. r/Jobs4Bitcoins\n",
      "   Description: Find Work, Find Workers! All payments done in Bitcoin and other cryptocurrencies.\n",
      "   Members: 70K, Online: 11\n",
      "   URL: https://www.reddit.com/r/Jobs4Bitcoins/\n",
      "------------------------------------------------------------\n",
      "35. r/pics\n",
      "   Description: A place for photographs, pictures, and other images.\n",
      "   Members: 32M, Online: 4K\n",
      "   URL: https://www.reddit.com/r/pics/\n",
      "------------------------------------------------------------\n",
      "36. r/teenagers\n",
      "   Description: r/teenagers is the biggest community forum run by teenagers for teenagers. Our subreddit is primaril...\n",
      "   Members: 3.2M, Online: 1.5K\n",
      "   URL: https://www.reddit.com/r/teenagers/\n",
      "------------------------------------------------------------\n",
      "37. r/jobsearchhacks\n",
      "   Description: Forget traditional job searching - improve your odds with good tips, tricks and tactics that help yo...\n",
      "   Members: 238K, Online: 67\n",
      "   URL: https://www.reddit.com/r/jobsearchhacks/\n",
      "------------------------------------------------------------\n",
      "38. r/VancouverJobs\n",
      "   Description: A place to match Vancouver job-seekers with Vancouver employers.\n",
      "   Members: 27K, Online: 6\n",
      "   URL: https://www.reddit.com/r/VancouverJobs/\n",
      "------------------------------------------------------------\n",
      "39. r/AdviceAnimals\n",
      "   Description: Reddit's Gold Mine\n",
      "   Members: 9.9M, Online: 449\n",
      "   URL: https://www.reddit.com/r/AdviceAnimals/\n",
      "------------------------------------------------------------\n",
      "40. r/TrollXChromosomes\n",
      "   Description: A subreddit for rage comics and other memes with a girly slant.\n",
      "   Members: 835K, Online: 29\n",
      "   URL: https://www.reddit.com/r/TrollXChromosomes/\n",
      "------------------------------------------------------------\n",
      "41. r/RemoteJobseekers\n",
      "   Description: Looking for a remote job ? We share effective strategies for finding remote jobs, inspiring stories ...\n",
      "   Members: 17K, Online: 5\n",
      "   URL: https://www.reddit.com/r/RemoteJobseekers/\n",
      "------------------------------------------------------------\n",
      "42. r/houstonjobs\n",
      "   Description: Connecting Employers and Employees since 2011\n",
      "   Members: 10K, Online: 1\n",
      "   URL: https://www.reddit.com/r/houstonjobs/\n",
      "------------------------------------------------------------\n",
      "43. r/nfl\n",
      "   Description: The place to discuss all NFL related things\n",
      "   Members: 12M, Online: 5K\n",
      "   URL: https://www.reddit.com/r/nfl/\n",
      "------------------------------------------------------------\n",
      "44. r/nursing\n",
      "   Description: \n",
      "   Members: 1.1M, Online: 212\n",
      "   URL: https://www.reddit.com/r/nursing/\n",
      "------------------------------------------------------------\n",
      "45. r/MachineLearningJobs\n",
      "   Description: ML jobs, AI jobs, LLM jobs, machine learning, artificial intelligence, large language models like ch...\n",
      "   Members: 22K, Online: 11\n",
      "   URL: https://www.reddit.com/r/MachineLearningJobs/\n",
      "------------------------------------------------------------\n",
      "46. r/Nonprofit_Jobs\n",
      "   Description: A discussion board for non-profit/charity/NGO/government/social enterprise employment.\n",
      "   Members: 11K, Online: 0\n",
      "   URL: https://www.reddit.com/r/Nonprofit_Jobs/\n",
      "------------------------------------------------------------\n",
      "47. r/india\n",
      "   Description: The Official Subreddit for India\n",
      "   Members: 2.9M, Online: 840\n",
      "   URL: https://www.reddit.com/r/india/\n",
      "------------------------------------------------------------\n",
      "48. r/ffxiv\n",
      "   Description: A community for fans of the critically acclaimed MMORPG Final Fantasy XIV, with an expanded free tri...\n",
      "   Members: 1.3M, Online: 793\n",
      "   URL: https://www.reddit.com/r/ffxiv/\n",
      "------------------------------------------------------------\n",
      "49. r/JapanJobs\n",
      "   Description: Welcome to /r/JapanJobs! This is a subreddit dedicated to helping people find and post information r...\n",
      "   Members: 10K, Online: 3\n",
      "   URL: https://www.reddit.com/r/JapanJobs/\n",
      "------------------------------------------------------------\n",
      "50. r/gameDevJobs\n",
      "   Description: Here you will only ever find job listings that offer compensation for your time and expertise.\n",
      "   Members: 14K, Online: 9\n",
      "   URL: https://www.reddit.com/r/gameDevJobs/\n",
      "------------------------------------------------------------\n",
      "51. r/mildlyinfuriating\n",
      "   Description: jugkfmghgug\n",
      "   Members: 10M, Online: 1.6K\n",
      "   URL: https://www.reddit.com/r/mildlyinfuriating/\n",
      "------------------------------------------------------------\n",
      "52. r/legaladvice\n",
      "   Description: A place to ask simple legal questions.\n",
      "   Members: 3M, Online: 638\n",
      "   URL: https://www.reddit.com/r/legaladvice/\n",
      "------------------------------------------------------------\n",
      "53. r/BigDataJobs\n",
      "   Description: Find and post work relating to \"Big Data\".\n",
      "   Members: 15K, Online: 7\n",
      "   URL: https://www.reddit.com/r/BigDataJobs/\n",
      "------------------------------------------------------------\n",
      "54. r/WebDeveloperJobs\n",
      "   Description: Jobs for JavaScript web developers who have experience with or want to learn new technologies such a...\n",
      "   Members: 15K, Online: 16\n",
      "   URL: https://www.reddit.com/r/WebDeveloperJobs/\n",
      "------------------------------------------------------------\n",
      "55. r/CasualConversation\n",
      "   Description: The friendlier part of Reddit. Have a fun conversation about anything that is on your mind. Ask a qu...\n",
      "   Members: 2.6M, Online: 319\n",
      "   URL: https://www.reddit.com/r/CasualConversation/\n",
      "------------------------------------------------------------\n",
      "56. r/resumes\n",
      "   Description: Get help with your resume! Checkout the wiki and/or sidebar for resources and info!\n",
      "   Members: 1.2M, Online: 84\n",
      "   URL: https://www.reddit.com/r/resumes/\n",
      "------------------------------------------------------------\n",
      "57. r/SFBayJobs\n",
      "   Description: SF Bay Jobs - [Hiring], [Seeking]\n",
      "   Members: 9.1K, Online: 3\n",
      "   URL: https://www.reddit.com/r/SFBayJobs/\n",
      "------------------------------------------------------------\n",
      "58. r/AnimalswithJobs\n",
      "   Description: Post anything here related to any kind of animal with a job! Lets see the cuteness\n",
      "   Members: 9.2K, Online: 1\n",
      "   URL: https://www.reddit.com/r/AnimalswithJobs/\n",
      "------------------------------------------------------------\n",
      "59. r/Conservative\n",
      "   Description: https://x.com/rcondiscord https://discord.gg/conservative\n",
      "   Members: 1.3M, Online: 2.9K\n",
      "   URL: https://www.reddit.com/r/Conservative/\n",
      "------------------------------------------------------------\n",
      "60. r/findapath\n",
      "   Description: For those who have a hobby, passion, pursuit, or life goal that they want to make a living out of, b...\n",
      "   Members: 708K, Online: 50\n",
      "   URL: https://www.reddit.com/r/findapath/\n",
      "------------------------------------------------------------\n",
      "61. r/dogswithjobs\n",
      "   Description: This is a community for real working dogs. These are jobs or tasks a dog is specifically trained to ...\n",
      "   Members: 912K, Online: 15\n",
      "   URL: https://www.reddit.com/r/dogswithjobs/\n",
      "------------------------------------------------------------\n",
      "62. r/javascript_jobs\n",
      "   Description: Jobs related to javascript\n",
      "   Members: 7.4K, Online: 1\n",
      "   URL: https://www.reddit.com/r/javascript_jobs/\n",
      "------------------------------------------------------------\n",
      "63. r/developersIndia\n",
      "   Description: A wholesome community made by & for software & tech folks in India. Have a doubt? Ask it out.\n",
      "   Members: 1.3M, Online: 339\n",
      "   URL: https://www.reddit.com/r/developersIndia/\n",
      "------------------------------------------------------------\n",
      "64. r/Teachers\n",
      "   Description: Dedicated to open discussion about all things teaching. Please read the rules before posting. Mail s...\n",
      "   Members: 1.5M, Online: 265\n",
      "   URL: https://www.reddit.com/r/Teachers/\n",
      "------------------------------------------------------------\n",
      "65. r/PhotographyJobs\n",
      "   Description: Freelance Photography jobs for Photography\n",
      "   Members: 7K, Online: 1\n",
      "   URL: https://www.reddit.com/r/PhotographyJobs/\n",
      "------------------------------------------------------------\n",
      "66. r/SoftwareEngineerJobs\n",
      "   Description: Find a job as a software engineer today!\n",
      "   Members: 11K, Online: 13\n",
      "   URL: https://www.reddit.com/r/SoftwareEngineerJobs/\n",
      "------------------------------------------------------------\n",
      "67. r/Jokes\n",
      "   Description: The funniest sub on Reddit. Hundreds of jokes posted each day, and some of them aren't even reposts!\n",
      "   Members: 30M, Online: 1.3K\n",
      "   URL: https://www.reddit.com/r/Jokes/\n",
      "------------------------------------------------------------\n",
      "68. r/unpopularopinion\n",
      "   Description: Share your burning hot takes and unpopular opinions!\n",
      "   Members: 4.7M, Online: 1.1K\n",
      "   URL: https://www.reddit.com/r/unpopularopinion/\n",
      "------------------------------------------------------------\n",
      "69. r/PHJobs\n",
      "   Description: Soon the No. 1 Job Office for the Philippines\n",
      "   Members: 103K, Online: 8\n",
      "   URL: https://www.reddit.com/r/PHJobs/\n",
      "------------------------------------------------------------\n",
      "70. r/SeaJobs\n",
      "   Description: \n",
      "   Members: 5.8K, Online: 6\n",
      "   URL: https://www.reddit.com/r/SeaJobs/\n",
      "------------------------------------------------------------\n",
      "71. r/Economics\n",
      "   Description: Reddit's largest economics community. Serving as a central forum for users to read, discuss, and lea...\n",
      "   Members: 5.5M, Online: 1.7K\n",
      "   URL: https://www.reddit.com/r/Economics/\n",
      "------------------------------------------------------------\n",
      "72. r/KitchenConfidential\n",
      "   Description: Home to the largest online community of foodservice professionals. Icon artwork by www.instagram.com...\n",
      "   Members: 1.2M, Online: 676\n",
      "   URL: https://www.reddit.com/r/KitchenConfidential/\n",
      "------------------------------------------------------------\n",
      "73. r/NashvilleJobs\n",
      "   Description: Nashville Jobs is a place where Middle Tennesseans can look for work and information on local employ...\n",
      "   Members: 6K, Online: 1\n",
      "   URL: https://www.reddit.com/r/NashvilleJobs/\n",
      "------------------------------------------------------------\n",
      "74. r/BostonJobs\n",
      "   Description: A place to post job vacancies and jobs wanted.\n",
      "   Members: 5.1K, Online: 3\n",
      "   URL: https://www.reddit.com/r/BostonJobs/\n",
      "------------------------------------------------------------\n",
      "75. r/australia\n",
      "   Description: A dusty corner on the internet where you can chew the fat about Australia and Australians.\n",
      "   Members: 2.6M, Online: 732\n",
      "   URL: https://www.reddit.com/r/australia/\n",
      "------------------------------------------------------------\n",
      "76. r/ADHD\n",
      "   Description: We're an inclusive, disability-oriented peer support group for people with ADHD with an emphasis on ...\n",
      "   Members: 2M, Online: 289\n",
      "   URL: https://www.reddit.com/r/ADHD/\n",
      "------------------------------------------------------------\n",
      "77. r/ChinaJobs\n",
      "   Description: For expats looking for a career in China!\n",
      "   Members: 5.4K, Online: 1\n",
      "   URL: https://www.reddit.com/r/ChinaJobs/\n",
      "------------------------------------------------------------\n",
      "78. r/OrlandoJobs\n",
      "   Description: Metro Orlando's only recruitment board. Job postings as well as personnel postings.\n",
      "   Members: 4.8K, Online: 1\n",
      "   URL: https://www.reddit.com/r/OrlandoJobs/\n",
      "------------------------------------------------------------\n",
      "79. r/conspiracy\n",
      "   Description: This is a forum for free thinking and for discussing issues which have captured your imagination. Pl...\n",
      "   Members: 2.2M, Online: 927\n",
      "   URL: https://www.reddit.com/r/conspiracy/\n",
      "------------------------------------------------------------\n",
      "80. r/ukpolitics\n",
      "   Description: Political news and debate concerning the United Kingdom. The rules of the subreddit can be found in ...\n",
      "   Members: 516K, Online: 812\n",
      "   URL: https://www.reddit.com/r/ukpolitics/\n",
      "------------------------------------------------------------\n",
      "81. r/EdmontonJobs\n",
      "   Description: \n",
      "   Members: 5K, Online: 9\n",
      "   URL: https://www.reddit.com/r/EdmontonJobs/\n",
      "------------------------------------------------------------\n",
      "82. r/PortlandJobs\n",
      "   Description: \n",
      "   Members: 4.6K, Online: 5\n",
      "   URL: https://www.reddit.com/r/PortlandJobs/\n",
      "------------------------------------------------------------\n",
      "83. r/careeradvice\n",
      "   Description: \n",
      "   Members: 655K, Online: 222\n",
      "   URL: https://www.reddit.com/r/careeradvice/\n",
      "------------------------------------------------------------\n",
      "84. r/NoStupidQuestions\n",
      "   Description: Ask away! Disclaimer: This is an anonymous forum so answers may not be correct\n",
      "   Members: 6M, Online: 10K\n",
      "   URL: https://www.reddit.com/r/NoStupidQuestions/\n",
      "------------------------------------------------------------\n",
      "85. r/SanDiegoJobs\n",
      "   Description: Putting San Diego Redditors to work!\n",
      "   Members: 8.4K, Online: 0\n",
      "   URL: https://www.reddit.com/r/SanDiegoJobs/\n",
      "------------------------------------------------------------\n",
      "86. r/Catswithjobs\n",
      "   Description: Kitties working hard and hardly working\n",
      "   Members: 1.4M, Online: 82\n",
      "   URL: https://www.reddit.com/r/Catswithjobs/\n",
      "------------------------------------------------------------\n",
      "87. r/canada\n",
      "   Description: Welcome to Canada’s official subreddit! This is the place to engage on all things Canada. Nous parlo...\n",
      "   Members: 3.9M, Online: 4.8K\n",
      "   URL: https://www.reddit.com/r/canada/\n",
      "------------------------------------------------------------\n",
      "88. r/sales\n",
      "   Description: Everything you need to know about sales, selling, business development, lead generation, prospecting...\n",
      "   Members: 463K, Online: 82\n",
      "   URL: https://www.reddit.com/r/sales/\n",
      "------------------------------------------------------------\n",
      "89. r/CaliforniaJobs\n",
      "   Description: This is for jobs in the State of California.\n",
      "   Members: 3.8K, Online: 4\n",
      "   URL: https://www.reddit.com/r/CaliforniaJobs/\n",
      "------------------------------------------------------------\n",
      "90. r/YoungJobs\n",
      "   Description: \n",
      "   Members: 3.7K, Online: 5\n",
      "   URL: https://www.reddit.com/r/YoungJobs/\n",
      "------------------------------------------------------------\n",
      "91. r/economy\n",
      "   Description: Forum for economy, business, politics, stocks, bonds, product releases, IPOs, advice, news, investme...\n",
      "   Members: 1M, Online: 244\n",
      "   URL: https://www.reddit.com/r/economy/\n",
      "------------------------------------------------------------\n",
      "92. r/aww\n",
      "   Description: Things that make you go AWW! -- like puppies, bunnies, babies, and so on... Feel free to post origin...\n",
      "   Members: 38M, Online: 1K\n",
      "   URL: https://www.reddit.com/r/aww/\n",
      "------------------------------------------------------------\n",
      "93. r/HalifaxJobs\n",
      "   Description: A subreddit for those who are looking to hire people and those who are searching for job.\n",
      "   Members: 3.7K, Online: 4\n",
      "   URL: https://www.reddit.com/r/HalifaxJobs/\n",
      "------------------------------------------------------------\n",
      "94. r/EngineeringJobs\n",
      "   Description: Your go-to source for locating an engineering job in the Midwest/great lakes region.\n",
      "   Members: 4K, Online: 5\n",
      "   URL: https://www.reddit.com/r/EngineeringJobs/\n",
      "------------------------------------------------------------\n",
      "95. r/sysadmin\n",
      "   Description: A reddit dedicated to the profession of Computer System Administration.\n",
      "   Members: 1M, Online: 1K\n",
      "   URL: https://www.reddit.com/r/sysadmin/\n",
      "------------------------------------------------------------\n",
      "96. r/FinancialCareers\n",
      "   Description: Plan your career in the wide world of finance.\n",
      "   Members: 1.6M, Online: 262\n",
      "   URL: https://www.reddit.com/r/FinancialCareers/\n",
      "------------------------------------------------------------\n",
      "97. r/CodingJobs\n",
      "   Description: Coding Jobs. Post New Jobs Daily. Get more jobs at https://echojobs.io/jobs\n",
      "   Members: 4K, Online: 10\n",
      "   URL: https://www.reddit.com/r/CodingJobs/\n",
      "------------------------------------------------------------\n",
      "98. r/blender_jobs\n",
      "   Description: Recruiting & jobseeking for Blender 3D artists. Post a job, or let people know you're looking for on...\n",
      "   Members: 3.8K, Online: 0\n",
      "   URL: https://www.reddit.com/r/blender_jobs/\n",
      "------------------------------------------------------------\n",
      "99. r/Futurology\n",
      "   Description: A subreddit devoted to the field of Future(s) Studies and evidence-based speculation about the devel...\n",
      "   Members: 22M, Online: 983\n",
      "   URL: https://www.reddit.com/r/Futurology/\n",
      "------------------------------------------------------------\n",
      "100. r/DeveloperJobs\n",
      "   Description: Software Developer Jobs Please try to limit your own listings to a handful per week. Spammers will b...\n",
      "   Members: 3.3K, Online: 14\n",
      "   URL: https://www.reddit.com/r/DeveloperJobs/\n",
      "------------------------------------------------------------\n",
      "101. r/CalgaryJobs\n",
      "   Description: Whether you're on the hunt for your first job, wanting to hire for a special event or seeking career...\n",
      "   Members: 2.5K, Online: 2\n",
      "   URL: https://www.reddit.com/r/CalgaryJobs/\n",
      "------------------------------------------------------------\n",
      "102. r/MarketingJobs\n",
      "   Description: The purpose of this subreddit is to connect marketing job seekers with companies seeking to fill ope...\n",
      "   Members: 3.1K, Online: 2\n",
      "   URL: https://www.reddit.com/r/MarketingJobs/\n",
      "------------------------------------------------------------\n",
      "103. r/ProgrammingJobs\n",
      "   Description: Programming jobs of all types\n",
      "   Members: 2.6K, Online: 5\n",
      "   URL: https://www.reddit.com/r/ProgrammingJobs/\n",
      "------------------------------------------------------------\n",
      "104. r/Cannabis_Jobs\n",
      "   Description: A place for finding, discussing, and listing jobs in the Cannabis Industry.\n",
      "   Members: 2.5K, Online: 1\n",
      "   URL: https://www.reddit.com/r/Cannabis_Jobs/\n",
      "------------------------------------------------------------\n",
      "105. r/AzureJobs\n",
      "   Description: A place to get hired to work with Microsoft Azure\n",
      "   Members: 2.4K, Online: 0\n",
      "   URL: https://www.reddit.com/r/AzureJobs/\n",
      "------------------------------------------------------------\n",
      "106. r/IndeedJobs\n",
      "   Description: This is the Unofficial Community of the App Indeed, where we can discuss about how effective it is w...\n",
      "   Members: 2.5K, Online: 6\n",
      "   URL: https://www.reddit.com/r/IndeedJobs/\n",
      "------------------------------------------------------------\n",
      "107. r/HawaiiJobs\n",
      "   Description: HawaiiJobs\n",
      "   Members: 2K, Online: 4\n",
      "   URL: https://www.reddit.com/r/HawaiiJobs/\n",
      "------------------------------------------------------------\n",
      "108. r/SwiftJobs\n",
      "   Description: Job listing board for the Swift programming language. Helping developers find careers and contract w...\n",
      "   Members: 2.6K, Online: 4\n",
      "   URL: https://www.reddit.com/r/SwiftJobs/\n",
      "------------------------------------------------------------\n",
      "109. r/PerthJobs\n",
      "   Description: Advertise or seek jobs of any kind in or around Perth, Western Australia\n",
      "   Members: 2.2K, Online: 0\n",
      "   URL: https://www.reddit.com/r/PerthJobs/\n",
      "------------------------------------------------------------\n",
      "110. r/CincinnatiJobs\n",
      "   Description: Post job openings in the Greater Cincinnati area\n",
      "   Members: 2.3K, Online: 2\n",
      "   URL: https://www.reddit.com/r/CincinnatiJobs/\n",
      "------------------------------------------------------------\n",
      "111. r/SpokaneJobs\n",
      "   Description: Hiring or Looking for Work in Spokane?\n",
      "   Members: 1.8K, Online: 1\n",
      "   URL: https://www.reddit.com/r/SpokaneJobs/\n",
      "------------------------------------------------------------\n",
      "112. r/AccountingJobs\n",
      "   Description: A place to share and find jobs in the accounting field. Recruiters must use job template pinned to t...\n",
      "   Members: 1.7K, Online: 2\n",
      "   URL: https://www.reddit.com/r/AccountingJobs/\n",
      "------------------------------------------------------------\n",
      "113. r/GraphicDesignJobs\n",
      "   Description: Hire or Get Hired! A subreddit for all Graphic Designers or people looking for Graphic Designers! Pl...\n",
      "   Members: 5.1K, Online: 1\n",
      "   URL: https://www.reddit.com/r/GraphicDesignJobs/\n",
      "------------------------------------------------------------\n",
      "114. r/BitcoinJobs\n",
      "   Description: Bitcoin Jobs board - earn Bitcoin Cash (BCH) or Bitcoin (BTC)\n",
      "   Members: 2K, Online: 1\n",
      "   URL: https://www.reddit.com/r/BitcoinJobs/\n",
      "------------------------------------------------------------\n",
      "115. r/EngineerJobs\n",
      "   Description: We help all types of engineers find and land their dream jobs. Please post your jobs, career questio...\n",
      "   Members: 2K, Online: 1\n",
      "   URL: https://www.reddit.com/r/EngineerJobs/\n",
      "------------------------------------------------------------\n",
      "116. r/BullshitJobs\n",
      "   Description: Post anything here about your job and how it is essentially useless, doesn't help society (or does t...\n",
      "   Members: 1.8K, Online: 3\n",
      "   URL: https://www.reddit.com/r/BullshitJobs/\n",
      "------------------------------------------------------------\n",
      "117. r/JobsCanada\n",
      "   Description: Looking for a job or an employee in Canada? You've come to the right place! This is also a great pla...\n",
      "   Members: 2.1K, Online: 4\n",
      "   URL: https://www.reddit.com/r/JobsCanada/\n",
      "------------------------------------------------------------\n",
      "118. r/TranscriptionJobs\n",
      "   Description: Transcription Jobs. Either Hiring or Looking for Job posts. No promotions or something that may look...\n",
      "   Members: 1.8K, Online: 3\n",
      "   URL: https://www.reddit.com/r/TranscriptionJobs/\n",
      "------------------------------------------------------------\n",
      "119. r/frontend_jobs\n",
      "   Description: Frontend jobs for software engineers\n",
      "   Members: 1.5K, Online: 3\n",
      "   URL: https://www.reddit.com/r/frontend_jobs/\n",
      "------------------------------------------------------------\n",
      "120. r/OmahaJobs\n",
      "   Description: A subreddit for folks who live in the Omaha to post job openings or job inquiries. Please include a ...\n",
      "   Members: 1.3K, Online: 1\n",
      "   URL: https://www.reddit.com/r/OmahaJobs/\n",
      "------------------------------------------------------------\n",
      "121. r/WilmingtonJobs\n",
      "   Description: Looking for jobs/employees in Wilmington NC? Post here\n",
      "   Members: 1K, Online: 5\n",
      "   URL: https://www.reddit.com/r/WilmingtonJobs/\n",
      "------------------------------------------------------------\n",
      "122. r/SteveJobs\n",
      "   Description: \n",
      "   Members: 1.3K, Online: 4\n",
      "   URL: https://www.reddit.com/r/SteveJobs/\n",
      "------------------------------------------------------------\n",
      "123. r/infosec_jobs\n",
      "   Description: This is the official subreddit of infosec-jobs.com - 🔎 it's all about finding your next job in InfoS...\n",
      "   Members: 1.3K, Online: 2\n",
      "   URL: https://www.reddit.com/r/infosec_jobs/\n",
      "------------------------------------------------------------\n",
      "124. r/LaravelJobs\n",
      "   Description: Post here for hire/hiring requests for Laravel Developers.\n",
      "   Members: 1.3K, Online: 0\n",
      "   URL: https://www.reddit.com/r/LaravelJobs/\n",
      "------------------------------------------------------------\n",
      "125. r/AdelaideJobs\n",
      "   Description: Welcome to AdelaideJobs! This sub is intended to help people find jobs and employees around the Adel...\n",
      "   Members: 1.3K, Online: 1\n",
      "   URL: https://www.reddit.com/r/AdelaideJobs/\n",
      "------------------------------------------------------------\n",
      "126. r/TucsonJobs\n",
      "   Description: \n",
      "   Members: 1.6K, Online: 1\n",
      "   URL: https://www.reddit.com/r/TucsonJobs/\n",
      "------------------------------------------------------------\n",
      "127. r/sports_jobs\n",
      "   Description: Job opportunities in the sports analytics world , software in sports/esports and betting! Posting th...\n",
      "   Members: 1.3K, Online: 7\n",
      "   URL: https://www.reddit.com/r/sports_jobs/\n",
      "------------------------------------------------------------\n",
      "128. r/ThaiJobs\n",
      "   Description: Available jobs in Thailand.\n",
      "   Members: 1.6K, Online: 6\n",
      "   URL: https://www.reddit.com/r/ThaiJobs/\n",
      "------------------------------------------------------------\n",
      "129. r/JobsUK\n",
      "   Description: \n",
      "   Members: 1.2K, Online: 1\n",
      "   URL: https://www.reddit.com/r/JobsUK/\n",
      "------------------------------------------------------------\n",
      "130. r/gayfootworshipJOBS\n",
      "   Description: This is for gay Male foot Fetish employment. This is not modeling it is work for foot worship scenes...\n",
      "   Members: 1.2K, Online: 3\n",
      "   URL: https://www.reddit.com/r/gayfootworshipJOBS/\n",
      "------------------------------------------------------------\n",
      "131. r/SaskatchewanJobs\n",
      "   Description: **r/SaskatchewanJobs is sub is meant to be a resource** not a site for job posts, here you'll find l...\n",
      "   Members: 1.2K, Online: 0\n",
      "   URL: https://www.reddit.com/r/SaskatchewanJobs/\n",
      "------------------------------------------------------------\n",
      "132. r/CryptoJobsList\n",
      "   Description: Web3 jobs, crypto jobs, blockchain jobs\n",
      "   Members: 4.1K, Online: 7\n",
      "   URL: https://www.reddit.com/r/CryptoJobsList/\n",
      "------------------------------------------------------------\n",
      "133. r/JobsUAE\n",
      "   Description: This community is dedicated to assisting individuals in finding employment opportunities in the foll...\n",
      "   Members: 1K, Online: 2\n",
      "   URL: https://www.reddit.com/r/JobsUAE/\n",
      "------------------------------------------------------------\n",
      "134. r/MexiJobs\n",
      "   Description: \n",
      "   Members: 781, Online: 5\n",
      "   URL: https://www.reddit.com/r/MexiJobs/\n",
      "------------------------------------------------------------\n",
      "135. r/JobsUSA\n",
      "   Description: This is a subreddit for people that are looking for work or working in the United States of America.\n",
      "   Members: 1.3K, Online: 3\n",
      "   URL: https://www.reddit.com/r/JobsUSA/\n",
      "------------------------------------------------------------\n",
      "136. r/WifiJobs\n",
      "   Description: Welcome to r/WifiJobs, the go-to subreddit for all things related to remote work opportunities!\n",
      "   Members: 773, Online: 3\n",
      "   URL: https://www.reddit.com/r/WifiJobs/\n",
      "------------------------------------------------------------\n",
      "137. r/IreJobs\n",
      "   Description: \n",
      "   Members: 1.3K, Online: 5\n",
      "   URL: https://www.reddit.com/r/IreJobs/\n",
      "------------------------------------------------------------\n",
      "138. r/MultipleJobs\n",
      "   Description: A community where you can discuss working multiple jobs and also acquiring jobs for the purposes of ...\n",
      "   Members: 700, Online: 4\n",
      "   URL: https://www.reddit.com/r/MultipleJobs/\n",
      "------------------------------------------------------------\n",
      "139. r/JacksonvilleJobs\n",
      "   Description: If you have a job to post, please precede your posts with [HIRING]. If you are someone who is lookin...\n",
      "   Members: 881, Online: 1\n",
      "   URL: https://www.reddit.com/r/JacksonvilleJobs/\n",
      "------------------------------------------------------------\n",
      "140. r/GlasgowJobs\n",
      "   Description: A place for people to post jobs in Glasgow for free, endorsed by /r/Glasgow.\n",
      "   Members: 1.3K, Online: 3\n",
      "   URL: https://www.reddit.com/r/GlasgowJobs/\n",
      "------------------------------------------------------------\n",
      "141. r/TruckerJobs\n",
      "   Description: Post your 'Now Hiring' ads, loads, job searches and anything related here. This community is an exte...\n",
      "   Members: 1.1K, Online: 2\n",
      "   URL: https://www.reddit.com/r/TruckerJobs/\n",
      "------------------------------------------------------------\n",
      "142. r/ContentWritingJobs\n",
      "   Description: A place where we gather the best paid content writing jobs across the web. Work from home, remotely,...\n",
      "   Members: 4.2K, Online: 3\n",
      "   URL: https://www.reddit.com/r/ContentWritingJobs/\n",
      "------------------------------------------------------------\n",
      "143. r/ArkansasJobs\n",
      "   Description: A community for posting job listings and other job hunting resources in Arkansas.\n",
      "   Members: 869, Online: 5\n",
      "   URL: https://www.reddit.com/r/ArkansasJobs/\n",
      "------------------------------------------------------------\n",
      "144. r/linux_jobs\n",
      "   Description: Discussing Linux jobs\n",
      "   Members: 846, Online: 5\n",
      "   URL: https://www.reddit.com/r/linux_jobs/\n",
      "------------------------------------------------------------\n",
      "145. r/ColoradoJobs\n",
      "   Description: Please post Part Time and Full Time jobs in Colorado. No commission or pay for your own supplies typ...\n",
      "   Members: 1.2K, Online: 1\n",
      "   URL: https://www.reddit.com/r/ColoradoJobs/\n",
      "------------------------------------------------------------\n",
      "146. r/SacJobs\n",
      "   Description: Seeking and For Hire requests for the residents of Sacramento and surrounding areas\n",
      "   Members: 1K, Online: 3\n",
      "   URL: https://www.reddit.com/r/SacJobs/\n",
      "------------------------------------------------------------\n",
      "147. r/backend_jobs\n",
      "   Description: Backend jobs for software engineers and architects\n",
      "   Members: 522, Online: 3\n",
      "   URL: https://www.reddit.com/r/backend_jobs/\n",
      "------------------------------------------------------------\n",
      "148. r/HuntsvilleAlabamaJobs\n",
      "   Description: Helping to connect the right candidate with the right employer. 31 People have found jobs from this ...\n",
      "   Members: 3.7K, Online: 4\n",
      "   URL: https://www.reddit.com/r/HuntsvilleAlabamaJobs/\n",
      "------------------------------------------------------------\n",
      "149. r/FlexJobs\n",
      "   Description: FlexJobs is the longtime leader in helping job seekers find the highest-quality remote, work-from-ho...\n",
      "   Members: 853, Online: 2\n",
      "   URL: https://www.reddit.com/r/FlexJobs/\n",
      "------------------------------------------------------------\n",
      "150. r/SyracuseJobs\n",
      "   Description: Help find other redditors find a job in Syracuse and neighboring areas.\n",
      "   Members: 485, Online: 3\n",
      "   URL: https://www.reddit.com/r/SyracuseJobs/\n",
      "------------------------------------------------------------\n",
      "151. r/GTAPaintJobs\n",
      "   Description: **GTA Custom Paint Jobs.. This is where you can post and find new and exciting ideas about Paint Job...\n",
      "   Members: 812, Online: 0\n",
      "   URL: https://www.reddit.com/r/GTAPaintJobs/\n",
      "------------------------------------------------------------\n",
      "152. r/TeenJobs\n",
      "   Description: A place for teens to come and discuss ways to earn money\n",
      "   Members: 758, Online: 3\n",
      "   URL: https://www.reddit.com/r/TeenJobs/\n",
      "------------------------------------------------------------\n",
      "153. r/NevadaJobs\n",
      "   Description: This subreddit has been created to post job opportunities that are located in the state of Nevada, U...\n",
      "   Members: 927, Online: 2\n",
      "   URL: https://www.reddit.com/r/NevadaJobs/\n",
      "------------------------------------------------------------\n",
      "154. r/JobsOffers\n",
      "   Description: Jobs Openings in Whole Over The World Offers Both Full and Part Time Jobs Build a Free Job Profile a...\n",
      "   Members: 878, Online: 2\n",
      "   URL: https://www.reddit.com/r/JobsOffers/\n",
      "------------------------------------------------------------\n",
      "155. r/Ottawa_jobs\n",
      "   Description: A sub-reddit for job openings available in Ottawa\n",
      "   Members: 1K, Online: 4\n",
      "   URL: https://www.reddit.com/r/Ottawa_jobs/\n",
      "------------------------------------------------------------\n",
      "156. r/JobsNYC\n",
      "   Description: Job Openings, Careers, and Hiring Events throughout New York City. If you're organization is hiring,...\n",
      "   Members: 654, Online: 1\n",
      "   URL: https://www.reddit.com/r/JobsNYC/\n",
      "------------------------------------------------------------\n",
      "157. r/AerospaceJobs\n",
      "   Description: Know about an aerospace job opening? Post it! Looking for an aerospace job? Post a [Looking] thread ...\n",
      "   Members: 660, Online: 0\n",
      "   URL: https://www.reddit.com/r/AerospaceJobs/\n",
      "------------------------------------------------------------\n",
      "158. r/SacramentoJobs\n",
      "   Description: To ensure the privacy and safety of yourself and all members here, please limit the amount of inform...\n",
      "   Members: 719, Online: 1\n",
      "   URL: https://www.reddit.com/r/SacramentoJobs/\n",
      "------------------------------------------------------------\n",
      "159. r/Austin_Jobs\n",
      "   Description: \n",
      "   Members: 693, Online: 5\n",
      "   URL: https://www.reddit.com/r/Austin_Jobs/\n",
      "------------------------------------------------------------\n",
      "160. r/JobsDarmstadt\n",
      "   Description: Hier werden in Zukunft regionale Jobs aus Darmstadt und der Region gepostet. Weitere Jobs findet Ihr...\n",
      "   Members: 339, Online: 0\n",
      "   URL: https://www.reddit.com/r/JobsDarmstadt/\n",
      "------------------------------------------------------------\n",
      "161. r/SFtechJobs\n",
      "   Description: Software engineer jobs in San Francisco, CA area. Post every hour. Find more on echojobs.io\n",
      "   Members: 639, Online: 3\n",
      "   URL: https://www.reddit.com/r/SFtechJobs/\n",
      "------------------------------------------------------------\n",
      "162. r/Jobs_Timisoara\n",
      "   Description: \n",
      "   Members: 329, Online: 2\n",
      "   URL: https://www.reddit.com/r/Jobs_Timisoara/\n",
      "------------------------------------------------------------\n",
      "163. r/jobsUSAimmigration\n",
      "   Description: We post 20 jobs with USA visa sponsorship daily. Skilled/unskilled available. Proudly supported by m...\n",
      "   Members: 626, Online: 2\n",
      "   URL: https://www.reddit.com/r/jobsUSAimmigration/\n",
      "------------------------------------------------------------\n",
      "164. r/CharlestonJobs\n",
      "   Description: \n",
      "   Members: 972, Online: 0\n",
      "   URL: https://www.reddit.com/r/CharlestonJobs/\n",
      "------------------------------------------------------------\n",
      "165. r/JobsPH\n",
      "   Description: \n",
      "   Members: 314, Online: 5\n",
      "   URL: https://www.reddit.com/r/JobsPH/\n",
      "------------------------------------------------------------\n",
      "166. r/blockchain_jobs\n",
      "   Description: blockchain related jobs\n",
      "   Members: 282, Online: 4\n",
      "   URL: https://www.reddit.com/r/blockchain_jobs/\n",
      "------------------------------------------------------------\n",
      "167. r/VueJobs\n",
      "   Description: A board to connect talented developers with companies & individuals looking to hire. If you're looki...\n",
      "   Members: 568, Online: 4\n",
      "   URL: https://www.reddit.com/r/VueJobs/\n",
      "------------------------------------------------------------\n",
      "168. r/JobsDresden\n",
      "   Description: Hier werden in Zukunft regionale Jobs aus Dresden und der Region gepostet. Weitere Jobs findet Ihr a...\n",
      "   Members: 561, Online: 2\n",
      "   URL: https://www.reddit.com/r/JobsDresden/\n",
      "------------------------------------------------------------\n",
      "169. r/jobshungary\n",
      "   Description: A munka általánosságban az életünk 1/3-át teszi ki. A mindennapjaink része, így bőven akad megvitatn...\n",
      "   Members: 36K, Online: 19\n",
      "   URL: https://www.reddit.com/r/jobshungary/\n",
      "------------------------------------------------------------\n",
      "170. r/AnimationJobs\n",
      "   Description: [[[[[[[[[[[[[[[[[[[[[[[ Looking to hire? ]]]]]]]]]]]]]]]]]]]]]]] Create a text post starting with [H...\n",
      "   Members: 731, Online: 1\n",
      "   URL: https://www.reddit.com/r/AnimationJobs/\n",
      "------------------------------------------------------------\n",
      "171. r/typescript_jobs\n",
      "   Description: Jobs for TypeScript developers\n",
      "   Members: 254, Online: 5\n",
      "   URL: https://www.reddit.com/r/typescript_jobs/\n",
      "------------------------------------------------------------\n",
      "172. r/sql_jobs\n",
      "   Description: Jobs for SQL developers\n",
      "   Members: 237, Online: 4\n",
      "   URL: https://www.reddit.com/r/sql_jobs/\n",
      "------------------------------------------------------------\n",
      "173. r/AlaskaJobs\n",
      "   Description: This subreddit has been created to post job opportunities that are located in Alaska, USA\n",
      "   Members: 723, Online: 4\n",
      "   URL: https://www.reddit.com/r/AlaskaJobs/\n",
      "------------------------------------------------------------\n",
      "174. r/FloridaJobs\n",
      "   Description: Looking for a job in Florida? You've come to the right place. For all career and employment news for...\n",
      "   Members: 571, Online: 0\n",
      "   URL: https://www.reddit.com/r/FloridaJobs/\n",
      "------------------------------------------------------------\n",
      "175. r/ChemJobs\n",
      "   Description: Listings/help/advice/direction and a forum for all things related to employment in chemistry related...\n",
      "   Members: 548, Online: 0\n",
      "   URL: https://www.reddit.com/r/ChemJobs/\n",
      "------------------------------------------------------------\n",
      "176. r/JobsAICopilot\n",
      "   Description: Welcome to JobsAICopilot, the ultimate platform for AI-powered job application automation. As featur...\n",
      "   Members: 511, Online: 0\n",
      "   URL: https://www.reddit.com/r/JobsAICopilot/\n",
      "------------------------------------------------------------\n",
      "177. r/PampangaJobs\n",
      "   Description: This subreddit is dedicated to connecting job seekers in Pampanga with local employment opportunitie...\n",
      "   Members: 218, Online: 4\n",
      "   URL: https://www.reddit.com/r/PampangaJobs/\n",
      "------------------------------------------------------------\n",
      "178. r/JobsLeipzig\n",
      "   Description: Hier werden in Zukunft regionale Jobs aus Leipzig und der Region gepostet. Weitere Jobs findet Ihr a...\n",
      "   Members: 226, Online: 4\n",
      "   URL: https://www.reddit.com/r/JobsLeipzig/\n",
      "------------------------------------------------------------\n",
      "179. r/BackendJobs\n",
      "   Description: \n",
      "   Members: 206, Online: 0\n",
      "   URL: https://www.reddit.com/r/BackendJobs/\n",
      "------------------------------------------------------------\n",
      "180. r/WebDevJobs\n",
      "   Description: A group to help Web Developers find work in the tech sector via other Redditors, and for those looki...\n",
      "   Members: 3.3K, Online: 13\n",
      "   URL: https://www.reddit.com/r/WebDevJobs/\n",
      "------------------------------------------------------------\n",
      "181. r/blockchainJobs\n",
      "   Description: blockchain related jobs\n",
      "   Members: 500, Online: 4\n",
      "   URL: https://www.reddit.com/r/blockchainJobs/\n",
      "------------------------------------------------------------\n",
      "182. r/AngularJobs\n",
      "   Description: Angular.js is a major JavaScript framework, backed by Google, and making waves in the web tech indus...\n",
      "   Members: 768, Online: 0\n",
      "   URL: https://www.reddit.com/r/AngularJobs/\n",
      "------------------------------------------------------------\n",
      "183. r/NairobiJobs\n",
      "   Description: Job Opportunities\n",
      "   Members: 160, Online: 4\n",
      "   URL: https://www.reddit.com/r/NairobiJobs/\n",
      "------------------------------------------------------------\n",
      "184. r/JobsFrankfurt\n",
      "   Description: Hier werden in Zukunft regionale Jobs aus Frankfurt und der Region gepostet. Weitere Jobs findet Ihr...\n",
      "   Members: 166, Online: 3\n",
      "   URL: https://www.reddit.com/r/JobsFrankfurt/\n",
      "------------------------------------------------------------\n",
      "185. r/JobsChicago\n",
      "   Description: All things employment/careers\n",
      "   Members: 150, Online: 3\n",
      "   URL: https://www.reddit.com/r/JobsChicago/\n",
      "------------------------------------------------------------\n",
      "186. r/cybersecurity_jobs\n",
      "   Description: Looking for the cybersecurity jobs subreddit? Use r/cybersecurityjobs!\n",
      "   Members: 150, Online: 5\n",
      "   URL: https://www.reddit.com/r/cybersecurity_jobs/\n",
      "------------------------------------------------------------\n",
      "187. r/JobsOsnabrueck\n",
      "   Description: Hier werden in Zukunft regionale Jobs aus Osnabrück und der Region gepostet. Weitere Jobs findet Ihr...\n",
      "   Members: 158, Online: 4\n",
      "   URL: https://www.reddit.com/r/JobsOsnabrueck/\n",
      "------------------------------------------------------------\n",
      "188. r/JobsKarlsruhe\n",
      "   Description: Hier werden in Zukunft regionale Jobs aus Karlsruhe und der Region gepostet. Weitere Jobs findet Ihr...\n",
      "   Members: 157, Online: 5\n",
      "   URL: https://www.reddit.com/r/JobsKarlsruhe/\n",
      "------------------------------------------------------------\n",
      "189. r/JobsMuenchen\n",
      "   Description: Hier werden in Zukunft regionale Jobs aus München und der Region gepostet. Weitere Jobs findet Ihr a...\n",
      "   Members: 500, Online: 3\n",
      "   URL: https://www.reddit.com/r/JobsMuenchen/\n",
      "------------------------------------------------------------\n",
      "190. r/JobsTrier\n",
      "   Description: Hier werden in Zukunft regionale Jobs aus Trier und der Region gepostet. Weitere Jobs findet Ihr auc...\n",
      "   Members: 136, Online: 4\n",
      "   URL: https://www.reddit.com/r/JobsTrier/\n",
      "------------------------------------------------------------\n",
      "191. r/JobsMainz\n",
      "   Description: Hier werden in Zukunft regionale Jobs aus Mainz und der Region gepostet. Weitere Jobs findet Ihr auc...\n",
      "   Members: 149, Online: 4\n",
      "   URL: https://www.reddit.com/r/JobsMainz/\n",
      "------------------------------------------------------------\n",
      "192. r/jobs_hamburg\n",
      "   Description: Hier werden in Zukunft regionale Jobs aus Hamburg und der Region gepostet. Weitere Jobs findet Ihr a...\n",
      "   Members: 644, Online: 0\n",
      "   URL: https://www.reddit.com/r/jobs_hamburg/\n",
      "------------------------------------------------------------\n",
      "193. r/python_jobs\n",
      "   Description: Jobs for Python programmers\n",
      "   Members: 144, Online: 0\n",
      "   URL: https://www.reddit.com/r/python_jobs/\n",
      "------------------------------------------------------------\n",
      "194. r/MississaugaJobs\n",
      "   Description: A place for people looking to hire or looking to get hired in Mississauga or close surrounding areas...\n",
      "   Members: 445, Online: 3\n",
      "   URL: https://www.reddit.com/r/MississaugaJobs/\n",
      "------------------------------------------------------------\n",
      "195. r/SydneyJobs\n",
      "   Description: Anything Sydney Job related is welcome.\n",
      "   Members: 391, Online: 3\n",
      "   URL: https://www.reddit.com/r/SydneyJobs/\n",
      "------------------------------------------------------------\n",
      "196. r/JobsDortmund\n",
      "   Description: Hier werden in Zukunft regionale Jobs aus Dortmund und der Region gepostet. Weitere Jobs findet Ihr ...\n",
      "   Members: 110, Online: 0\n",
      "   URL: https://www.reddit.com/r/JobsDortmund/\n",
      "------------------------------------------------------------\n",
      "197. r/AlabamaJobs\n",
      "   Description: This subreddit has been created to post job opportunities that are located in the state of Alabama, ...\n",
      "   Members: 406, Online: 0\n",
      "   URL: https://www.reddit.com/r/AlabamaJobs/\n",
      "------------------------------------------------------------\n",
      "198. r/php_jobs\n",
      "   Description: Jobs for PHP programmers\n",
      "   Members: 108, Online: 4\n",
      "   URL: https://www.reddit.com/r/php_jobs/\n",
      "------------------------------------------------------------\n",
      "199. r/Education_Jobs\n",
      "   Description: Education Jobs Listing\n",
      "   Members: 102, Online: 3\n",
      "   URL: https://www.reddit.com/r/Education_Jobs/\n",
      "------------------------------------------------------------\n",
      "200. r/JobsUlm\n",
      "   Description: Hier werden in Zukunft regionale Jobs aus Ulm und der Region gepostet. Weitere Jobs findet Ihr auch ...\n",
      "   Members: 85, Online: 0\n",
      "   URL: https://www.reddit.com/r/JobsUlm/\n",
      "------------------------------------------------------------\n",
      "201. r/EmailJobs\n",
      "   Description: Find great local, global and remote email marketing jobs here. Find and Post email marketing related...\n",
      "   Members: 744, Online: 5\n",
      "   URL: https://www.reddit.com/r/EmailJobs/\n",
      "------------------------------------------------------------\n",
      "202. r/SwissJobs\n",
      "   Description: Recruit in Switzerland. Automated posts are allowed, but no more than once a day.\n",
      "   Members: 586, Online: 1\n",
      "   URL: https://www.reddit.com/r/SwissJobs/\n",
      "------------------------------------------------------------\n",
      "203. r/FirefighterJobs\n",
      "   Description: New job postings for firefighter and/or related fields. Hiring process discussions encouraged. All s...\n",
      "   Members: 263, Online: 2\n",
      "   URL: https://www.reddit.com/r/FirefighterJobs/\n",
      "------------------------------------------------------------\n",
      "204. r/JobsRetail\n",
      "   Description: Community for publishing job openings in the retail industry, and also for people looking for new op...\n",
      "   Members: 66, Online: 2\n",
      "   URL: https://www.reddit.com/r/JobsRetail/\n",
      "------------------------------------------------------------\n",
      "205. r/LeetcodeJobs\n",
      "   Description: A community to post all the hiring needs for the tech industry, specially for developers of all rank...\n",
      "   Members: 349, Online: 4\n",
      "   URL: https://www.reddit.com/r/LeetcodeJobs/\n",
      "------------------------------------------------------------\n",
      "206. r/AutasticJobs\n",
      "   Description: This community is currently inactive. I am unable to delete subreddits, so it can either stay like t...\n",
      "   Members: 73, Online: 5\n",
      "   URL: https://www.reddit.com/r/AutasticJobs/\n",
      "------------------------------------------------------------\n",
      "207. r/JobsKrefeld\n",
      "   Description: Hier werden in Zukunft regionale Jobs aus Krefeld und der Region gepostet. Weitere Jobs findet Ihr a...\n",
      "   Members: 56, Online: 2\n",
      "   URL: https://www.reddit.com/r/JobsKrefeld/\n",
      "------------------------------------------------------------\n",
      "208. r/AviationJobs\n",
      "   Description: The goal here is to provide a place that people in the aviation career field can come to link up and...\n",
      "   Members: 663, Online: 2\n",
      "   URL: https://www.reddit.com/r/AviationJobs/\n",
      "------------------------------------------------------------\n",
      "209. r/jobs_at\n",
      "   Description: Ein Subreddit für alle, die in Österreich einen Job suchen oder ein interessantes/lustiges/spannende...\n",
      "   Members: 53, Online: 0\n",
      "   URL: https://www.reddit.com/r/jobs_at/\n",
      "------------------------------------------------------------\n",
      "210. r/startup_jobs\n",
      "   Description: Ask questions, share experiences, give advice about finding and securing jobs in early stage startup...\n",
      "   Members: 50, Online: 1\n",
      "   URL: https://www.reddit.com/r/startup_jobs/\n",
      "------------------------------------------------------------\n",
      "211. r/Engineering_Jobs\n",
      "   Description: You've done the work, you have the degree, now you need the right opportunity to kickstart your care...\n",
      "   Members: 379, Online: 3\n",
      "   URL: https://www.reddit.com/r/Engineering_Jobs/\n",
      "------------------------------------------------------------\n",
      "212. r/fullstack_jobs\n",
      "   Description: Full Stack jobs\n",
      "   Members: 657, Online: 0\n",
      "   URL: https://www.reddit.com/r/fullstack_jobs/\n",
      "------------------------------------------------------------\n",
      "213. r/JobsWolfsburg\n",
      "   Description: Hier werden in Zukunft regionale Jobs aus Wolfsburg und der Region gepostet. Weitere Jobs findet Ihr...\n",
      "   Members: 42, Online: 3\n",
      "   URL: https://www.reddit.com/r/JobsWolfsburg/\n",
      "------------------------------------------------------------\n",
      "214. r/JobsWuerzburg\n",
      "   Description: Hier werden in Zukunft regionale Jobs aus Würzburg und der Region gepostet. Weitere Jobs findet Ihr ...\n",
      "   Members: 319, Online: 3\n",
      "   URL: https://www.reddit.com/r/JobsWuerzburg/\n",
      "------------------------------------------------------------\n",
      "215. r/nintendo_jobs\n",
      "   Description: Jobs at Nintendo\n",
      "   Members: 35, Online: 5\n",
      "   URL: https://www.reddit.com/r/nintendo_jobs/\n",
      "------------------------------------------------------------\n",
      "216. r/programming_jobs\n",
      "   Description: \n",
      "   Members: 362, Online: 3\n",
      "   URL: https://www.reddit.com/r/programming_jobs/\n",
      "------------------------------------------------------------\n",
      "217. r/JobsDueren\n",
      "   Description: Hier werden in Zukunft regionale Jobs aus Düren und der Region gepostet. Weitere Jobs findet Ihr auc...\n",
      "   Members: 27, Online: 3\n",
      "   URL: https://www.reddit.com/r/JobsDueren/\n",
      "------------------------------------------------------------\n",
      "218. r/CaliJobs\n",
      "   Description: This subreddit offers resident of California Any resident of California looking for careers ranging ...\n",
      "   Members: 36, Online: 1\n",
      "   URL: https://www.reddit.com/r/CaliJobs/\n",
      "------------------------------------------------------------\n",
      "219. r/attorney_jobs\n",
      "   Description: Available attorney jobs throughout the United States and job related topics such as hiring trends or...\n",
      "   Members: 19, Online: 1\n",
      "   URL: https://www.reddit.com/r/attorney_jobs/\n",
      "------------------------------------------------------------\n",
      "220. r/FresnoJobs\n",
      "   Description: This is a subreddit that offers job opportunities to people all over the state of Fresno, USA.\n",
      "   Members: 26, Online: 3\n",
      "   URL: https://www.reddit.com/r/FresnoJobs/\n",
      "------------------------------------------------------------\n",
      "221. r/hartford_jobs\n",
      "   Description: \n",
      "   Members: 24, Online: 5\n",
      "   URL: https://www.reddit.com/r/hartford_jobs/\n",
      "------------------------------------------------------------\n",
      "222. r/JobsBielefeld\n",
      "   Description: Hier werden in Zukunft regionale Jobs aus Bielefeld und der Region gepostet. Weitere Jobs findet Ihr...\n",
      "   Members: 327, Online: 3\n",
      "   URL: https://www.reddit.com/r/JobsBielefeld/\n",
      "------------------------------------------------------------\n",
      "223. r/NiceJobs\n",
      "   Description: \n",
      "   Members: 13, Online: 1\n",
      "   URL: https://www.reddit.com/r/NiceJobs/\n",
      "------------------------------------------------------------\n",
      "224. r/OculusJobs\n",
      "   Description: A subreddit to connect Virtual Reality startups with talented individuals.\n",
      "   Members: 279, Online: 3\n",
      "   URL: https://www.reddit.com/r/OculusJobs/\n",
      "------------------------------------------------------------\n",
      "225. r/java_jobs\n",
      "   Description: Jobs for Java programmers\n",
      "   Members: 271, Online: 0\n",
      "   URL: https://www.reddit.com/r/java_jobs/\n",
      "------------------------------------------------------------\n",
      "226. r/oracle_jobs\n",
      "   Description: Lastest Oracle jobs (international, contract, permanent) and news. .\n",
      "   Members: 288, Online: 5\n",
      "   URL: https://www.reddit.com/r/oracle_jobs/\n",
      "------------------------------------------------------------\n",
      "227. r/RelocationJobs\n",
      "   Description: Find relocation jobs or post jobs openings to help people relocate.\n",
      "   Members: 284, Online: 3\n",
      "   URL: https://www.reddit.com/r/RelocationJobs/\n",
      "------------------------------------------------------------\n",
      "228. r/JobsMuenster\n",
      "   Description: Hier werden in Zukunft regionale Jobs aus Münster und der Region gepostet. Weitere Jobs findet Ihr a...\n",
      "   Members: 168, Online: 5\n",
      "   URL: https://www.reddit.com/r/JobsMuenster/\n",
      "------------------------------------------------------------\n",
      "229. r/datascience_jobs\n",
      "   Description: Data science\n",
      "   Members: 166, Online: 0\n",
      "   URL: https://www.reddit.com/r/datascience_jobs/\n",
      "------------------------------------------------------------\n",
      "230. r/ScienceJobs\n",
      "   Description: A community to share science and research positions worldwide. Anyone is welcome to post or share in...\n",
      "   Members: 619, Online: 4\n",
      "   URL: https://www.reddit.com/r/ScienceJobs/\n",
      "------------------------------------------------------------\n",
      "231. r/JobsNuernberg\n",
      "   Description: Hier werden in Zukunft regionale Jobs aus Nürnberg und der Region gepostet. Weitere Jobs findet Ihr ...\n",
      "   Members: 237, Online: 4\n",
      "   URL: https://www.reddit.com/r/JobsNuernberg/\n",
      "------------------------------------------------------------\n",
      "232. r/TokyoJobs\n",
      "   Description: Find jobs in Tokyo\n",
      "   Members: 430, Online: 0\n",
      "   URL: https://www.reddit.com/r/TokyoJobs/\n",
      "------------------------------------------------------------\n",
      "233. r/LibraryJobs\n",
      "   Description: A place to discuss Library jobs in USA\n",
      "   Members: 149, Online: 1\n",
      "   URL: https://www.reddit.com/r/LibraryJobs/\n",
      "------------------------------------------------------------\n",
      "234. r/LAFilmJobs\n",
      "   Description: A subreddit about film jobs only! Must list position needed AND PAYRATE! Spam and soliciting will ge...\n",
      "   Members: 240, Online: 3\n",
      "   URL: https://www.reddit.com/r/LAFilmJobs/\n",
      "------------------------------------------------------------\n",
      "235. r/JobsHannover\n",
      "   Description: Hier werden in Zukunft regionale Jobs aus Hannover und der Region gepostet. Weitere Jobs findet Ihr ...\n",
      "   Members: 230, Online: 2\n",
      "   URL: https://www.reddit.com/r/JobsHannover/\n",
      "------------------------------------------------------------\n",
      "236. r/AirflowJobs\n",
      "   Description: Interested in Apache Airflow? Want to work with it full-time? You've come to the right place!\n",
      "   Members: 232, Online: 4\n",
      "   URL: https://www.reddit.com/r/AirflowJobs/\n",
      "------------------------------------------------------------\n",
      "237. r/GreenJobs\n",
      "   Description: A place to post jobs and contracts related to Renewable Energy, Sustainable Agriculture, Ecology, En...\n",
      "   Members: 544, Online: 3\n",
      "   URL: https://www.reddit.com/r/GreenJobs/\n",
      "------------------------------------------------------------\n",
      "238. r/AngJobs\n",
      "   Description: Job board for angularJs/vuejs/frontend developers or full stack developers\n",
      "   Members: 290, Online: 3\n",
      "   URL: https://www.reddit.com/r/AngJobs/\n",
      "------------------------------------------------------------\n",
      "239. r/VermontJobs\n",
      "   Description: This subreddit has been created to post job opportunities that are located in Vermont, USA.\n",
      "   Members: 516, Online: 5\n",
      "   URL: https://www.reddit.com/r/VermontJobs/\n",
      "------------------------------------------------------------\n",
      "240. r/Canada_Jobs\n",
      "   Description: Find Latest Jobs in Canada https://www.governmentofcanadajobs.com\n",
      "   Members: 523, Online: 2\n",
      "   URL: https://www.reddit.com/r/Canada_Jobs/\n",
      "------------------------------------------------------------\n",
      "241. r/JobsAschaffenburg\n",
      "   Description: Hier werden in Zukunft regionale Jobs aus Aschaffenburg und der Region gepostet. Weitere Jobs findet...\n",
      "   Members: 115, Online: 5\n",
      "   URL: https://www.reddit.com/r/JobsAschaffenburg/\n",
      "------------------------------------------------------------\n",
      "242. r/LakelandJobs\n",
      "   Description: Hiring for your business in Lakeland, Florida? Post a note here starting with the tag [Hiring]. Are ...\n",
      "   Members: 392, Online: 2\n",
      "   URL: https://www.reddit.com/r/LakelandJobs/\n",
      "------------------------------------------------------------\n",
      "243. r/matlab_jobs\n",
      "   Description: Jobs for those who know MATLAB\n",
      "   Members: 232, Online: 0\n",
      "   URL: https://www.reddit.com/r/matlab_jobs/\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-28 15:22:16,450 - INFO - Closed web driver\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Enhanced Reddit Community Scraper - With Continuous Scrolling\n",
    "This script directly accesses the full communities search page for a keyword\n",
    "and scrolls continuously until no new content appears\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "import csv\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger('reddit_community_scraper')\n",
    "\n",
    "class RedditCommunityScraper:\n",
    "    def __init__(self):\n",
    "        # Set up Chrome options\n",
    "        self.options = Options()\n",
    "        self.options.add_argument('--headless')\n",
    "        self.options.add_argument('--disable-gpu')\n",
    "        self.options.add_argument('--no-sandbox')\n",
    "        self.options.add_argument('--disable-dev-shm-usage')\n",
    "        self.options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')\n",
    "        \n",
    "        # Initialize the driver\n",
    "        self.driver = webdriver.Chrome(options=self.options)\n",
    "        self.communities = []\n",
    "        \n",
    "    def scrape_communities(self, keyword, max_scrolls=30, wait_time=3):\n",
    "        \"\"\"\n",
    "        Directly access the full communities search page for a keyword and extract all data\n",
    "        with continuous scrolling until no new content appears\n",
    "        \n",
    "        Parameters:\n",
    "        - keyword: The search term to find communities\n",
    "        - max_scrolls: Maximum number of scroll attempts (default: 30)\n",
    "        - wait_time: Time to wait after each scroll in seconds (default: 3)\n",
    "        \"\"\"\n",
    "        # Reddit communities search URL\n",
    "        search_url = f\"https://www.reddit.com/search/?q={keyword}&type=communities\"\n",
    "        logger.info(f\"Accessing full communities search for keyword: {keyword}\")\n",
    "        logger.info(f\"Search URL: {search_url}\")\n",
    "        \n",
    "        try:\n",
    "            # Load the full communities search page\n",
    "            self.driver.get(search_url)\n",
    "            \n",
    "            # Wait for community results to load\n",
    "            WebDriverWait(self.driver, 10).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, \"div[data-testid='search-community']\"))\n",
    "            )\n",
    "            \n",
    "            # Give the page a moment to fully load\n",
    "            time.sleep(wait_time)\n",
    "            \n",
    "            # Initial extraction to compare against\n",
    "            initial_communities = self.extract_visible_communities()\n",
    "            current_community_count = len(initial_communities)\n",
    "            logger.info(f\"Initially found {current_community_count} communities\")\n",
    "            \n",
    "            # Scroll and extract until no new communities appear\n",
    "            last_height = self.driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            consecutive_no_new_communities = 0\n",
    "            scroll_count = 0\n",
    "            \n",
    "            while scroll_count < max_scrolls and consecutive_no_new_communities < 3:\n",
    "                # Scroll down\n",
    "                self.driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                scroll_count += 1\n",
    "                \n",
    "                # Wait for new content to load\n",
    "                time.sleep(wait_time)\n",
    "                \n",
    "                # Calculate new scroll height\n",
    "                new_height = self.driver.execute_script(\"return document.body.scrollHeight\")\n",
    "                \n",
    "                # If heights are the same, we might have reached the bottom\n",
    "                if new_height == last_height:\n",
    "                    consecutive_no_new_communities += 1\n",
    "                    logger.info(f\"No height change after scroll {scroll_count} - attempt {consecutive_no_new_communities}/3\")\n",
    "                else:\n",
    "                    last_height = new_height\n",
    "                    \n",
    "                    # Extract communities again\n",
    "                    current_communities = self.extract_visible_communities()\n",
    "                    \n",
    "                    # Check if we found new communities\n",
    "                    if len(current_communities) > current_community_count:\n",
    "                        new_communities = len(current_communities) - current_community_count\n",
    "                        logger.info(f\"Scroll {scroll_count}: Found {new_communities} new communities (total: {len(current_communities)})\")\n",
    "                        current_community_count = len(current_communities)\n",
    "                        consecutive_no_new_communities = 0\n",
    "                    else:\n",
    "                        consecutive_no_new_communities += 1\n",
    "                        logger.info(f\"No new communities after scroll {scroll_count} - attempt {consecutive_no_new_communities}/3\")\n",
    "                \n",
    "                # If we've had 3 consecutive scrolls with no new communities, we're likely done\n",
    "                if consecutive_no_new_communities >= 3:\n",
    "                    logger.info(f\"No new communities found for 3 consecutive scrolls - stopping\")\n",
    "                    break\n",
    "            \n",
    "            # Final extraction of all communities\n",
    "            self.communities = self.extract_visible_communities()\n",
    "            logger.info(f\"Total communities found after {scroll_count} scrolls: {len(self.communities)}\")\n",
    "            \n",
    "            return self.communities\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error accessing or processing search page: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def extract_visible_communities(self):\n",
    "        \"\"\"\n",
    "        Extract all currently visible community information from the page\n",
    "        Returns a list of community dictionaries\n",
    "        \"\"\"\n",
    "        communities = []\n",
    "        \n",
    "        try:\n",
    "            # Find all community elements\n",
    "            community_elements = self.driver.find_elements(By.CSS_SELECTOR, \"div[data-testid='search-community']\")\n",
    "            \n",
    "            # Process each community element\n",
    "            for element in community_elements:\n",
    "                try:\n",
    "                    # Extract subreddit name\n",
    "                    name_element = element.find_element(By.CSS_SELECTOR, \"span[id^='search-community-title']\")\n",
    "                    name = name_element.text.strip() if name_element else \"Unknown\"\n",
    "                    \n",
    "                    # Skip if we already have this community (by name)\n",
    "                    if any(c[\"name\"] == name for c in communities):\n",
    "                        continue\n",
    "                    \n",
    "                    # Extract description\n",
    "                    try:\n",
    "                        desc_element = element.find_element(By.CSS_SELECTOR, \"p[data-testid='search-subreddit-desc-text']\")\n",
    "                        description = desc_element.text.strip()\n",
    "                    except:\n",
    "                        description = \"\"\n",
    "                    \n",
    "                    # Extract member count and online count\n",
    "                    try:\n",
    "                        members_text = \"Unknown\"\n",
    "                        online_text = \"Unknown\"\n",
    "                        \n",
    "                        # Try to find the stats element with member counts\n",
    "                        stats_element = element.find_element(By.CSS_SELECTOR, \"div.text-12.text-neutral-content-weak\")\n",
    "                        if stats_element:\n",
    "                            stats_text = stats_element.text.strip()\n",
    "                            \n",
    "                            # Example: \"16M members · 4.5K online\"\n",
    "                            parts = stats_text.split('·')\n",
    "                            \n",
    "                            if len(parts) >= 1 and \"members\" in parts[0]:\n",
    "                                members_text = parts[0].replace(\"members\", \"\").strip()\n",
    "                                \n",
    "                            if len(parts) >= 2 and \"online\" in parts[1]:\n",
    "                                online_text = parts[1].replace(\"online\", \"\").strip()\n",
    "                    except:\n",
    "                        members_text = \"Unknown\"\n",
    "                        online_text = \"Unknown\"\n",
    "                    \n",
    "                    # Extract subreddit URL\n",
    "                    try:\n",
    "                        url_element = element.find_element(By.TAG_NAME, \"a\")\n",
    "                        url = url_element.get_attribute(\"href\") if url_element else \"\"\n",
    "                    except:\n",
    "                        url = \"\"\n",
    "                    \n",
    "                    community_info = {\n",
    "                        \"name\": name,\n",
    "                        \"description\": description,\n",
    "                        \"members\": members_text,\n",
    "                        \"online\": online_text,\n",
    "                        \"url\": url\n",
    "                    }\n",
    "                    \n",
    "                    communities.append(community_info)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error extracting community data: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            return communities\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error extracting visible communities: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def save_to_csv(self, filename=None):\n",
    "        \"\"\"Save community information to a CSV file.\"\"\"\n",
    "        if filename is None:\n",
    "            filename = \"reddit_communities.csv\"\n",
    "            \n",
    "        try:\n",
    "            with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "                fieldnames = ['name', 'description', 'members', 'online', 'url']\n",
    "                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "                \n",
    "                writer.writeheader()\n",
    "                for community in self.communities:\n",
    "                    writer.writerow(community)\n",
    "            \n",
    "            logger.info(f\"Saved {len(self.communities)} communities to {filename}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving to CSV: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def print_results(self):\n",
    "        \"\"\"Print the extracted community information.\"\"\"\n",
    "        print(f\"\\nFound {len(self.communities)} communities related to the search:\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        for i, comm in enumerate(self.communities, 1):\n",
    "            print(f\"{i}. {comm['name']}\")\n",
    "            print(f\"   Description: {comm['description'][:100]}...\" if len(comm['description']) > 100 else f\"   Description: {comm['description']}\")\n",
    "            print(f\"   Members: {comm['members']}, Online: {comm['online']}\")\n",
    "            print(f\"   URL: {comm['url']}\")\n",
    "            print(\"-\" * 60)\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Close the web driver.\"\"\"\n",
    "        self.driver.quit()\n",
    "        logger.info(\"Closed web driver\")\n",
    "\n",
    "def main():\n",
    "    # Define the keyword to search\n",
    "    keyword = \"jobs\"\n",
    "    \n",
    "    # Create the scraper\n",
    "    scraper = RedditCommunityScraper()\n",
    "    \n",
    "    try:\n",
    "        # Scrape communities information with continuous scrolling\n",
    "        scraper.scrape_communities(keyword)\n",
    "        \n",
    "        # Print the results\n",
    "        scraper.print_results()\n",
    "        \n",
    "        # Save the results to a CSV file\n",
    "        scraper.save_to_csv(f\"{keyword}_communities.csv\")\n",
    "        \n",
    "        return scraper.communities\n",
    "    \n",
    "    finally:\n",
    "        # Close the driver\n",
    "        scraper.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb7fcaf2-a434-430e-8a6c-f0c680adeab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>members</th>\n",
       "      <th>online</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>r/jobs</td>\n",
       "      <td>/r/jobs is the number one community for advice...</td>\n",
       "      <td>2M</td>\n",
       "      <td>535</td>\n",
       "      <td>https://www.reddit.com/r/jobs/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>r/RemoteJobs</td>\n",
       "      <td>Remote jobs: the future of work! READ RULES BE...</td>\n",
       "      <td>273K</td>\n",
       "      <td>46</td>\n",
       "      <td>https://www.reddit.com/r/RemoteJobs/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>r/DesignJobs</td>\n",
       "      <td>Some redditors are skilled professionals, some...</td>\n",
       "      <td>163K</td>\n",
       "      <td>26</td>\n",
       "      <td>https://www.reddit.com/r/DesignJobs/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>r/AskReddit</td>\n",
       "      <td>r/AskReddit is the place to ask and answer tho...</td>\n",
       "      <td>55M</td>\n",
       "      <td>11K</td>\n",
       "      <td>https://www.reddit.com/r/AskReddit/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>r/antiwork</td>\n",
       "      <td>A subreddit for those who want to end work, ar...</td>\n",
       "      <td>2.9M</td>\n",
       "      <td>1.1K</td>\n",
       "      <td>https://www.reddit.com/r/antiwork/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>r/VermontJobs</td>\n",
       "      <td>This subreddit has been created to post job op...</td>\n",
       "      <td>516</td>\n",
       "      <td>5</td>\n",
       "      <td>https://www.reddit.com/r/VermontJobs/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>r/Canada_Jobs</td>\n",
       "      <td>Find Latest Jobs in Canada https://www.governm...</td>\n",
       "      <td>523</td>\n",
       "      <td>2</td>\n",
       "      <td>https://www.reddit.com/r/Canada_Jobs/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>r/JobsAschaffenburg</td>\n",
       "      <td>Hier werden in Zukunft regionale Jobs aus Asch...</td>\n",
       "      <td>115</td>\n",
       "      <td>5</td>\n",
       "      <td>https://www.reddit.com/r/JobsAschaffenburg/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>r/LakelandJobs</td>\n",
       "      <td>Hiring for your business in Lakeland, Florida?...</td>\n",
       "      <td>392</td>\n",
       "      <td>2</td>\n",
       "      <td>https://www.reddit.com/r/LakelandJobs/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>r/matlab_jobs</td>\n",
       "      <td>Jobs for those who know MATLAB</td>\n",
       "      <td>232</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.reddit.com/r/matlab_jobs/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>243 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    name                                        description  \\\n",
       "0                 r/jobs  /r/jobs is the number one community for advice...   \n",
       "1           r/RemoteJobs  Remote jobs: the future of work! READ RULES BE...   \n",
       "2           r/DesignJobs  Some redditors are skilled professionals, some...   \n",
       "3            r/AskReddit  r/AskReddit is the place to ask and answer tho...   \n",
       "4             r/antiwork  A subreddit for those who want to end work, ar...   \n",
       "..                   ...                                                ...   \n",
       "238        r/VermontJobs  This subreddit has been created to post job op...   \n",
       "239        r/Canada_Jobs  Find Latest Jobs in Canada https://www.governm...   \n",
       "240  r/JobsAschaffenburg  Hier werden in Zukunft regionale Jobs aus Asch...   \n",
       "241       r/LakelandJobs  Hiring for your business in Lakeland, Florida?...   \n",
       "242        r/matlab_jobs                     Jobs for those who know MATLAB   \n",
       "\n",
       "    members online                                          url  \n",
       "0        2M    535               https://www.reddit.com/r/jobs/  \n",
       "1      273K     46         https://www.reddit.com/r/RemoteJobs/  \n",
       "2      163K     26         https://www.reddit.com/r/DesignJobs/  \n",
       "3       55M    11K          https://www.reddit.com/r/AskReddit/  \n",
       "4      2.9M   1.1K           https://www.reddit.com/r/antiwork/  \n",
       "..      ...    ...                                          ...  \n",
       "238     516      5        https://www.reddit.com/r/VermontJobs/  \n",
       "239     523      2        https://www.reddit.com/r/Canada_Jobs/  \n",
       "240     115      5  https://www.reddit.com/r/JobsAschaffenburg/  \n",
       "241     392      2       https://www.reddit.com/r/LakelandJobs/  \n",
       "242     232      0        https://www.reddit.com/r/matlab_jobs/  \n",
       "\n",
       "[243 rows x 5 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('jobs_communities.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b8552543-1410-4d2d-a5cb-adb45b4623ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 community names without 'r/' prefix:\n",
      "1. funny\n",
      "2. AskReddit\n",
      "3. aww\n",
      "4. memes\n",
      "5. Showerthoughts\n",
      "6. pics\n",
      "7. Jokes\n",
      "8. AmItheAsshole\n",
      "9. Futurology\n",
      "10. personalfinance\n",
      "\n",
      "Saved 243 community names to 'community_names.txt'\n",
      "\n",
      "Community names list (first 10): ['funny', 'AskReddit', 'aww', 'memes', 'Showerthoughts', 'pics', 'Jokes', 'AmItheAsshole', 'Futurology', 'personalfinance']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Script to extract community names from sorted DataFrame without the 'r/' prefix\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('jobs_communities.csv')\n",
    "\n",
    "# Function to convert member count to numeric value\n",
    "def convert_member_count(member_str):\n",
    "    if pd.isna(member_str) or member_str == 'Unknown':\n",
    "        return 0\n",
    "    \n",
    "    # Remove any non-numeric characters except decimal point and K/M/B\n",
    "    member_str = member_str.strip()\n",
    "    \n",
    "    # Convert to numeric value\n",
    "    if 'K' in member_str:\n",
    "        return float(member_str.replace('K', '')) * 1000\n",
    "    elif 'M' in member_str:\n",
    "        return float(member_str.replace('M', '')) * 1000000\n",
    "    elif 'B' in member_str:\n",
    "        return float(member_str.replace('B', '')) * 1000000000\n",
    "    else:\n",
    "        # Try to extract numeric value using regex\n",
    "        numeric_match = re.search(r'(\\d+\\.?\\d*)', member_str)\n",
    "        if numeric_match:\n",
    "            return float(numeric_match.group(1))\n",
    "        return 0\n",
    "\n",
    "# Create a numeric column for sorting\n",
    "df['member_count_numeric'] = df['members'].apply(convert_member_count)\n",
    "\n",
    "# Sort by member count (descending)\n",
    "df_sorted = df.sort_values(by='member_count_numeric', ascending=False)\n",
    "\n",
    "# Function to remove 'r/' prefix from community names\n",
    "def remove_r_prefix(name):\n",
    "    if isinstance(name, str) and name.startswith('r/'):\n",
    "        return name[2:]\n",
    "    return name\n",
    "    \n",
    "df_sorted['clean_name'] = df_sorted['name'].apply(remove_r_prefix)\n",
    "community_names = df_sorted['clean_name'].tolist()\n",
    "print(\"First 10 community names without 'r/' prefix:\")\n",
    "for i, name in enumerate(community_names[:10]):\n",
    "    print(f\"{i+1}. {name}\")\n",
    "\n",
    "# Save the list to a text file\n",
    "with open('community_names.txt', 'w') as f:\n",
    "    for name in community_names:\n",
    "        f.write(f\"{name}\\n\")\n",
    "\n",
    "print(f\"\\nSaved {len(community_names)} community names to 'community_names.txt'\")\n",
    "print(\"\\nCommunity names list (first 10):\", community_names[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bb031d-fd39-4eee-9995-c9b5a5938228",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cebb023-7e6e-43ab-892a-6813cf17712b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcca819-572f-483a-8c56-aba60f672c7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81c0d6f2-cc1e-45fc-b84f-0667badd9f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-28 15:16:58,911 - INFO - Searching Reddit for: data science job\n",
      "2025-04-28 15:16:58,912 - INFO - Search URL: https://www.reddit.com/search/?q=data+science+job\n",
      "2025-04-28 15:17:03,825 - INFO - Found 7 posts in search results\n",
      "2025-04-28 15:17:03,984 - INFO - Extracted data for 7 posts\n",
      "2025-04-28 15:17:03,984 - INFO - Processing post 1/3: Update half a year into my first data science job...\n",
      "2025-04-28 15:17:03,984 - INFO - Scraping post: Update half a year into my first data science job...\n",
      "2025-04-28 15:17:08,181 - INFO - Found 10 comments in post\n",
      "2025-04-28 15:17:10,452 - INFO - Processing post 2/3: How's the job market for data scientists?\n",
      "2025-04-28 15:17:10,453 - INFO - Scraping post: How's the job market for data scientists?\n",
      "2025-04-28 15:17:14,729 - INFO - Found 9 comments in post\n",
      "2025-04-28 15:17:17,389 - INFO - Processing post 3/3: Should I accept this data science job (i.e. how bad is the job market?)\n",
      "2025-04-28 15:17:17,389 - INFO - Scraping post: Should I accept this data science job (i.e. how bad is the job market?)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 350\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;66;03m# Process search results\u001b[39;00m\n\u001b[1;32m--> 350\u001b[0m     scraper\u001b[38;5;241m.\u001b[39mprocess_search_results(search_query, max_posts\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m    352\u001b[0m     \u001b[38;5;66;03m# Save the results to CSV files\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[13], line 222\u001b[0m, in \u001b[0;36mRedditCommentScraper.process_search_results\u001b[1;34m(self, search_query, max_posts)\u001b[0m\n\u001b[0;32m    221\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing post \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(posts_to_scrape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpost[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 222\u001b[0m post_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscrape_post_and_comments(post[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m'\u001b[39m], post[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m post_data:\n",
      "Cell \u001b[1;32mIn[13], line 87\u001b[0m, in \u001b[0;36mRedditCommentScraper.scrape_post_and_comments\u001b[1;34m(self, post_url, post_title)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;66;03m# Navigate to the post\u001b[39;00m\n\u001b[1;32m---> 87\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdriver\u001b[38;5;241m.\u001b[39mget(post_url)\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;66;03m# Wait for the post content to load\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:353\u001b[0m, in \u001b[0;36mWebDriver.get\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Loads a web page in the current browser session.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 353\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecute(Command\u001b[38;5;241m.\u001b[39mGET, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m: url})\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:342\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    340\u001b[0m         params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msessionId\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession_id\n\u001b[1;32m--> 342\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\remote_connection.py:297\u001b[0m, in \u001b[0;36mRemoteConnection.execute\u001b[1;34m(self, command, params)\u001b[0m\n\u001b[0;32m    296\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 297\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(command_info[\u001b[38;5;241m0\u001b[39m], url, body\u001b[38;5;241m=\u001b[39mdata)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\remote_connection.py:318\u001b[0m, in \u001b[0;36mRemoteConnection._request\u001b[1;34m(self, method, url, body)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeep_alive:\n\u001b[1;32m--> 318\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conn\u001b[38;5;241m.\u001b[39mrequest(method, url, body\u001b[38;5;241m=\u001b[39mbody, headers\u001b[38;5;241m=\u001b[39mheaders)\n\u001b[0;32m    319\u001b[0m     statuscode \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\request.py:81\u001b[0m, in \u001b[0;36mRequestMethods.request\u001b[1;34m(self, method, url, fields, headers, **urlopen_kw)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_encode_body(\n\u001b[0;32m     82\u001b[0m         method, url, fields\u001b[38;5;241m=\u001b[39mfields, headers\u001b[38;5;241m=\u001b[39mheaders, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39murlopen_kw\n\u001b[0;32m     83\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\request.py:173\u001b[0m, in \u001b[0;36mRequestMethods.request_encode_body\u001b[1;34m(self, method, url, fields, headers, encode_multipart, multipart_boundary, **urlopen_kw)\u001b[0m\n\u001b[0;32m    171\u001b[0m extra_kw\u001b[38;5;241m.\u001b[39mupdate(urlopen_kw)\n\u001b[1;32m--> 173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murlopen(method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mextra_kw)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\poolmanager.py:376\u001b[0m, in \u001b[0;36mPoolManager.urlopen\u001b[1;34m(self, method, url, redirect, **kw)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 376\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(method, u\u001b[38;5;241m.\u001b[39mrequest_uri, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    378\u001b[0m redirect_location \u001b[38;5;241m=\u001b[39m redirect \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mget_redirect_location()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:715\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    714\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[1;32m--> 715\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    716\u001b[0m     conn,\n\u001b[0;32m    717\u001b[0m     method,\n\u001b[0;32m    718\u001b[0m     url,\n\u001b[0;32m    719\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    720\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    721\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    722\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    723\u001b[0m )\n\u001b[0;32m    725\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[0;32m    726\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[0;32m    728\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:467\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    463\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    464\u001b[0m             \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[0;32m    465\u001b[0m             \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[0;32m    466\u001b[0m             \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m--> 467\u001b[0m             six\u001b[38;5;241m.\u001b[39mraise_from(e, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    468\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:462\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 462\u001b[0m     httplib_response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[0;32m    463\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    464\u001b[0m     \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[0;32m    465\u001b[0m     \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[0;32m    466\u001b[0m     \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:1386\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1385\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1386\u001b[0m     response\u001b[38;5;241m.\u001b[39mbegin()\n\u001b[0;32m   1387\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:325\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    324\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 325\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_status()\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:286\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 286\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline(_MAXLINE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 706\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[0;32m    707\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 430\u001b[0m\n\u001b[0;32m    427\u001b[0m     process_html_file(html_file_path)\n\u001b[0;32m    428\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    429\u001b[0m     \u001b[38;5;66;03m# Otherwise, do a live search and scrape\u001b[39;00m\n\u001b[1;32m--> 430\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[13], line 379\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    375\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m scraper\u001b[38;5;241m.\u001b[39mposts\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    378\u001b[0m     \u001b[38;5;66;03m# Close the driver\u001b[39;00m\n\u001b[1;32m--> 379\u001b[0m     scraper\u001b[38;5;241m.\u001b[39mclose()\n",
      "Cell \u001b[1;32mIn[13], line 338\u001b[0m, in \u001b[0;36mRedditCommentScraper.close\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    336\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclose\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    337\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Close the web driver\"\"\"\u001b[39;00m\n\u001b[1;32m--> 338\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdriver\u001b[38;5;241m.\u001b[39mquit()\n\u001b[0;32m    339\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClosed web driver\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\chromium\\webdriver.py:192\u001b[0m, in \u001b[0;36mChromiumDriver.quit\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m--> 192\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice\u001b[38;5;241m.\u001b[39mstop()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\common\\service.py:146\u001b[0m, in \u001b[0;36mService.stop\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 146\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend_remote_shutdown_command()\n\u001b[0;32m    147\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\common\\service.py:126\u001b[0m, in \u001b[0;36mService.send_remote_shutdown_command\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Dispatch an HTTP request to the shutdown endpoint for the service in\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;124;03man attempt to stop it.\"\"\"\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 126\u001b[0m     request\u001b[38;5;241m.\u001b[39murlopen(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/shutdown\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m URLError:\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\urllib\\request.py:216\u001b[0m, in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    215\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[1;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m opener\u001b[38;5;241m.\u001b[39mopen(url, data, timeout)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\urllib\\request.py:519\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    516\u001b[0m     req \u001b[38;5;241m=\u001b[39m meth(req)\n\u001b[0;32m    518\u001b[0m sys\u001b[38;5;241m.\u001b[39maudit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murllib.Request\u001b[39m\u001b[38;5;124m'\u001b[39m, req\u001b[38;5;241m.\u001b[39mfull_url, req\u001b[38;5;241m.\u001b[39mdata, req\u001b[38;5;241m.\u001b[39mheaders, req\u001b[38;5;241m.\u001b[39mget_method())\n\u001b[1;32m--> 519\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_open(req, data)\n\u001b[0;32m    521\u001b[0m \u001b[38;5;66;03m# post-process response\u001b[39;00m\n\u001b[0;32m    522\u001b[0m meth_name \u001b[38;5;241m=\u001b[39m protocol\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_response\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\urllib\\request.py:536\u001b[0m, in \u001b[0;36mOpenerDirector._open\u001b[1;34m(self, req, data)\u001b[0m\n\u001b[0;32m    533\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m    535\u001b[0m protocol \u001b[38;5;241m=\u001b[39m req\u001b[38;5;241m.\u001b[39mtype\n\u001b[1;32m--> 536\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_chain(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_open, protocol, protocol \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m    537\u001b[0m                           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_open\u001b[39m\u001b[38;5;124m'\u001b[39m, req)\n\u001b[0;32m    538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[0;32m    539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\urllib\\request.py:496\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[0;32m    495\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[1;32m--> 496\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m    497\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    498\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\urllib\\request.py:1377\u001b[0m, in \u001b[0;36mHTTPHandler.http_open\u001b[1;34m(self, req)\u001b[0m\n\u001b[0;32m   1376\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttp_open\u001b[39m(\u001b[38;5;28mself\u001b[39m, req):\n\u001b[1;32m-> 1377\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_open(http\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mHTTPConnection, req)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\urllib\\request.py:1352\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[1;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[0;32m   1350\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err: \u001b[38;5;66;03m# timeout error\u001b[39;00m\n\u001b[0;32m   1351\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m URLError(err)\n\u001b[1;32m-> 1352\u001b[0m     r \u001b[38;5;241m=\u001b[39m h\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[0;32m   1353\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m   1354\u001b[0m     h\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:1386\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1385\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1386\u001b[0m         response\u001b[38;5;241m.\u001b[39mbegin()\n\u001b[0;32m   1387\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[0;32m   1388\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:325\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    324\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 325\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_status()\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:286\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 286\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline(_MAXLINE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[0;32m    288\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Reddit Data Science Posts and Comments Scraper\n",
    "This script scrapes posts and their comments from Reddit for \"data science job\" posts\n",
    "\"\"\"\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import re\n",
    "import logging\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger('reddit_comment_scraper')\n",
    "\n",
    "class RedditCommentScraper:\n",
    "    def __init__(self):\n",
    "        # Set up Chrome options\n",
    "        self.options = Options()\n",
    "        self.options.add_argument('--headless')\n",
    "        self.options.add_argument('--disable-gpu')\n",
    "        self.options.add_argument('--no-sandbox')\n",
    "        self.options.add_argument('--disable-dev-shm-usage')\n",
    "        self.options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')\n",
    "        \n",
    "        # Initialize the driver\n",
    "        self.driver = webdriver.Chrome(options=self.options)\n",
    "        self.posts = []\n",
    "        \n",
    "    def search_posts(self, search_query):\n",
    "        \"\"\"\n",
    "        Search for posts related to the search query\n",
    "        \"\"\"\n",
    "        # Format the search query for URL\n",
    "        formatted_query = search_query.replace(' ', '+')\n",
    "        search_url = f\"https://www.reddit.com/search/?q={formatted_query}\"\n",
    "        logger.info(f\"Searching Reddit for: {search_query}\")\n",
    "        logger.info(f\"Search URL: {search_url}\")\n",
    "        \n",
    "        try:\n",
    "            # Load the search page\n",
    "            self.driver.get(search_url)\n",
    "            \n",
    "            # Wait for post results to load\n",
    "            WebDriverWait(self.driver, 10).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, \"a[data-testid='post-title']\"))\n",
    "            )\n",
    "            \n",
    "            # Give the page a moment to fully load\n",
    "            time.sleep(3)\n",
    "            \n",
    "            # Find all post links\n",
    "            post_links = self.driver.find_elements(By.CSS_SELECTOR, \"a[data-testid='post-title']\")\n",
    "            logger.info(f\"Found {len(post_links)} posts in search results\")\n",
    "            \n",
    "            # Extract post URLs and titles\n",
    "            posts_data = []\n",
    "            for link in post_links:\n",
    "                try:\n",
    "                    url = link.get_attribute(\"href\")\n",
    "                    title = link.find_element(By.TAG_NAME, \"faceplate-screen-reader-content\").text.strip()\n",
    "                    posts_data.append({\"url\": url, \"title\": title})\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error extracting post data: {e}\")\n",
    "            \n",
    "            logger.info(f\"Extracted data for {len(posts_data)} posts\")\n",
    "            return posts_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error searching Reddit: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def scrape_post_and_comments(self, post_url, post_title):\n",
    "        \"\"\"\n",
    "        Scrape a specific post and all its comments\n",
    "        \"\"\"\n",
    "        logger.info(f\"Scraping post: {post_title}\")\n",
    "        \n",
    "        try:\n",
    "            # Navigate to the post\n",
    "            self.driver.get(post_url)\n",
    "            \n",
    "            # Wait for the post content to load\n",
    "            WebDriverWait(self.driver, 10).until(\n",
    "                EC.presence_of_element_located((By.TAG_NAME, \"shreddit-comment\"))\n",
    "            )\n",
    "            \n",
    "            # Give the page a moment to fully load\n",
    "            time.sleep(3)\n",
    "            \n",
    "            # Extract post author\n",
    "            try:\n",
    "                post_author_element = self.driver.find_element(By.CSS_SELECTOR, \"a[data-testid='post_author_link']\")\n",
    "                post_author = post_author_element.text.strip()\n",
    "            except NoSuchElementException:\n",
    "                post_author = \"Unknown\"\n",
    "            \n",
    "            # Create post data structure\n",
    "            post_data = {\n",
    "                \"title\": post_title,\n",
    "                \"url\": post_url,\n",
    "                \"author\": post_author,\n",
    "                \"comments\": []\n",
    "            }\n",
    "            \n",
    "            # Find all comments\n",
    "            comment_elements = self.driver.find_elements(By.TAG_NAME, \"shreddit-comment\")\n",
    "            logger.info(f\"Found {len(comment_elements)} comments in post\")\n",
    "            \n",
    "            # Process each comment\n",
    "            for comment in comment_elements:\n",
    "                try:\n",
    "                    # Extract username\n",
    "                    try:\n",
    "                        # Try to find the username link\n",
    "                        username_element = comment.find_element(By.CSS_SELECTOR, \"a.text-12.font-bold\")\n",
    "                        username = username_element.text.strip()\n",
    "                    except NoSuchElementException:\n",
    "                        # If the link is not found, try to find the username span (for deleted users)\n",
    "                        try:\n",
    "                            username_span = comment.find_element(By.CSS_SELECTOR, \"span.text-neutral-content-weak\")\n",
    "                            username = username_span.text.strip()\n",
    "                        except NoSuchElementException:\n",
    "                            # If all else fails, check if there's any username element\n",
    "                            try:\n",
    "                                username_element = comment.find_element(By.CSS_SELECTOR, \".author-name-meta a\")\n",
    "                                username = username_element.text.strip()\n",
    "                            except:\n",
    "                                username = \"Unknown User\"\n",
    "                    \n",
    "                    # Extract comment content\n",
    "                    try:\n",
    "                        content_element = comment.find_element(By.CSS_SELECTOR, \"div[slot='comment']\")\n",
    "                        content = content_element.text.strip()\n",
    "                    except NoSuchElementException:\n",
    "                        content = \"No content found\"\n",
    "                    \n",
    "                    # Add to comments list\n",
    "                    post_data[\"comments\"].append({\n",
    "                        \"username\": username,\n",
    "                        \"content\": content\n",
    "                    })\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error extracting comment data: {e}\")\n",
    "            \n",
    "            # Click on \"More replies\" buttons to load additional comments\n",
    "            try:\n",
    "                more_replies_buttons = self.driver.find_elements(By.XPATH, \"//button[contains(text(), 'more repl')]\")\n",
    "                for button in more_replies_buttons[:5]:  # Limit to 5 to avoid too many clicks\n",
    "                    try:\n",
    "                        button.click()\n",
    "                        time.sleep(2)  # Wait for new comments to load\n",
    "                        \n",
    "                        # Get the newly loaded comments\n",
    "                        new_comments = self.driver.find_elements(By.TAG_NAME, \"shreddit-comment\")\n",
    "                        logger.info(f\"Loaded {len(new_comments) - len(comment_elements)} additional comments\")\n",
    "                        \n",
    "                        # Process the new comments (only the ones we haven't processed yet)\n",
    "                        for comment in new_comments[len(comment_elements):]:\n",
    "                            try:\n",
    "                                # Extract username\n",
    "                                try:\n",
    "                                    username_element = comment.find_element(By.CSS_SELECTOR, \"a.text-12.font-bold\")\n",
    "                                    username = username_element.text.strip()\n",
    "                                except NoSuchElementException:\n",
    "                                    try:\n",
    "                                        username_span = comment.find_element(By.CSS_SELECTOR, \"span.text-neutral-content-weak\")\n",
    "                                        username = username_span.text.strip()\n",
    "                                    except:\n",
    "                                        username = \"Unknown User\"\n",
    "                                \n",
    "                                # Extract comment content\n",
    "                                try:\n",
    "                                    content_element = comment.find_element(By.CSS_SELECTOR, \"div[slot='comment']\")\n",
    "                                    content = content_element.text.strip()\n",
    "                                except NoSuchElementException:\n",
    "                                    content = \"No content found\"\n",
    "                                \n",
    "                                # Add to comments list\n",
    "                                post_data[\"comments\"].append({\n",
    "                                    \"username\": username,\n",
    "                                    \"content\": content\n",
    "                                })\n",
    "                                \n",
    "                            except Exception as e:\n",
    "                                logger.error(f\"Error extracting new comment data: {e}\")\n",
    "                        \n",
    "                        # Update our list of processed comments\n",
    "                        comment_elements = new_comments\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"Error clicking more replies button: {e}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error finding more replies buttons: {e}\")\n",
    "            \n",
    "            return post_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error scraping post {post_url}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def process_search_results(self, search_query, max_posts=5):\n",
    "        \"\"\"\n",
    "        Process the search results and scrape posts and comments\n",
    "        \"\"\"\n",
    "        # Search for posts\n",
    "        posts_data = self.search_posts(search_query)\n",
    "        \n",
    "        # Limit to max_posts\n",
    "        posts_to_scrape = posts_data[:max_posts]\n",
    "        \n",
    "        # Scrape each post\n",
    "        for i, post in enumerate(posts_to_scrape):\n",
    "            logger.info(f\"Processing post {i+1}/{len(posts_to_scrape)}: {post['title']}\")\n",
    "            post_data = self.scrape_post_and_comments(post['url'], post['title'])\n",
    "            \n",
    "            if post_data:\n",
    "                self.posts.append(post_data)\n",
    "            \n",
    "            # Wait between posts to avoid rate limiting\n",
    "            if i < len(posts_to_scrape) - 1:\n",
    "                time.sleep(2)\n",
    "        \n",
    "        return self.posts\n",
    "    \n",
    "    def save_to_csv(self, filename_prefix=None):\n",
    "        \"\"\"\n",
    "        Save the scraped data to CSV files (one for posts, one for comments)\n",
    "        \"\"\"\n",
    "        if filename_prefix is None:\n",
    "            filename_prefix = \"reddit_data\"\n",
    "        \n",
    "        posts_filename = f\"{filename_prefix}_posts.csv\"\n",
    "        comments_filename = f\"{filename_prefix}_comments.csv\"\n",
    "        \n",
    "        # Save posts to CSV\n",
    "        try:\n",
    "            with open(posts_filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "                fieldnames = ['title', 'url', 'author', 'comment_count']\n",
    "                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "                \n",
    "                writer.writeheader()\n",
    "                for post in self.posts:\n",
    "                    writer.writerow({\n",
    "                        'title': post['title'],\n",
    "                        'url': post['url'],\n",
    "                        'author': post['author'],\n",
    "                        'comment_count': len(post['comments'])\n",
    "                    })\n",
    "            \n",
    "            logger.info(f\"Saved {len(self.posts)} posts to {posts_filename}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving posts to CSV: {e}\")\n",
    "        \n",
    "        # Save comments to CSV\n",
    "        try:\n",
    "            with open(comments_filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "                fieldnames = ['post_title', 'post_url', 'username', 'comment_content']\n",
    "                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "                \n",
    "                writer.writeheader()\n",
    "                for post in self.posts:\n",
    "                    for comment in post['comments']:\n",
    "                        writer.writerow({\n",
    "                            'post_title': post['title'],\n",
    "                            'post_url': post['url'],\n",
    "                            'username': comment['username'],\n",
    "                            'comment_content': comment['content']\n",
    "                        })\n",
    "            \n",
    "            total_comments = sum(len(post['comments']) for post in self.posts)\n",
    "            logger.info(f\"Saved {total_comments} comments to {comments_filename}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving comments to CSV: {e}\")\n",
    "    \n",
    "    def extract_comments_from_html(self, html_content):\n",
    "        \"\"\"\n",
    "        Extract comments from pre-downloaded HTML content\n",
    "        \"\"\"\n",
    "        logger.info(\"Extracting comments from provided HTML content\")\n",
    "        comments = []\n",
    "        \n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        \n",
    "        # Find all comment elements\n",
    "        comment_elements = soup.find_all('shreddit-comment')\n",
    "        logger.info(f\"Found {len(comment_elements)} comment elements in HTML\")\n",
    "        \n",
    "        # Process each comment element\n",
    "        for comment in comment_elements:\n",
    "            try:\n",
    "                # Extract username\n",
    "                username = \"Unknown User\"\n",
    "                \n",
    "                # Try different ways to get the username\n",
    "                username_element = comment.select_one(\"a.text-12.font-bold\")\n",
    "                if username_element:\n",
    "                    username = username_element.text.strip()\n",
    "                else:\n",
    "                    # Try for deleted users\n",
    "                    deleted_username = comment.select_one(\"span.text-neutral-content-weak\")\n",
    "                    if deleted_username:\n",
    "                        username = deleted_username.text.strip()\n",
    "                    else:\n",
    "                        # Try another selector for usernames\n",
    "                        author_element = comment.select_one(\".author-name-meta a, .author-name-meta span\")\n",
    "                        if author_element:\n",
    "                            username = author_element.text.strip()\n",
    "                \n",
    "                # Extract comment content\n",
    "                content = \"No content found\"\n",
    "                content_element = comment.select_one(\"div[slot='comment']\")\n",
    "                if content_element:\n",
    "                    content = content_element.text.strip()\n",
    "                \n",
    "                # Add to comments list\n",
    "                comments.append({\n",
    "                    \"username\": username,\n",
    "                    \"content\": content\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error extracting comment data from HTML: {e}\")\n",
    "        \n",
    "        logger.info(f\"Extracted {len(comments)} comments from HTML\")\n",
    "        return comments\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Close the web driver\"\"\"\n",
    "        self.driver.quit()\n",
    "        logger.info(\"Closed web driver\")\n",
    "\n",
    "def main():\n",
    "    # Define the search query\n",
    "    search_query = \"data science job\"\n",
    "    \n",
    "    # Create the scraper\n",
    "    scraper = RedditCommentScraper()\n",
    "    \n",
    "    try:\n",
    "        # Process search results\n",
    "        scraper.process_search_results(search_query, max_posts=3)\n",
    "        \n",
    "        # Save the results to CSV files\n",
    "        scraper.save_to_csv(\"data_science_job\")\n",
    "        \n",
    "        # Print a summary of the results\n",
    "        print(f\"\\nFound {len(scraper.posts)} posts about data science jobs:\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        for i, post in enumerate(scraper.posts, 1):\n",
    "            print(f\"Post {i}: {post['title']}\")\n",
    "            print(f\"Author: {post['author']}\")\n",
    "            print(f\"URL: {post['url']}\")\n",
    "            print(f\"Comments: {len(post['comments'])}\")\n",
    "            \n",
    "            # Print first few comments\n",
    "            print(\"\\nSample Comments:\")\n",
    "            for j, comment in enumerate(post['comments'][:3], 1):\n",
    "                print(f\"  {j}. {comment['username']}: {comment['content'][:100]}...\" if len(comment['content']) > 100 else f\"  {j}. {comment['username']}: {comment['content']}\")\n",
    "            \n",
    "            if len(post['comments']) > 3:\n",
    "                print(f\"  ...and {len(post['comments']) - 3} more comments\")\n",
    "            \n",
    "            print(\"=\" * 80)\n",
    "        \n",
    "        return scraper.posts\n",
    "    \n",
    "    finally:\n",
    "        # Close the driver\n",
    "        scraper.close()\n",
    "\n",
    "# Function to process pre-downloaded HTML\n",
    "def process_html_file(file_path):\n",
    "    try:\n",
    "        # Read the HTML file\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            html_content = f.read()\n",
    "        \n",
    "        # Create the scraper (we don't need to use Selenium for this)\n",
    "        scraper = RedditCommentScraper()\n",
    "        \n",
    "        # Extract comments from the HTML\n",
    "        comments = scraper.extract_comments_from_html(html_content)\n",
    "        \n",
    "        # Print the results\n",
    "        print(f\"\\nExtracted {len(comments)} comments from HTML file:\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        for i, comment in enumerate(comments, 1):\n",
    "            print(f\"{i}. {comment['username']}: {comment['content'][:100]}...\" if len(comment['content']) > 100 else f\"{i}. {comment['username']}: {comment['content']}\")\n",
    "        \n",
    "        # Save to CSV\n",
    "        with open('extracted_comments.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            fieldnames = ['username', 'comment_content']\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "            \n",
    "            writer.writeheader()\n",
    "            for comment in comments:\n",
    "                writer.writerow({\n",
    "                    'username': comment['username'],\n",
    "                    'comment_content': comment['content']\n",
    "                })\n",
    "        \n",
    "        logger.info(f\"Saved {len(comments)} comments to extracted_comments.csv\")\n",
    "        \n",
    "        return comments\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing HTML file: {e}\")\n",
    "        return []\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # If we have a downloaded HTML file, process it directly\n",
    "    html_file_path = \"reddit_post.html\"  # Change this to your HTML file path\n",
    "    \n",
    "    import os\n",
    "    if os.path.exists(html_file_path):\n",
    "        process_html_file(html_file_path)\n",
    "    else:\n",
    "        # Otherwise, do a live search and scrape\n",
    "        main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0ef2e45-02a1-4899-bd8b-61ad5ea111fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>members</th>\n",
       "      <th>online</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>r/jobs</td>\n",
       "      <td>/r/jobs is the number one community for advice...</td>\n",
       "      <td>2M</td>\n",
       "      <td>516</td>\n",
       "      <td>https://www.reddit.com/r/jobs/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>r/RemoteJobs</td>\n",
       "      <td>Remote jobs: the future of work! READ RULES BE...</td>\n",
       "      <td>273K</td>\n",
       "      <td>28</td>\n",
       "      <td>https://www.reddit.com/r/RemoteJobs/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>r/DesignJobs</td>\n",
       "      <td>Some redditors are skilled professionals, some...</td>\n",
       "      <td>163K</td>\n",
       "      <td>32</td>\n",
       "      <td>https://www.reddit.com/r/DesignJobs/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>r/AskReddit</td>\n",
       "      <td>r/AskReddit is the place to ask and answer tho...</td>\n",
       "      <td>55M</td>\n",
       "      <td>12K</td>\n",
       "      <td>https://www.reddit.com/r/AskReddit/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>r/antiwork</td>\n",
       "      <td>A subreddit for those who want to end work, ar...</td>\n",
       "      <td>2.9M</td>\n",
       "      <td>1.5K</td>\n",
       "      <td>https://www.reddit.com/r/antiwork/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>r/torontoJobs</td>\n",
       "      <td>Redditor approved jobs in the GTHA.</td>\n",
       "      <td>61K</td>\n",
       "      <td>19</td>\n",
       "      <td>https://www.reddit.com/r/torontoJobs/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>r/CanadaJobs</td>\n",
       "      <td>This is a place to support job seekers and hir...</td>\n",
       "      <td>23K</td>\n",
       "      <td>12</td>\n",
       "      <td>https://www.reddit.com/r/CanadaJobs/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>r/recruitinghell</td>\n",
       "      <td>Did a recruiter make you send them a resume an...</td>\n",
       "      <td>844K</td>\n",
       "      <td>574</td>\n",
       "      <td>https://www.reddit.com/r/recruitinghell/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>r/careerguidance</td>\n",
       "      <td>A place to discuss career options, to ask ques...</td>\n",
       "      <td>4.7M</td>\n",
       "      <td>1.6K</td>\n",
       "      <td>https://www.reddit.com/r/careerguidance/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>r/DubaiJobs</td>\n",
       "      <td>We all know how daunting the job hunt can be i...</td>\n",
       "      <td>15K</td>\n",
       "      <td>16</td>\n",
       "      <td>https://www.reddit.com/r/DubaiJobs/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>r/Germany_Jobs</td>\n",
       "      <td>Do you want to work in Germany? We want to bui...</td>\n",
       "      <td>16K</td>\n",
       "      <td>7</td>\n",
       "      <td>https://www.reddit.com/r/Germany_Jobs/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>r/cscareerquestions</td>\n",
       "      <td>CSCareerQuestions is a community for those who...</td>\n",
       "      <td>2.2M</td>\n",
       "      <td>484</td>\n",
       "      <td>https://www.reddit.com/r/cscareerquestions/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>r/UKJobs</td>\n",
       "      <td>A community intended to provide a place for us...</td>\n",
       "      <td>292K</td>\n",
       "      <td>149</td>\n",
       "      <td>https://www.reddit.com/r/UKJobs/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>r/SideJobs</td>\n",
       "      <td>Jobs you can do outside of your normal job for...</td>\n",
       "      <td>3.5K</td>\n",
       "      <td>7</td>\n",
       "      <td>https://www.reddit.com/r/SideJobs/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>r/IndiaJobsOpenings</td>\n",
       "      <td>Your go-to subreddit for finding and sharing j...</td>\n",
       "      <td>7K</td>\n",
       "      <td>10</td>\n",
       "      <td>https://www.reddit.com/r/IndiaJobsOpenings/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>r/politics</td>\n",
       "      <td>/r/Politics is for news and discussion about U...</td>\n",
       "      <td>8.8M</td>\n",
       "      <td>11K</td>\n",
       "      <td>https://www.reddit.com/r/politics/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>r/WorkOnlineJobs</td>\n",
       "      <td>Online Jobs - Work at Home</td>\n",
       "      <td>5K</td>\n",
       "      <td>3</td>\n",
       "      <td>https://www.reddit.com/r/WorkOnlineJobs/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>r/NetworkingJobs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15K</td>\n",
       "      <td>6</td>\n",
       "      <td>https://www.reddit.com/r/NetworkingJobs/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>r/memes</td>\n",
       "      <td>Memes! A way of describing cultural informatio...</td>\n",
       "      <td>35M</td>\n",
       "      <td>1.6K</td>\n",
       "      <td>https://www.reddit.com/r/memes/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>r/funny</td>\n",
       "      <td>Reddit's largest humor depository</td>\n",
       "      <td>67M</td>\n",
       "      <td>2.4K</td>\n",
       "      <td>https://www.reddit.com/r/funny/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>r/RemoteJobsSearch</td>\n",
       "      <td>Hi everyone! This is a group where we can post...</td>\n",
       "      <td>1.7K</td>\n",
       "      <td>4</td>\n",
       "      <td>https://www.reddit.com/r/RemoteJobsSearch/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>r/PythonJobs</td>\n",
       "      <td>Python jobs are posted here</td>\n",
       "      <td>28K</td>\n",
       "      <td>5</td>\n",
       "      <td>https://www.reddit.com/r/PythonJobs/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>r/personalfinance</td>\n",
       "      <td>Learn about budgeting, saving, getting out of ...</td>\n",
       "      <td>21M</td>\n",
       "      <td>1.8K</td>\n",
       "      <td>https://www.reddit.com/r/personalfinance/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>r/ITCareerQuestions</td>\n",
       "      <td>This subreddit is designed to help anyone in o...</td>\n",
       "      <td>492K</td>\n",
       "      <td>94</td>\n",
       "      <td>https://www.reddit.com/r/ITCareerQuestions/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>r/OnlineJobsPH</td>\n",
       "      <td>💪 For Filipino Remote Workers to improve and l...</td>\n",
       "      <td>4.9K</td>\n",
       "      <td>6</td>\n",
       "      <td>https://www.reddit.com/r/OnlineJobsPH/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>r/CyberSecurityJobs</td>\n",
       "      <td>A forum for discussing cybersecurity career in...</td>\n",
       "      <td>41K</td>\n",
       "      <td>5</td>\n",
       "      <td>https://www.reddit.com/r/CyberSecurityJobs/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>r/Showerthoughts</td>\n",
       "      <td>A subreddit for sharing those miniature epipha...</td>\n",
       "      <td>34M</td>\n",
       "      <td>539</td>\n",
       "      <td>https://www.reddit.com/r/Showerthoughts/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>r/Accounting</td>\n",
       "      <td>Primarily for accountants and aspiring account...</td>\n",
       "      <td>1.1M</td>\n",
       "      <td>400</td>\n",
       "      <td>https://www.reddit.com/r/Accounting/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>r/JobsPhilippines</td>\n",
       "      <td>NaN</td>\n",
       "      <td>73K</td>\n",
       "      <td>22</td>\n",
       "      <td>https://www.reddit.com/r/JobsPhilippines/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>r/DataScienceJobs</td>\n",
       "      <td>A place for people to post data science/machin...</td>\n",
       "      <td>32K</td>\n",
       "      <td>7</td>\n",
       "      <td>https://www.reddit.com/r/DataScienceJobs/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>r/WritingPrompts</td>\n",
       "      <td>Writing Prompts. You're a writer and you just ...</td>\n",
       "      <td>19M</td>\n",
       "      <td>345</td>\n",
       "      <td>https://www.reddit.com/r/WritingPrompts/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>r/AmItheAsshole</td>\n",
       "      <td>A catharsis for the frustrated moral philosoph...</td>\n",
       "      <td>24M</td>\n",
       "      <td>8.7K</td>\n",
       "      <td>https://www.reddit.com/r/AmItheAsshole/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>r/IWantOutJobs</td>\n",
       "      <td>The purpose of this subreddit is to post emplo...</td>\n",
       "      <td>16K</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.reddit.com/r/IWantOutJobs/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>r/Jobs4Bitcoins</td>\n",
       "      <td>Find Work, Find Workers! All payments done in ...</td>\n",
       "      <td>69K</td>\n",
       "      <td>3</td>\n",
       "      <td>https://www.reddit.com/r/Jobs4Bitcoins/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>r/pics</td>\n",
       "      <td>A place for photographs, pictures, and other i...</td>\n",
       "      <td>32M</td>\n",
       "      <td>6.1K</td>\n",
       "      <td>https://www.reddit.com/r/pics/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>r/teenagers</td>\n",
       "      <td>r/teenagers is the biggest community forum run...</td>\n",
       "      <td>3.2M</td>\n",
       "      <td>1.5K</td>\n",
       "      <td>https://www.reddit.com/r/teenagers/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>r/jobsearchhacks</td>\n",
       "      <td>Forget traditional job searching - improve you...</td>\n",
       "      <td>238K</td>\n",
       "      <td>53</td>\n",
       "      <td>https://www.reddit.com/r/jobsearchhacks/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>r/VancouverJobs</td>\n",
       "      <td>A place to match Vancouver job-seekers with Va...</td>\n",
       "      <td>27K</td>\n",
       "      <td>10</td>\n",
       "      <td>https://www.reddit.com/r/VancouverJobs/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>r/AdviceAnimals</td>\n",
       "      <td>Reddit's Gold Mine</td>\n",
       "      <td>9.9M</td>\n",
       "      <td>370</td>\n",
       "      <td>https://www.reddit.com/r/AdviceAnimals/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>r/TrollXChromosomes</td>\n",
       "      <td>A subreddit for rage comics and other memes wi...</td>\n",
       "      <td>835K</td>\n",
       "      <td>34</td>\n",
       "      <td>https://www.reddit.com/r/TrollXChromosomes/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>r/RemoteJobseekers</td>\n",
       "      <td>Looking for a remote job ? We share effective ...</td>\n",
       "      <td>17K</td>\n",
       "      <td>4</td>\n",
       "      <td>https://www.reddit.com/r/RemoteJobseekers/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>r/houstonjobs</td>\n",
       "      <td>Connecting Employers and Employees since 2011</td>\n",
       "      <td>10K</td>\n",
       "      <td>6</td>\n",
       "      <td>https://www.reddit.com/r/houstonjobs/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>r/nfl</td>\n",
       "      <td>The place to discuss all NFL related things</td>\n",
       "      <td>12M</td>\n",
       "      <td>5K</td>\n",
       "      <td>https://www.reddit.com/r/nfl/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>r/nursing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.1M</td>\n",
       "      <td>232</td>\n",
       "      <td>https://www.reddit.com/r/nursing/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>r/MachineLearningJobs</td>\n",
       "      <td>ML jobs, AI jobs, LLM jobs, machine learning, ...</td>\n",
       "      <td>22K</td>\n",
       "      <td>4</td>\n",
       "      <td>https://www.reddit.com/r/MachineLearningJobs/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>r/Nonprofit_Jobs</td>\n",
       "      <td>A discussion board for non-profit/charity/NGO/...</td>\n",
       "      <td>11K</td>\n",
       "      <td>3</td>\n",
       "      <td>https://www.reddit.com/r/Nonprofit_Jobs/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>r/india</td>\n",
       "      <td>The Official Subreddit for India</td>\n",
       "      <td>2.9M</td>\n",
       "      <td>883</td>\n",
       "      <td>https://www.reddit.com/r/india/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>r/ffxiv</td>\n",
       "      <td>A community for fans of the critically acclaim...</td>\n",
       "      <td>1.3M</td>\n",
       "      <td>654</td>\n",
       "      <td>https://www.reddit.com/r/ffxiv/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>r/JapanJobs</td>\n",
       "      <td>Welcome to /r/JapanJobs! This is a subreddit d...</td>\n",
       "      <td>10K</td>\n",
       "      <td>3</td>\n",
       "      <td>https://www.reddit.com/r/JapanJobs/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>r/gameDevJobs</td>\n",
       "      <td>Here you will only ever find job listings that...</td>\n",
       "      <td>14K</td>\n",
       "      <td>14</td>\n",
       "      <td>https://www.reddit.com/r/gameDevJobs/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>r/mildlyinfuriating</td>\n",
       "      <td>jugkfmghgug</td>\n",
       "      <td>10M</td>\n",
       "      <td>1.7K</td>\n",
       "      <td>https://www.reddit.com/r/mildlyinfuriating/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>r/legaladvice</td>\n",
       "      <td>A place to ask simple legal questions.</td>\n",
       "      <td>3M</td>\n",
       "      <td>783</td>\n",
       "      <td>https://www.reddit.com/r/legaladvice/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>r/BigDataJobs</td>\n",
       "      <td>Find and post work relating to \"Big Data\".</td>\n",
       "      <td>15K</td>\n",
       "      <td>7</td>\n",
       "      <td>https://www.reddit.com/r/BigDataJobs/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>r/WebDeveloperJobs</td>\n",
       "      <td>Jobs for JavaScript web developers who have ex...</td>\n",
       "      <td>15K</td>\n",
       "      <td>14</td>\n",
       "      <td>https://www.reddit.com/r/WebDeveloperJobs/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>r/CasualConversation</td>\n",
       "      <td>The friendlier part of Reddit. Have a fun conv...</td>\n",
       "      <td>2.6M</td>\n",
       "      <td>342</td>\n",
       "      <td>https://www.reddit.com/r/CasualConversation/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>r/resumes</td>\n",
       "      <td>Get help with your resume! Checkout the wiki a...</td>\n",
       "      <td>1.2M</td>\n",
       "      <td>67</td>\n",
       "      <td>https://www.reddit.com/r/resumes/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>r/SFBayJobs</td>\n",
       "      <td>SF Bay Jobs - [Hiring], [Seeking]</td>\n",
       "      <td>9.1K</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.reddit.com/r/SFBayJobs/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>r/AnimalswithJobs</td>\n",
       "      <td>Post anything here related to any kind of anim...</td>\n",
       "      <td>9.2K</td>\n",
       "      <td>5</td>\n",
       "      <td>https://www.reddit.com/r/AnimalswithJobs/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>r/Conservative</td>\n",
       "      <td>https://x.com/rcondiscord https://discord.gg/c...</td>\n",
       "      <td>1.3M</td>\n",
       "      <td>2.8K</td>\n",
       "      <td>https://www.reddit.com/r/Conservative/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     name                                        description  \\\n",
       "0                  r/jobs  /r/jobs is the number one community for advice...   \n",
       "1            r/RemoteJobs  Remote jobs: the future of work! READ RULES BE...   \n",
       "2            r/DesignJobs  Some redditors are skilled professionals, some...   \n",
       "3             r/AskReddit  r/AskReddit is the place to ask and answer tho...   \n",
       "4              r/antiwork  A subreddit for those who want to end work, ar...   \n",
       "5           r/torontoJobs                Redditor approved jobs in the GTHA.   \n",
       "6            r/CanadaJobs  This is a place to support job seekers and hir...   \n",
       "7        r/recruitinghell  Did a recruiter make you send them a resume an...   \n",
       "8        r/careerguidance  A place to discuss career options, to ask ques...   \n",
       "9             r/DubaiJobs  We all know how daunting the job hunt can be i...   \n",
       "10         r/Germany_Jobs  Do you want to work in Germany? We want to bui...   \n",
       "11    r/cscareerquestions  CSCareerQuestions is a community for those who...   \n",
       "12               r/UKJobs  A community intended to provide a place for us...   \n",
       "13             r/SideJobs  Jobs you can do outside of your normal job for...   \n",
       "14    r/IndiaJobsOpenings  Your go-to subreddit for finding and sharing j...   \n",
       "15             r/politics  /r/Politics is for news and discussion about U...   \n",
       "16       r/WorkOnlineJobs                         Online Jobs - Work at Home   \n",
       "17       r/NetworkingJobs                                                NaN   \n",
       "18                r/memes  Memes! A way of describing cultural informatio...   \n",
       "19                r/funny                  Reddit's largest humor depository   \n",
       "20     r/RemoteJobsSearch  Hi everyone! This is a group where we can post...   \n",
       "21           r/PythonJobs                        Python jobs are posted here   \n",
       "22      r/personalfinance  Learn about budgeting, saving, getting out of ...   \n",
       "23    r/ITCareerQuestions  This subreddit is designed to help anyone in o...   \n",
       "24         r/OnlineJobsPH  💪 For Filipino Remote Workers to improve and l...   \n",
       "25    r/CyberSecurityJobs  A forum for discussing cybersecurity career in...   \n",
       "26       r/Showerthoughts  A subreddit for sharing those miniature epipha...   \n",
       "27           r/Accounting  Primarily for accountants and aspiring account...   \n",
       "28      r/JobsPhilippines                                                NaN   \n",
       "29      r/DataScienceJobs  A place for people to post data science/machin...   \n",
       "30       r/WritingPrompts  Writing Prompts. You're a writer and you just ...   \n",
       "31        r/AmItheAsshole  A catharsis for the frustrated moral philosoph...   \n",
       "32         r/IWantOutJobs  The purpose of this subreddit is to post emplo...   \n",
       "33        r/Jobs4Bitcoins  Find Work, Find Workers! All payments done in ...   \n",
       "34                 r/pics  A place for photographs, pictures, and other i...   \n",
       "35            r/teenagers  r/teenagers is the biggest community forum run...   \n",
       "36       r/jobsearchhacks  Forget traditional job searching - improve you...   \n",
       "37        r/VancouverJobs  A place to match Vancouver job-seekers with Va...   \n",
       "38        r/AdviceAnimals                                 Reddit's Gold Mine   \n",
       "39    r/TrollXChromosomes  A subreddit for rage comics and other memes wi...   \n",
       "40     r/RemoteJobseekers  Looking for a remote job ? We share effective ...   \n",
       "41          r/houstonjobs      Connecting Employers and Employees since 2011   \n",
       "42                  r/nfl        The place to discuss all NFL related things   \n",
       "43              r/nursing                                                NaN   \n",
       "44  r/MachineLearningJobs  ML jobs, AI jobs, LLM jobs, machine learning, ...   \n",
       "45       r/Nonprofit_Jobs  A discussion board for non-profit/charity/NGO/...   \n",
       "46                r/india                   The Official Subreddit for India   \n",
       "47                r/ffxiv  A community for fans of the critically acclaim...   \n",
       "48            r/JapanJobs  Welcome to /r/JapanJobs! This is a subreddit d...   \n",
       "49          r/gameDevJobs  Here you will only ever find job listings that...   \n",
       "50    r/mildlyinfuriating                                        jugkfmghgug   \n",
       "51          r/legaladvice             A place to ask simple legal questions.   \n",
       "52          r/BigDataJobs         Find and post work relating to \"Big Data\".   \n",
       "53     r/WebDeveloperJobs  Jobs for JavaScript web developers who have ex...   \n",
       "54   r/CasualConversation  The friendlier part of Reddit. Have a fun conv...   \n",
       "55              r/resumes  Get help with your resume! Checkout the wiki a...   \n",
       "56            r/SFBayJobs                  SF Bay Jobs - [Hiring], [Seeking]   \n",
       "57      r/AnimalswithJobs  Post anything here related to any kind of anim...   \n",
       "58         r/Conservative  https://x.com/rcondiscord https://discord.gg/c...   \n",
       "\n",
       "   members online                                            url  \n",
       "0       2M    516                 https://www.reddit.com/r/jobs/  \n",
       "1     273K     28           https://www.reddit.com/r/RemoteJobs/  \n",
       "2     163K     32           https://www.reddit.com/r/DesignJobs/  \n",
       "3      55M    12K            https://www.reddit.com/r/AskReddit/  \n",
       "4     2.9M   1.5K             https://www.reddit.com/r/antiwork/  \n",
       "5      61K     19          https://www.reddit.com/r/torontoJobs/  \n",
       "6      23K     12           https://www.reddit.com/r/CanadaJobs/  \n",
       "7     844K    574       https://www.reddit.com/r/recruitinghell/  \n",
       "8     4.7M   1.6K       https://www.reddit.com/r/careerguidance/  \n",
       "9      15K     16            https://www.reddit.com/r/DubaiJobs/  \n",
       "10     16K      7         https://www.reddit.com/r/Germany_Jobs/  \n",
       "11    2.2M    484    https://www.reddit.com/r/cscareerquestions/  \n",
       "12    292K    149               https://www.reddit.com/r/UKJobs/  \n",
       "13    3.5K      7             https://www.reddit.com/r/SideJobs/  \n",
       "14      7K     10    https://www.reddit.com/r/IndiaJobsOpenings/  \n",
       "15    8.8M    11K             https://www.reddit.com/r/politics/  \n",
       "16      5K      3       https://www.reddit.com/r/WorkOnlineJobs/  \n",
       "17     15K      6       https://www.reddit.com/r/NetworkingJobs/  \n",
       "18     35M   1.6K                https://www.reddit.com/r/memes/  \n",
       "19     67M   2.4K                https://www.reddit.com/r/funny/  \n",
       "20    1.7K      4     https://www.reddit.com/r/RemoteJobsSearch/  \n",
       "21     28K      5           https://www.reddit.com/r/PythonJobs/  \n",
       "22     21M   1.8K      https://www.reddit.com/r/personalfinance/  \n",
       "23    492K     94    https://www.reddit.com/r/ITCareerQuestions/  \n",
       "24    4.9K      6         https://www.reddit.com/r/OnlineJobsPH/  \n",
       "25     41K      5    https://www.reddit.com/r/CyberSecurityJobs/  \n",
       "26     34M    539       https://www.reddit.com/r/Showerthoughts/  \n",
       "27    1.1M    400           https://www.reddit.com/r/Accounting/  \n",
       "28     73K     22      https://www.reddit.com/r/JobsPhilippines/  \n",
       "29     32K      7      https://www.reddit.com/r/DataScienceJobs/  \n",
       "30     19M    345       https://www.reddit.com/r/WritingPrompts/  \n",
       "31     24M   8.7K        https://www.reddit.com/r/AmItheAsshole/  \n",
       "32     16K      1         https://www.reddit.com/r/IWantOutJobs/  \n",
       "33     69K      3        https://www.reddit.com/r/Jobs4Bitcoins/  \n",
       "34     32M   6.1K                 https://www.reddit.com/r/pics/  \n",
       "35    3.2M   1.5K            https://www.reddit.com/r/teenagers/  \n",
       "36    238K     53       https://www.reddit.com/r/jobsearchhacks/  \n",
       "37     27K     10        https://www.reddit.com/r/VancouverJobs/  \n",
       "38    9.9M    370        https://www.reddit.com/r/AdviceAnimals/  \n",
       "39    835K     34    https://www.reddit.com/r/TrollXChromosomes/  \n",
       "40     17K      4     https://www.reddit.com/r/RemoteJobseekers/  \n",
       "41     10K      6          https://www.reddit.com/r/houstonjobs/  \n",
       "42     12M     5K                  https://www.reddit.com/r/nfl/  \n",
       "43    1.1M    232              https://www.reddit.com/r/nursing/  \n",
       "44     22K      4  https://www.reddit.com/r/MachineLearningJobs/  \n",
       "45     11K      3       https://www.reddit.com/r/Nonprofit_Jobs/  \n",
       "46    2.9M    883                https://www.reddit.com/r/india/  \n",
       "47    1.3M    654                https://www.reddit.com/r/ffxiv/  \n",
       "48     10K      3            https://www.reddit.com/r/JapanJobs/  \n",
       "49     14K     14          https://www.reddit.com/r/gameDevJobs/  \n",
       "50     10M   1.7K    https://www.reddit.com/r/mildlyinfuriating/  \n",
       "51      3M    783          https://www.reddit.com/r/legaladvice/  \n",
       "52     15K      7          https://www.reddit.com/r/BigDataJobs/  \n",
       "53     15K     14     https://www.reddit.com/r/WebDeveloperJobs/  \n",
       "54    2.6M    342   https://www.reddit.com/r/CasualConversation/  \n",
       "55    1.2M     67              https://www.reddit.com/r/resumes/  \n",
       "56    9.1K      1            https://www.reddit.com/r/SFBayJobs/  \n",
       "57    9.2K      5      https://www.reddit.com/r/AnimalswithJobs/  \n",
       "58    1.3M   2.8K         https://www.reddit.com/r/Conservative/  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv('jobs_communities.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "320789bf-e38e-4960-ab55-74aefc7937c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-28 14:59:07,210 - INFO - Searching Reddit for: data science job\n",
      "2025-04-28 14:59:07,210 - INFO - Search URL: https://www.reddit.com/search/?q=data+science+job\n",
      "2025-04-28 14:59:12,069 - INFO - Found 7 posts in search results\n",
      "2025-04-28 14:59:12,233 - INFO - Extracted data for 7 posts\n",
      "2025-04-28 14:59:12,233 - INFO - Processing post 1/3: Anyone working as a data scientist/ data analyst/ data engineer? Is getting a job in this field tough as a fresher?\n",
      "2025-04-28 14:59:12,233 - INFO - Scraping post: Anyone working as a data scientist/ data analyst/ data engineer? Is getting a job in this field tough as a fresher?\n",
      "2025-04-28 14:59:16,533 - INFO - Found 2 comments in post\n",
      "2025-04-28 14:59:18,611 - INFO - Processing post 2/3: Jobs in Entry level Data Science ?\n",
      "2025-04-28 14:59:18,612 - INFO - Scraping post: Jobs in Entry level Data Science ?\n",
      "2025-04-28 14:59:22,645 - INFO - Found 2 comments in post\n",
      "2025-04-28 14:59:24,714 - INFO - Processing post 3/3: How's the job market for data scientists?\n",
      "2025-04-28 14:59:24,714 - INFO - Scraping post: How's the job market for data scientists?\n",
      "2025-04-28 14:59:29,229 - INFO - Found 9 comments in post\n",
      "2025-04-28 14:59:29,437 - INFO - Saved 10 comments to data_science_job_comments.csv\n",
      "2025-04-28 14:59:29,441 - INFO - Extracted 10 unique usernames from comments\n",
      "2025-04-28 14:59:29,441 - INFO - Preparing to scrape 5 user profiles\n",
      "2025-04-28 14:59:29,441 - INFO - Processing user 1/5: qcjb\n",
      "2025-04-28 14:59:29,441 - INFO - Scraping profile: https://www.reddit.com/user/qcjb/\n",
      "2025-04-28 14:59:33,723 - INFO - No age verification required\n",
      "2025-04-28 14:59:33,759 - INFO - Found Comments tab with contains match\n",
      "2025-04-28 14:59:33,760 - INFO - Clicking Comments tab\n",
      "2025-04-28 14:59:34,838 - ERROR - Error scraping profile qcjb: Message: element click intercepted: Element <a rpl=\"\" class=\"\n",
      "button-medium px-[var(--rem14)]\n",
      "button-plain\n",
      "\n",
      "\n",
      "\n",
      "items-center justify-center\n",
      "button inline-flex \" href=\"/user/qcjb/comments/\" slot=\"page-3\" style=\"background-color: transparent;\" tabindex=\"-1\" type=\"button\">...</a> is not clickable at point (235, 188). Other element would receive the click: <html lang=\"en-US\" class=\"theme-beta\" style=\"--dvh-unit: 1dvh; --viewport-height: calc(calc(var(--dvh-unit) * 100));\">...</html>\n",
      "  (Session info: chrome=135.0.7049.115)\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF692CBEFA5+77893]\n",
      "\tGetHandleVerifier [0x00007FF692CBF000+77984]\n",
      "\t(No symbol) [0x00007FF692A891BA]\n",
      "\t(No symbol) [0x00007FF692AE70A9]\n",
      "\t(No symbol) [0x00007FF692AE4A62]\n",
      "\t(No symbol) [0x00007FF692AE1B01]\n",
      "\t(No symbol) [0x00007FF692AE0A01]\n",
      "\t(No symbol) [0x00007FF692AD2134]\n",
      "\t(No symbol) [0x00007FF692B0712A]\n",
      "\t(No symbol) [0x00007FF692AD19E6]\n",
      "\t(No symbol) [0x00007FF692B07340]\n",
      "\t(No symbol) [0x00007FF692B2F07F]\n",
      "\t(No symbol) [0x00007FF692B06F03]\n",
      "\t(No symbol) [0x00007FF692AD0328]\n",
      "\t(No symbol) [0x00007FF692AD1093]\n",
      "\tGetHandleVerifier [0x00007FF692F77B6D+2931725]\n",
      "\tGetHandleVerifier [0x00007FF692F72132+2908626]\n",
      "\tGetHandleVerifier [0x00007FF692F900F3+3031443]\n",
      "\tGetHandleVerifier [0x00007FF692CD91EA+184970]\n",
      "\tGetHandleVerifier [0x00007FF692CE086F+215311]\n",
      "\tGetHandleVerifier [0x00007FF692CC6EC4+110436]\n",
      "\tGetHandleVerifier [0x00007FF692CC7072+110866]\n",
      "\tGetHandleVerifier [0x00007FF692CAD479+5401]\n",
      "\tBaseThreadInitThunk [0x00007FF8B646259D+29]\n",
      "\tRtlUserThreadStart [0x00007FF8B6ACAF38+40]\n",
      "\n",
      "2025-04-28 14:59:36,840 - INFO - Processing user 2/5: itsMalarky\n",
      "2025-04-28 14:59:36,841 - INFO - Scraping profile: https://www.reddit.com/user/itsMalarky/\n",
      "2025-04-28 14:59:41,333 - INFO - No age verification required\n",
      "2025-04-28 14:59:41,353 - INFO - Found Comments tab with contains match\n",
      "2025-04-28 14:59:41,356 - INFO - Clicking Comments tab\n",
      "2025-04-28 14:59:46,094 - INFO - Scroll 1/10 for user itsMalarky\n",
      "2025-04-28 14:59:55,542 - INFO - Found 67 new comments after scroll 1\n",
      "2025-04-28 14:59:55,543 - INFO - Reached maximum comment limit (50)\n",
      "2025-04-28 14:59:55,544 - INFO - Total comments extracted for itsMalarky: 67\n",
      "2025-04-28 14:59:55,545 - INFO - Successfully scraped 50 comments from itsMalarky's profile\n",
      "2025-04-28 14:59:57,546 - INFO - Processing user 3/5: Party_Lingonberry_58\n",
      "2025-04-28 14:59:57,547 - INFO - Scraping profile: https://www.reddit.com/user/Party_Lingonberry_58/\n",
      "2025-04-28 15:00:01,948 - INFO - No age verification required\n",
      "2025-04-28 15:00:01,968 - INFO - Found Comments tab with contains match\n",
      "2025-04-28 15:00:01,969 - INFO - Clicking Comments tab\n",
      "2025-04-28 15:00:06,578 - INFO - Scroll 1/10 for user Party_Lingonberry_58\n",
      "2025-04-28 15:00:14,200 - INFO - Found 51 new comments after scroll 1\n",
      "2025-04-28 15:00:14,201 - INFO - Reached maximum comment limit (50)\n",
      "2025-04-28 15:00:14,201 - INFO - Total comments extracted for Party_Lingonberry_58: 51\n",
      "2025-04-28 15:00:14,202 - INFO - Successfully scraped 50 comments from Party_Lingonberry_58's profile\n",
      "2025-04-28 15:00:16,203 - INFO - Processing user 4/5: occasional_cynic\n",
      "2025-04-28 15:00:16,203 - INFO - Scraping profile: https://www.reddit.com/user/occasional_cynic/\n",
      "2025-04-28 15:00:20,548 - INFO - No age verification required\n",
      "2025-04-28 15:00:20,571 - INFO - Found Comments tab with contains match\n",
      "2025-04-28 15:00:20,572 - INFO - Clicking Comments tab\n",
      "2025-04-28 15:00:25,191 - INFO - Scroll 1/10 for user occasional_cynic\n",
      "2025-04-28 15:00:34,378 - INFO - Found 67 new comments after scroll 1\n",
      "2025-04-28 15:00:34,379 - INFO - Reached maximum comment limit (50)\n",
      "2025-04-28 15:00:34,379 - INFO - Total comments extracted for occasional_cynic: 67\n",
      "2025-04-28 15:00:34,380 - INFO - Successfully scraped 50 comments from occasional_cynic's profile\n",
      "2025-04-28 15:00:36,382 - INFO - Processing user 5/5: AtmosphereGrouchy245\n",
      "2025-04-28 15:00:36,383 - INFO - Scraping profile: https://www.reddit.com/user/AtmosphereGrouchy245/\n",
      "2025-04-28 15:00:40,796 - INFO - No age verification required\n",
      "2025-04-28 15:00:40,824 - INFO - Found Comments tab with contains match\n",
      "2025-04-28 15:00:40,824 - INFO - Clicking Comments tab\n",
      "2025-04-28 15:00:45,339 - INFO - Scroll 1/10 for user AtmosphereGrouchy245\n",
      "2025-04-28 15:00:48,605 - INFO - Found 3 new comments after scroll 1\n",
      "2025-04-28 15:00:48,609 - INFO - No more content to load (scroll height unchanged)\n",
      "2025-04-28 15:00:48,609 - INFO - Scroll 2/10 for user AtmosphereGrouchy245\n",
      "2025-04-28 15:00:51,888 - INFO - No new comments found after scroll 2\n",
      "2025-04-28 15:00:51,888 - INFO - No more content to load (scroll height unchanged)\n",
      "2025-04-28 15:00:51,888 - INFO - No new comments for 3 consecutive scrolls, stopping\n",
      "2025-04-28 15:00:51,888 - INFO - Total comments extracted for AtmosphereGrouchy245: 3\n",
      "2025-04-28 15:00:51,888 - INFO - Successfully scraped 3 comments from AtmosphereGrouchy245's profile\n",
      "2025-04-28 15:00:51,892 - INFO - Saved 153 user comments to reddit_user_comments.csv\n",
      "2025-04-28 15:00:51,897 - INFO - Saved data for 4 users to reddit_users.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scraped 3 posts about data science jobs\n",
      "Scraped 4 user profiles\n",
      "\n",
      "User: itsMalarky\n",
      "Comments: 50\n",
      "  1. [] I like the label.\n",
      "     Content: Definitely worth it. Same area\n",
      "  2. [] Making friends\n",
      "     Content: Game night (Tuesday), trivia on Wednesdays (try to jump into a team near the bar) and cribbage night...\n",
      "  ...and 48 more comments\n",
      "\n",
      "User: Party_Lingonberry_58\n",
      "Comments: 50\n",
      "  1. [] ECE 606 - final?\n",
      "     Content: The grades are posted in Learn!\n",
      "  2. [] ECE 606 - final?\n",
      "     Content: I haven’t seen the grades yet\n",
      "  ...and 48 more comments\n",
      "\n",
      "User: occasional_cynic\n",
      "Comments: 50\n",
      "  1. [] Opposing QB’s WILL need diapers. Giants select DL, Darius Alexander (Toledo).\n",
      "     Content: BJ Hill has been pretty darn good.  He just got into Judge's doghouse because he was a terrible coac...\n",
      "  2. [] Opposing QB’s WILL need diapers. Giants select DL, Darius Alexander (Toledo).\n",
      "     Content: It's very important.  Dominating twenty year olds when you are that old is a red flag.  See: JMS.\n",
      "  ...and 48 more comments\n",
      "\n",
      "User: AtmosphereGrouchy245\n",
      "Comments: 3\n",
      "  1. [] Anyone working as a data scientist/ data analyst/ data engineer? Is getting a job in this field tough as a fresher?\n",
      "     Content: Hey folks, I’m on the hunt for a data science job! I’ve got a couple of years of experience working ...\n",
      "  2. [Unknown subreddit] Unknown title\n",
      "     Content: 2\n",
      "  ...and 1 more comments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-28 15:00:54,107 - INFO - Closed web driver\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Enhanced Reddit User Comments Scraper\n",
    "This script focuses on scraping user comments from Reddit profiles\n",
    "with improved scrolling to capture all available comments\n",
    "\"\"\"\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import re\n",
    "import logging\n",
    "import time\n",
    "import json\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException, ElementNotInteractableException\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger('reddit_comments_scraper')\n",
    "\n",
    "class RedditCommentsScraper:\n",
    "    def __init__(self):\n",
    "        # Set up Chrome options\n",
    "        self.options = Options()\n",
    "        self.options.add_argument('--headless')\n",
    "        self.options.add_argument('--disable-gpu')\n",
    "        self.options.add_argument('--no-sandbox')\n",
    "        self.options.add_argument('--disable-dev-shm-usage')\n",
    "        self.options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')\n",
    "        \n",
    "        # Initialize the driver\n",
    "        self.driver = webdriver.Chrome(options=self.options)\n",
    "        self.posts = []\n",
    "        self.user_data = {}\n",
    "        \n",
    "    def search_posts(self, search_query):\n",
    "        \"\"\"\n",
    "        Search for posts related to the search query\n",
    "        \"\"\"\n",
    "        # Format the search query for URL\n",
    "        formatted_query = search_query.replace(' ', '+')\n",
    "        search_url = f\"https://www.reddit.com/search/?q={formatted_query}\"\n",
    "        logger.info(f\"Searching Reddit for: {search_query}\")\n",
    "        logger.info(f\"Search URL: {search_url}\")\n",
    "        \n",
    "        try:\n",
    "            # Load the search page\n",
    "            self.driver.get(search_url)\n",
    "            \n",
    "            # Wait for post results to load\n",
    "            WebDriverWait(self.driver, 10).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, \"a[data-testid='post-title']\"))\n",
    "            )\n",
    "            \n",
    "            # Give the page a moment to fully load\n",
    "            time.sleep(3)\n",
    "            \n",
    "            # Find all post links\n",
    "            post_links = self.driver.find_elements(By.CSS_SELECTOR, \"a[data-testid='post-title']\")\n",
    "            logger.info(f\"Found {len(post_links)} posts in search results\")\n",
    "            \n",
    "            # Extract post URLs and titles\n",
    "            posts_data = []\n",
    "            for link in post_links:\n",
    "                try:\n",
    "                    url = link.get_attribute(\"href\")\n",
    "                    title = link.find_element(By.TAG_NAME, \"faceplate-screen-reader-content\").text.strip()\n",
    "                    posts_data.append({\"url\": url, \"title\": title})\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error extracting post data: {e}\")\n",
    "            \n",
    "            logger.info(f\"Extracted data for {len(posts_data)} posts\")\n",
    "            return posts_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error searching Reddit: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def scrape_post_and_comments(self, post_url, post_title):\n",
    "        \"\"\"\n",
    "        Scrape a specific post and all its comments\n",
    "        \"\"\"\n",
    "        logger.info(f\"Scraping post: {post_title}\")\n",
    "        \n",
    "        try:\n",
    "            # Navigate to the post\n",
    "            self.driver.get(post_url)\n",
    "            \n",
    "            # Wait for the post content to load\n",
    "            WebDriverWait(self.driver, 10).until(\n",
    "                EC.presence_of_element_located((By.TAG_NAME, \"shreddit-comment\"))\n",
    "            )\n",
    "            \n",
    "            # Give the page a moment to fully load\n",
    "            time.sleep(3)\n",
    "            \n",
    "            # Extract post author\n",
    "            try:\n",
    "                post_author_element = self.driver.find_element(By.CSS_SELECTOR, \"a[data-testid='post_author_link']\")\n",
    "                post_author = post_author_element.text.strip()\n",
    "            except NoSuchElementException:\n",
    "                post_author = \"Unknown\"\n",
    "            \n",
    "            # Create post data structure\n",
    "            post_data = {\n",
    "                \"title\": post_title,\n",
    "                \"url\": post_url,\n",
    "                \"author\": post_author,\n",
    "                \"comments\": []\n",
    "            }\n",
    "            \n",
    "            # Find all comments\n",
    "            comment_elements = self.driver.find_elements(By.TAG_NAME, \"shreddit-comment\")\n",
    "            logger.info(f\"Found {len(comment_elements)} comments in post\")\n",
    "            \n",
    "            # Process each comment\n",
    "            for comment in comment_elements:\n",
    "                try:\n",
    "                    # Extract username\n",
    "                    try:\n",
    "                        # Try to find the username link\n",
    "                        username_element = comment.find_element(By.CSS_SELECTOR, \"a.text-12.font-bold\")\n",
    "                        username = username_element.text.strip()\n",
    "                    except NoSuchElementException:\n",
    "                        # If the link is not found, try to find the username span (for deleted users)\n",
    "                        try:\n",
    "                            username_span = comment.find_element(By.CSS_SELECTOR, \"span.text-neutral-content-weak\")\n",
    "                            username = username_span.text.strip()\n",
    "                        except NoSuchElementException:\n",
    "                            # If all else fails, check if there's any username element\n",
    "                            try:\n",
    "                                username_element = comment.find_element(By.CSS_SELECTOR, \".author-name-meta a\")\n",
    "                                username = username_element.text.strip()\n",
    "                            except:\n",
    "                                username = \"Unknown User\"\n",
    "                    \n",
    "                    # Skip [deleted] users\n",
    "                    if username == \"[deleted]\":\n",
    "                        continue\n",
    "                    \n",
    "                    # Extract comment content\n",
    "                    try:\n",
    "                        content_element = comment.find_element(By.CSS_SELECTOR, \"div[slot='comment']\")\n",
    "                        content = content_element.text.strip()\n",
    "                    except NoSuchElementException:\n",
    "                        content = \"No content found\"\n",
    "                    \n",
    "                    # Add to comments list\n",
    "                    post_data[\"comments\"].append({\n",
    "                        \"username\": username,\n",
    "                        \"content\": content\n",
    "                    })\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error extracting comment data: {e}\")\n",
    "            \n",
    "            # Click on \"More replies\" buttons to load additional comments\n",
    "            try:\n",
    "                more_replies_buttons = self.driver.find_elements(By.XPATH, \"//button[contains(text(), 'more repl')]\")\n",
    "                for button in more_replies_buttons[:5]:  # Limit to 5 to avoid too many clicks\n",
    "                    try:\n",
    "                        button.click()\n",
    "                        time.sleep(2)  # Wait for new comments to load\n",
    "                        \n",
    "                        # Get the newly loaded comments\n",
    "                        new_comments = self.driver.find_elements(By.TAG_NAME, \"shreddit-comment\")\n",
    "                        logger.info(f\"Loaded {len(new_comments) - len(comment_elements)} additional comments\")\n",
    "                        \n",
    "                        # Process the new comments (only the ones we haven't processed yet)\n",
    "                        for comment in new_comments[len(comment_elements):]:\n",
    "                            try:\n",
    "                                # Extract username\n",
    "                                try:\n",
    "                                    username_element = comment.find_element(By.CSS_SELECTOR, \"a.text-12.font-bold\")\n",
    "                                    username = username_element.text.strip()\n",
    "                                except NoSuchElementException:\n",
    "                                    try:\n",
    "                                        username_span = comment.find_element(By.CSS_SELECTOR, \"span.text-neutral-content-weak\")\n",
    "                                        username = username_span.text.strip()\n",
    "                                    except:\n",
    "                                        username = \"Unknown User\"\n",
    "                                \n",
    "                                # Skip [deleted] users\n",
    "                                if username == \"[deleted]\":\n",
    "                                    continue\n",
    "                                \n",
    "                                # Extract comment content\n",
    "                                try:\n",
    "                                    content_element = comment.find_element(By.CSS_SELECTOR, \"div[slot='comment']\")\n",
    "                                    content = content_element.text.strip()\n",
    "                                except NoSuchElementException:\n",
    "                                    content = \"No content found\"\n",
    "                                \n",
    "                                # Add to comments list\n",
    "                                post_data[\"comments\"].append({\n",
    "                                    \"username\": username,\n",
    "                                    \"content\": content\n",
    "                                })\n",
    "                                \n",
    "                            except Exception as e:\n",
    "                                logger.error(f\"Error extracting new comment data: {e}\")\n",
    "                        \n",
    "                        # Update our list of processed comments\n",
    "                        comment_elements = new_comments\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"Error clicking more replies button: {e}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error finding more replies buttons: {e}\")\n",
    "            \n",
    "            return post_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error scraping post {post_url}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def process_search_results(self, search_query, max_posts=5):\n",
    "        \"\"\"\n",
    "        Process the search results and scrape posts and comments\n",
    "        \"\"\"\n",
    "        # Search for posts\n",
    "        posts_data = self.search_posts(search_query)\n",
    "        \n",
    "        # Limit to max_posts\n",
    "        posts_to_scrape = posts_data[:max_posts]\n",
    "        \n",
    "        # Scrape each post\n",
    "        for i, post in enumerate(posts_to_scrape):\n",
    "            logger.info(f\"Processing post {i+1}/{len(posts_to_scrape)}: {post['title']}\")\n",
    "            post_data = self.scrape_post_and_comments(post['url'], post['title'])\n",
    "            \n",
    "            if post_data:\n",
    "                self.posts.append(post_data)\n",
    "            \n",
    "            # Wait between posts to avoid rate limiting\n",
    "            if i < len(posts_to_scrape) - 1:\n",
    "                time.sleep(2)\n",
    "        \n",
    "        return self.posts\n",
    "    \n",
    "    def save_to_csv(self, filename_prefix=None):\n",
    "        \"\"\"\n",
    "        Save the scraped comments to a CSV file\n",
    "        \"\"\"\n",
    "        if filename_prefix is None:\n",
    "            filename_prefix = \"reddit_data\"\n",
    "        \n",
    "        comments_filename = f\"{filename_prefix}_comments.csv\"\n",
    "        \n",
    "        # Save comments to CSV\n",
    "        try:\n",
    "            with open(comments_filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "                fieldnames = ['post_title', 'post_url', 'username', 'comment_content']\n",
    "                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "                \n",
    "                writer.writeheader()\n",
    "                for post in self.posts:\n",
    "                    for comment in post['comments']:\n",
    "                        writer.writerow({\n",
    "                            'post_title': post['title'],\n",
    "                            'post_url': post['url'],\n",
    "                            'username': comment['username'],\n",
    "                            'comment_content': comment['content']\n",
    "                        })\n",
    "            \n",
    "            total_comments = sum(len(post['comments']) for post in self.posts)\n",
    "            logger.info(f\"Saved {total_comments} comments to {comments_filename}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving comments to CSV: {e}\")\n",
    "        \n",
    "    # USER PROFILE METHODS\n",
    "    \n",
    "    def handle_age_verification(self):\n",
    "        \"\"\"Handle age verification if it appears\"\"\"\n",
    "        try:\n",
    "            # Look for the age verification element with a short timeout\n",
    "            # Fixed quotes in XPath expression\n",
    "            age_verification = WebDriverWait(self.driver, 3).until(\n",
    "                EC.presence_of_element_located((By.XPATH, \"//span[contains(text(), \\\"Yes, I'm Over 18\\\")]\"))\n",
    "            )\n",
    "            \n",
    "            # If found, click it\n",
    "            logger.info(\"Age verification detected, clicking 'Yes, I'm Over 18'\")\n",
    "            age_verification.click()\n",
    "            \n",
    "            # Wait for the page to load after verification\n",
    "            time.sleep(2)\n",
    "            return True\n",
    "            \n",
    "        except (TimeoutException, NoSuchElementException, ElementNotInteractableException):\n",
    "            # No age verification found, which is fine\n",
    "            logger.info(\"No age verification required\")\n",
    "            return False\n",
    "    \n",
    "    def click_comments_tab(self):\n",
    "        \"\"\"Click the Comments tab to only see comments\"\"\"\n",
    "        try:\n",
    "            # Try to find and click the Comments tab\n",
    "            try:\n",
    "                # First try more specific XPath with text()\n",
    "                comments_tab = self.driver.find_element(By.XPATH, \"//span[text()='Comments']/ancestor::a\")\n",
    "                logger.info(\"Found Comments tab with exact text match\")\n",
    "            except NoSuchElementException:\n",
    "                # Try more general contains() match\n",
    "                comments_tab = self.driver.find_element(By.XPATH, \"//span[contains(text(), 'Comments')]/ancestor::a\")\n",
    "                logger.info(\"Found Comments tab with contains match\")\n",
    "            \n",
    "            logger.info(\"Clicking Comments tab\")\n",
    "            comments_tab.click()\n",
    "            time.sleep(2)\n",
    "            return True\n",
    "            \n",
    "        except (NoSuchElementException, ElementNotInteractableException) as e:\n",
    "            logger.error(f\"Failed to click Comments tab: {e}\")\n",
    "            # Try to find Overview tab instead\n",
    "            try:\n",
    "                overview_tab = self.driver.find_element(By.XPATH, \"//span[contains(text(), 'Overview')]/ancestor::a\")\n",
    "                logger.info(\"Clicking Overview tab instead\")\n",
    "                overview_tab.click()\n",
    "                time.sleep(2)\n",
    "                return True\n",
    "            except:\n",
    "                logger.error(\"Failed to find any navigation tabs\")\n",
    "                return False\n",
    "    \n",
    "    def scroll_and_scrape_comments(self, username, max_scrolls=10, max_comments=50):\n",
    "        \"\"\"\n",
    "        Scroll down the page multiple times to load all comments\n",
    "        and extract them after each scroll\n",
    "        \"\"\"\n",
    "        comments = []\n",
    "        last_comments_count = 0\n",
    "        consecutive_no_new_comments = 0\n",
    "        \n",
    "        # Initial height\n",
    "        last_height = self.driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        \n",
    "        # Scroll and extract comments in batches\n",
    "        for scroll_num in range(max_scrolls):\n",
    "            logger.info(f\"Scroll {scroll_num+1}/{max_scrolls} for user {username}\")\n",
    "            \n",
    "            # Scroll down\n",
    "            self.driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            \n",
    "            # Wait for new content to load\n",
    "            time.sleep(3)\n",
    "            \n",
    "            # Get new scroll height\n",
    "            new_height = self.driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            \n",
    "            # Extract comments after scrolling\n",
    "            scroll_comments = self.extract_comments_from_page()\n",
    "            \n",
    "            # Add only new comments\n",
    "            new_comments = [comment for comment in scroll_comments if comment not in comments]\n",
    "            if new_comments:\n",
    "                logger.info(f\"Found {len(new_comments)} new comments after scroll {scroll_num+1}\")\n",
    "                comments.extend(new_comments)\n",
    "                consecutive_no_new_comments = 0\n",
    "            else:\n",
    "                consecutive_no_new_comments += 1\n",
    "                logger.info(f\"No new comments found after scroll {scroll_num+1}\")\n",
    "            \n",
    "            # If we've reached max comments, stop scrolling\n",
    "            if len(comments) >= max_comments:\n",
    "                logger.info(f\"Reached maximum comment limit ({max_comments})\")\n",
    "                break\n",
    "                \n",
    "            # If same height for 3 consecutive times, no more content\n",
    "            if new_height == last_height:\n",
    "                logger.info(\"No more content to load (scroll height unchanged)\")\n",
    "                consecutive_no_new_comments += 1\n",
    "            else:\n",
    "                last_height = new_height\n",
    "            \n",
    "            # If no new comments for 3 consecutive scrolls, stop\n",
    "            if consecutive_no_new_comments >= 3:\n",
    "                logger.info(f\"No new comments for 3 consecutive scrolls, stopping\")\n",
    "                break\n",
    "        \n",
    "        logger.info(f\"Total comments extracted for {username}: {len(comments)}\")\n",
    "        return comments[:max_comments]\n",
    "    \n",
    "    def extract_comments_from_page(self):\n",
    "        \"\"\"Extract comments from the current page\"\"\"\n",
    "        comments = []\n",
    "        \n",
    "        # Find all comment elements on the page using multiple selectors\n",
    "        comment_selectors = [\n",
    "            \"div.Comment\", \n",
    "            \"shreddit-comment\", \n",
    "            \"div.p-md\",\n",
    "            \"div.mt-2xs\"\n",
    "        ]\n",
    "        \n",
    "        comment_elements = []\n",
    "        for selector in comment_selectors:\n",
    "            elements = self.driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "            comment_elements.extend(elements)\n",
    "        \n",
    "        # Process each comment element\n",
    "        for element in comment_elements:\n",
    "            try:\n",
    "                comment_data = {}\n",
    "                \n",
    "                # Get the subreddit and title context\n",
    "                try:\n",
    "                    # Find links to subreddits\n",
    "                    subreddit_elements = element.find_elements(By.CSS_SELECTOR, \"a[href^='/r/']\")\n",
    "                    if subreddit_elements:\n",
    "                        comment_data[\"subreddit\"] = subreddit_elements[0].text.strip()\n",
    "                    else:\n",
    "                        comment_data[\"subreddit\"] = \"Unknown subreddit\"\n",
    "                    \n",
    "                    # Find the post title\n",
    "                    title_elements = element.find_elements(By.CSS_SELECTOR, \"a[aria-label*='Thread for'], a[href*='/comments/']\")\n",
    "                    if title_elements:\n",
    "                        for title_elem in title_elements:\n",
    "                            aria_label = title_elem.get_attribute(\"aria-label\")\n",
    "                            if aria_label and \"comment on \" in aria_label:\n",
    "                                comment_data[\"title\"] = aria_label.split(\"comment on \")[1]\n",
    "                                break\n",
    "                            elif title_elem.text and len(title_elem.text.strip()) > 5:\n",
    "                                comment_data[\"title\"] = title_elem.text.strip()\n",
    "                                break\n",
    "                    else:\n",
    "                        comment_data[\"title\"] = \"Unknown title\"\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error extracting comment context: {e}\")\n",
    "                    comment_data[\"subreddit\"] = \"Unknown subreddit\"\n",
    "                    comment_data[\"title\"] = \"Unknown title\"\n",
    "                \n",
    "                # Get the comment content\n",
    "                try:\n",
    "                    # Try different content selectors\n",
    "                    content_selectors = [\n",
    "                        \"div.md p\", \n",
    "                        \"div[data-click-id='text'] p\", \n",
    "                        \"div.text-neutral-content-strong p\",\n",
    "                        \"div[slot='comment']\",\n",
    "                        \"div.text-neutral-content-strong\",\n",
    "                        \"p\"\n",
    "                    ]\n",
    "                    \n",
    "                    content = \"No content found\"\n",
    "                    for selector in content_selectors:\n",
    "                        try:\n",
    "                            content_elements = element.find_elements(By.CSS_SELECTOR, selector)\n",
    "                            if content_elements:\n",
    "                                content = content_elements[0].text.strip()\n",
    "                                if content:\n",
    "                                    break\n",
    "                        except:\n",
    "                            continue\n",
    "                    \n",
    "                    comment_data[\"content\"] = content\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error extracting comment content: {e}\")\n",
    "                    comment_data[\"content\"] = \"Error extracting content\"\n",
    "                \n",
    "                # Get the timestamp\n",
    "                try:\n",
    "                    time_elements = element.find_elements(By.TAG_NAME, \"time\")\n",
    "                    if time_elements:\n",
    "                        comment_data[\"time\"] = time_elements[0].get_attribute(\"datetime\")\n",
    "                        comment_data[\"time_display\"] = time_elements[0].text.strip()\n",
    "                    else:\n",
    "                        comment_data[\"time\"] = \"Unknown time\"\n",
    "                        comment_data[\"time_display\"] = \"Unknown time\"\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error extracting comment time: {e}\")\n",
    "                    comment_data[\"time\"] = \"Unknown time\"\n",
    "                    comment_data[\"time_display\"] = \"Unknown time\"\n",
    "                \n",
    "                # Add to comments list if it contains actual content\n",
    "                if comment_data[\"content\"] and comment_data[\"content\"] != \"No content found\" and comment_data[\"content\"] != \"Error extracting content\":\n",
    "                    comments.append(comment_data)\n",
    "            \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing comment element: {e}\")\n",
    "        \n",
    "        return comments\n",
    "    \n",
    "    def scrape_user_profile(self, username):\n",
    "        \"\"\"Scrape a Reddit user's profile focusing on comments\"\"\"\n",
    "        profile_url = f\"https://www.reddit.com/user/{username}/\"\n",
    "        logger.info(f\"Scraping profile: {profile_url}\")\n",
    "        \n",
    "        try:\n",
    "            # Skip [deleted] users\n",
    "            if username == \"[deleted]\":\n",
    "                logger.info(f\"Skipping [deleted] user\")\n",
    "                return None\n",
    "                \n",
    "            # Navigate to the profile\n",
    "            self.driver.get(profile_url)\n",
    "            \n",
    "            # Wait for the page to load\n",
    "            WebDriverWait(self.driver, 10).until(\n",
    "                EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n",
    "            )\n",
    "            \n",
    "            # Handle age verification if needed\n",
    "            self.handle_age_verification()\n",
    "            \n",
    "            # Try to click on the Comments tab to focus only on comments\n",
    "            self.click_comments_tab()\n",
    "            \n",
    "            # Give the page a moment to load\n",
    "            time.sleep(2)\n",
    "            \n",
    "            # Scroll and extract comments\n",
    "            comments = self.scroll_and_scrape_comments(username)\n",
    "            \n",
    "            # Store the data\n",
    "            self.user_data[username] = {\n",
    "                \"username\": username,\n",
    "                \"profile_url\": profile_url,\n",
    "                \"comments\": comments\n",
    "            }\n",
    "            \n",
    "            logger.info(f\"Successfully scraped {len(comments)} comments from {username}'s profile\")\n",
    "            return self.user_data[username]\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error scraping profile {username}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def get_unique_usernames_from_comments(self):\n",
    "        \"\"\"Extract unique usernames from collected comments\"\"\"\n",
    "        usernames = set()\n",
    "        \n",
    "        for post in self.posts:\n",
    "            for comment in post.get(\"comments\", []):\n",
    "                username = comment.get(\"username\", \"\")\n",
    "                # Skip [deleted] users and unknown users\n",
    "                if username and username != \"[deleted]\" and username != \"Unknown User\":\n",
    "                    usernames.add(username)\n",
    "        \n",
    "        logger.info(f\"Extracted {len(usernames)} unique usernames from comments\")\n",
    "        return list(usernames)\n",
    "    \n",
    "    def scrape_all_users_from_comments(self, max_users=None):\n",
    "        \"\"\"Scrape profiles of all users who left comments\"\"\"\n",
    "        usernames = self.get_unique_usernames_from_comments()\n",
    "        \n",
    "        # Limit to max_users if specified\n",
    "        if max_users:\n",
    "            usernames = usernames[:min(max_users, len(usernames))]\n",
    "        \n",
    "        logger.info(f\"Preparing to scrape {len(usernames)} user profiles\")\n",
    "        \n",
    "        for i, username in enumerate(usernames):\n",
    "            logger.info(f\"Processing user {i+1}/{len(usernames)}: {username}\")\n",
    "            self.scrape_user_profile(username)\n",
    "            \n",
    "            # Wait between profiles to avoid rate limiting\n",
    "            if i < len(usernames) - 1:\n",
    "                time.sleep(2)\n",
    "        \n",
    "        return self.user_data\n",
    "    \n",
    "    def save_user_comments_to_csv(self, filename=\"reddit_user_comments.csv\"):\n",
    "        \"\"\"Save user comments to CSV\"\"\"\n",
    "        try:\n",
    "            with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "                fieldnames = ['username', 'subreddit', 'post_title', 'comment_content', 'time']\n",
    "                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "                \n",
    "                writer.writeheader()\n",
    "                for username, user_data in self.user_data.items():\n",
    "                    for comment in user_data.get('comments', []):\n",
    "                        writer.writerow({\n",
    "                            'username': username,\n",
    "                            'subreddit': comment.get('subreddit', 'Unknown'),\n",
    "                            'post_title': comment.get('title', 'Unknown'),\n",
    "                            'comment_content': comment.get('content', 'No content'),\n",
    "                            'time': comment.get('time', 'Unknown')\n",
    "                        })\n",
    "            \n",
    "            total_comments = sum(len(user_data.get('comments', [])) for user_data in self.user_data.values())\n",
    "            logger.info(f\"Saved {total_comments} user comments to {filename}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving user comments to CSV: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def save_user_data_to_json(self, filename=\"reddit_users.json\"):\n",
    "        \"\"\"Save user data to JSON\"\"\"\n",
    "        try:\n",
    "            with open(filename, 'w', encoding='utf-8') as f:\n",
    "                json.dump(self.user_data, f, indent=4)\n",
    "            \n",
    "            logger.info(f\"Saved data for {len(self.user_data)} users to {filename}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving user data to JSON: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Close the web driver\"\"\"\n",
    "        self.driver.quit()\n",
    "        logger.info(\"Closed web driver\")\n",
    "\n",
    "def main():\n",
    "    # Define the search query\n",
    "    search_query = \"data science job\"\n",
    "    \n",
    "    # Create the scraper\n",
    "    scraper = RedditCommentsScraper()\n",
    "    \n",
    "    try:\n",
    "        # Process search results\n",
    "        scraper.process_search_results(search_query, max_posts=3)\n",
    "        \n",
    "        # Save the comment results to CSV file\n",
    "        scraper.save_to_csv(\"data_science_job\")\n",
    "        \n",
    "        # Extract and scrape user profiles from the comments\n",
    "        scraper.scrape_all_users_from_comments(max_users=5)  # Limit to 5 users for testing\n",
    "        \n",
    "        # Save user comments data\n",
    "        scraper.save_user_comments_to_csv()\n",
    "        scraper.save_user_data_to_json()\n",
    "        \n",
    "        # Print a summary of the results\n",
    "        print(f\"\\nScraped {len(scraper.posts)} posts about data science jobs\")\n",
    "        print(f\"Scraped {len(scraper.user_data)} user profiles\")\n",
    "        \n",
    "        # Sample of user comments\n",
    "        for username, user_data in scraper.user_data.items():\n",
    "            print(f\"\\nUser: {username}\")\n",
    "            print(f\"Comments: {len(user_data.get('comments', []))}\")\n",
    "            \n",
    "            # Print first 2 comments as a sample\n",
    "            for i, comment in enumerate(user_data.get('comments', [])[:2]):\n",
    "                print(f\"  {i+1}. [{comment.get('subreddit', 'Unknown')}] {comment.get('title', 'Unknown title')}\")\n",
    "                print(f\"     Content: {comment.get('content', 'No content')[:100]}...\" if len(comment.get('content', '')) > 100 else f\"     Content: {comment.get('content', 'No content')}\")\n",
    "            \n",
    "            if len(user_data.get('comments', [])) > 2:\n",
    "                print(f\"  ...and {len(user_data.get('comments', [])) - 2} more comments\")\n",
    "        \n",
    "        return {\"posts\": scraper.posts, \"users\": scraper.user_data}\n",
    "    \n",
    "    finally:\n",
    "        # Close the driver\n",
    "        scraper.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d392fb40-fd42-44fe-bbbf-d6edf8636ed0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>post_title</th>\n",
       "      <th>comment_content</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>itsMalarky</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I like the label.</td>\n",
       "      <td>Definitely worth it. Same area</td>\n",
       "      <td>2025-04-28T00:58:56.716Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>itsMalarky</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Making friends</td>\n",
       "      <td>Game night (Tuesday), trivia on Wednesdays (tr...</td>\n",
       "      <td>2025-04-27T23:43:02.042Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>itsMalarky</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I like the label.</td>\n",
       "      <td>Good beer everywhere. No point even making a c...</td>\n",
       "      <td>2025-04-27T18:17:59.349Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>itsMalarky</td>\n",
       "      <td>NaN</td>\n",
       "      <td>You are now your username. What are you?</td>\n",
       "      <td>Bullshit.</td>\n",
       "      <td>2025-04-27T00:50:48.460Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>itsMalarky</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nightmare pairing with a Marines</td>\n",
       "      <td>Cool, still proving my point lol. Here ya go 🖍...</td>\n",
       "      <td>2025-04-27T00:48:06.562Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>occasional_cynic</td>\n",
       "      <td>Unknown subreddit</td>\n",
       "      <td>Unknown title</td>\n",
       "      <td>because AWS doesn’t route public replies back ...</td>\n",
       "      <td>Unknown time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>occasional_cynic</td>\n",
       "      <td>Unknown subreddit</td>\n",
       "      <td>Unknown title</td>\n",
       "      <td>In theory yes.</td>\n",
       "      <td>Unknown time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>AtmosphereGrouchy245</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Anyone working as a data scientist/ data analy...</td>\n",
       "      <td>Hey folks, I’m on the hunt for a data science ...</td>\n",
       "      <td>2025-03-26T01:54:35.773Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>AtmosphereGrouchy245</td>\n",
       "      <td>Unknown subreddit</td>\n",
       "      <td>Unknown title</td>\n",
       "      <td>2</td>\n",
       "      <td>2022-02-17T00:00:00.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>AtmosphereGrouchy245</td>\n",
       "      <td>Unknown subreddit</td>\n",
       "      <td>Unknown title</td>\n",
       "      <td>Hey folks, I’m on the hunt for a data science ...</td>\n",
       "      <td>Unknown time</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>153 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 username          subreddit  \\\n",
       "0              itsMalarky                NaN   \n",
       "1              itsMalarky                NaN   \n",
       "2              itsMalarky                NaN   \n",
       "3              itsMalarky                NaN   \n",
       "4              itsMalarky                NaN   \n",
       "..                    ...                ...   \n",
       "148      occasional_cynic  Unknown subreddit   \n",
       "149      occasional_cynic  Unknown subreddit   \n",
       "150  AtmosphereGrouchy245                NaN   \n",
       "151  AtmosphereGrouchy245  Unknown subreddit   \n",
       "152  AtmosphereGrouchy245  Unknown subreddit   \n",
       "\n",
       "                                            post_title  \\\n",
       "0                                    I like the label.   \n",
       "1                                       Making friends   \n",
       "2                                    I like the label.   \n",
       "3             You are now your username. What are you?   \n",
       "4                     Nightmare pairing with a Marines   \n",
       "..                                                 ...   \n",
       "148                                      Unknown title   \n",
       "149                                      Unknown title   \n",
       "150  Anyone working as a data scientist/ data analy...   \n",
       "151                                      Unknown title   \n",
       "152                                      Unknown title   \n",
       "\n",
       "                                       comment_content  \\\n",
       "0                       Definitely worth it. Same area   \n",
       "1    Game night (Tuesday), trivia on Wednesdays (tr...   \n",
       "2    Good beer everywhere. No point even making a c...   \n",
       "3                                            Bullshit.   \n",
       "4    Cool, still proving my point lol. Here ya go 🖍...   \n",
       "..                                                 ...   \n",
       "148  because AWS doesn’t route public replies back ...   \n",
       "149                                     In theory yes.   \n",
       "150  Hey folks, I’m on the hunt for a data science ...   \n",
       "151                                                  2   \n",
       "152  Hey folks, I’m on the hunt for a data science ...   \n",
       "\n",
       "                         time  \n",
       "0    2025-04-28T00:58:56.716Z  \n",
       "1    2025-04-27T23:43:02.042Z  \n",
       "2    2025-04-27T18:17:59.349Z  \n",
       "3    2025-04-27T00:50:48.460Z  \n",
       "4    2025-04-27T00:48:06.562Z  \n",
       "..                        ...  \n",
       "148              Unknown time  \n",
       "149              Unknown time  \n",
       "150  2025-03-26T01:54:35.773Z  \n",
       "151  2022-02-17T00:00:00.000Z  \n",
       "152              Unknown time  \n",
       "\n",
       "[153 rows x 5 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('reddit_user_comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171ff494-8708-4370-afd4-d28eb0eb6845",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
